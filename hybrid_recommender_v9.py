# =====================================================================================================
# ğŸ¬ MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 9 (ìµœì¢… ì™„ë²½ ìˆ˜ì •)
# =====================================================================================================
# 
# [ë…¼ë¬¸ ê°œìš”]
# ì´ ì½”ë“œëŠ” í˜‘ì—… í•„í„°ë§(CF)ê³¼ ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§(CB)ì„ ê²°í•©í•œ
# 5ê°€ì§€ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.
#
# [ì£¼ìš” íŠ¹ì§•]
# 1. ì •í™•ë„ì™€ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— ì¸¡ì •í•˜ëŠ” 18ê°œ í‰ê°€ ì§€í‘œ
# 2. MovieLens Small(100K) ë° 1M ë°ì´í„°ì…‹ ìë™ ì²˜ë¦¬
# 3. í¬ì†Œì„±ì„ ê³ ë ¤í•œ ê³µì •í•œ ì„±ëŠ¥ í‰ê°€
# 4. í•„í„° ë²„ë¸” í˜„ìƒ ì™„í™” ëŠ¥ë ¥ ì¸¡ì •
#
# [ì˜ˆìƒ ë…ì]
# - ì¶”ì²œ ì‹œìŠ¤í…œ ì—°êµ¬ì
# - ë°ì´í„° ê³¼í•™ì
# - ë…¼ë¬¸ ì‘ì„±ì
#
# =====================================================================================================

import os
import io
import requests
import zipfile
import pandas as pd
import numpy as np
import math
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import MultiLabelBinarizer
import warnings

warnings.filterwarnings('ignore')

print("=" * 100)
print("ğŸ¬ MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 9 (ìµœì¢… ì™„ë²½ ìˆ˜ì •)")
print("=" * 100)


# =====================================================================================================
# ì„¹ì…˜ 1: í‰ê°€ ì§€í‘œ í´ë˜ìŠ¤
# =====================================================================================================

class AdvancedMetrics:
    """ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ì§€í‘œ: ì •í™•ë„(3) + ìˆœìœ„(6) + ë‹¤ì–‘ì„±(4) = ì´ 18ê°œ"""

    @staticmethod
    def precision_at_k(recommended, relevant, k=10):
        """ì¶”ì²œ ì¤‘ ê´€ë ¨ ì•„ì´í…œ ë¹„ìœ¨"""
        if k == 0 or not recommended:
            return 0.0
        rec_k = set(recommended[:k])
        rel_set = set(relevant)
        return len(rec_k & rel_set) / k if len(rec_k) > 0 else 0.0

    @staticmethod
    def recall_at_k(recommended, relevant, k=10):
        """ê´€ë ¨ ì•„ì´í…œ ì¤‘ ì¶”ì²œëœ ë¹„ìœ¨"""
        if not relevant:
            return 0.0
        rec_k = set(recommended[:k])
        rel_set = set(relevant)
        return len(rec_k & rel_set) / len(rel_set)

    @staticmethod
    def f1_at_k(recommended, relevant, k=10):
        """Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· """
        p = AdvancedMetrics.precision_at_k(recommended, relevant, k)
        r = AdvancedMetrics.recall_at_k(recommended, relevant, k)
        if p + r == 0:
            return 0.0
        return 2 * (p * r) / (p + r)

    @staticmethod
    def ndcg_at_k(relevance, k=10):
        """ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì„±ëŠ¥ í‰ê°€ (DCG / IDCG)"""
        if not relevance:
            return 0.0
        rel = relevance[:k]
        dcg = sum(r / math.log2(i + 2) for i, r in enumerate(rel))
        ideal = sorted(relevance, reverse=True)[:k]
        idcg = sum(r / math.log2(i + 2) for i, r in enumerate(ideal))
        return dcg / idcg if idcg > 0 else 0.0

    @staticmethod
    def map_at_k(recommended, relevant, k=10):
        """ê° ê´€ë ¨ ì•„ì´í…œ ë°œê²¬ ì‹œ Precisionì˜ í‰ê· """
        if not relevant:
            return 0.0
        rec_k = recommended[:k]
        rel_set = set(relevant)
        score = 0.0
        num_hits = 0
        for i, rec in enumerate(rec_k):
            if rec in rel_set:
                num_hits += 1
                score += num_hits / (i + 1)
        return score / min(len(rel_set), k)

    @staticmethod
    def mrr_at_k(recommended, relevant, k=10):
        """ì²« ì¢‹ì€ ì¶”ì²œê¹Œì§€ì˜ ê±°ë¦¬ ì—­ìˆ˜"""
        if not relevant:
            return 0.0
        rec_k = recommended[:k]
        rel_set = set(relevant)
        for i, rec in enumerate(rec_k):
            if rec in rel_set:
                return 1.0 / (i + 1)
        return 0.0

    @staticmethod
    def intra_list_diversity(recs, sim_matrix, item_to_idx):
        """ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ë‚´ ì•„ì´í…œ ë‹¤ì–‘ì„± (1 - í‰ê· ìœ ì‚¬ë„)"""
        if len(recs) < 2:
            return 0.0
        dists = []
        for i in range(len(recs)):
            for j in range(i + 1, len(recs)):
                if recs[i] in item_to_idx and recs[j] in item_to_idx:
                    idx_i = item_to_idx[recs[i]]
                    idx_j = item_to_idx[recs[j]]
                    similarity = sim_matrix[idx_i][idx_j]
                    dists.append(1 - similarity)
        return np.mean(dists) if dists else 0.0

    @staticmethod
    def coverage(all_recs, total_items):
        """ì „ì²´ ì¹´íƒˆë¡œê·¸ ì¤‘ ì¶”ì²œëœ ì•„ì´í…œ ë¹„ìœ¨"""
        unique_recs = set()
        for recs in all_recs:
            unique_recs.update(recs)
        return len(unique_recs) / total_items if total_items > 0 else 0.0

    @staticmethod
    def novelty(recs, popularity):
        """ì¶”ì²œ ì•„ì´í…œì˜ ì‹ ê·œì„± (-log2(ì¸ê¸°ë„))"""
        if not recs:
            return 0.0
        novelty_scores = []
        for rec in recs:
            pop = popularity.get(rec, 0.5)
            pop = max(pop, 0.001)
            novelty_scores.append(-math.log2(pop))
        return np.mean(novelty_scores) if novelty_scores else 0.0

    @staticmethod
    def popularity_bias(recs, popularity):
        """ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ì˜ í‰ê·  ì¸ê¸°ë„"""
        if not recs:
            return 0.0
        pop_scores = [popularity.get(rec, 0.5) for rec in recs]
        return np.mean(pop_scores)

    @staticmethod
    def sparsity_aware_score(test_data, predictions, num_users, num_items):
        """í¬ì†Œì„±ì„ ê³ ë ¤í•œ ì •ê·œí™” ì ìˆ˜"""
        rmse = np.sqrt(mean_squared_error(test_data, predictions))
        mae = mean_absolute_error(test_data, predictions)
        total_possible_pairs = num_users * num_items
        actual_pairs = len(test_data)
        sparsity = 1 - (actual_pairs / total_possible_pairs) if total_possible_pairs > 0 else 1.0
        adjusted_rmse = rmse / (1 + sparsity)
        adjusted_mae = mae / (1 + sparsity)
        
        return {
            'RMSE': rmse,
            'MAE': mae,
            'Sparsity': sparsity,
            'Adjusted_RMSE': adjusted_rmse,
            'Adjusted_MAE': adjusted_mae
        }


# =====================================================================================================
# ì„¹ì…˜ 2: ë°ì´í„° ë¡œë“œ
# =====================================================================================================

def load_movielens(dataset_type='Small'):
    """MovieLens ë°ì´í„°ì…‹ ìë™ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ"""
    datasets_info = {
        'Small': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip',
            'extract_dir': 'movielens_data/ml-latest-small',
            'encoding': 'utf-8'
        },
        '1M': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-1m.zip',
            'extract_dir': 'movielens_data/ml-1m',
            'encoding': 'iso-8859-1'
        }
    }

    if dataset_type not in datasets_info:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_type}")
        return None, None

    info = datasets_info[dataset_type]
    os.makedirs('movielens_data', exist_ok=True)

    if not os.path.exists(info['extract_dir']):
        print(f"ğŸ“¥ {dataset_type} ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        try:
            response = requests.get(info['url'], timeout=30)
            response.raise_for_status()
            with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:
                zip_ref.extractall('movielens_data')
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None

    try:
        extract_path = info['extract_dir']
        encoding = info['encoding']

        if dataset_type == 'Small':
            ratings = pd.read_csv(f'{extract_path}/ratings.csv', encoding=encoding)
            movies = pd.read_csv(f'{extract_path}/movies.csv', encoding=encoding)
        elif dataset_type == '1M':
            ratings = pd.read_csv(
                f'{extract_path}/ratings.dat',
                sep='::',
                header=None,
                engine='python',
                encoding=encoding,
                names=['userId', 'movieId', 'rating', 'timestamp']
            )
            movies = pd.read_csv(
                f'{extract_path}/movies.dat',
                sep='::',
                header=None,
                engine='python',
                encoding=encoding,
                names=['movieId', 'title', 'genres']
            )

        print(f"âœ… {dataset_type} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
        print(f" ğŸ“Œ ì‚¬ìš©ì: {ratings['userId'].nunique():,}ëª…")
        print(f" ğŸ“Œ ì˜í™”: {movies['movieId'].nunique():,}ê°œ")
        print(f" ğŸ“Œ í‰ì : {len(ratings):,}ê°œ")
        
        sparsity = 1 - (len(ratings) / (ratings['userId'].nunique() * movies['movieId'].nunique()))
        print(f" ğŸ“Œ í¬ì†Œì„±: {sparsity:.4f} ({sparsity*100:.2f}%)")

        return ratings, movies

    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# =====================================================================================================
# ì„¹ì…˜ 3: í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ
# =====================================================================================================

class OptimizedHybridRecommender:
    """5ê°€ì§€ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„"""

    def __init__(self, ratings, movies, name='recommender', svd_dim=200):
        self.name = name
        self.ratings = ratings
        self.movies = movies
        self.svd_dim = svd_dim
        self.metrics = AdvancedMetrics()
        
        self.mean_rating = ratings['rating'].mean()
        self.std_rating = ratings['rating'].std()
        
        self.user_factors = None
        self.movie_factors = None
        self.user_bias = {}
        self.movie_bias = {}
        self.movie_features = None
        self.item_similarity = None
        self.item_to_idx = None
        self.train = None
        self.test = None
        self.um = None
        self.popularity = {}
        
        print(f"ğŸš€ {name} ì´ˆê¸°í™” ì™„ë£Œ")

    def _prepare(self):
        """ëª¨ë¸ í•™ìŠµ ë° ì „ì²˜ë¦¬"""
        print(f"\nğŸ“Š {self.name} ì „ì²˜ë¦¬ ì¤‘...")

        # Train/Test ë¶„í•  (80/20)
        unique_users = self.ratings['userId'].unique()
        train_users, test_users = train_test_split(
            unique_users, test_size=0.2, random_state=42
        )
        self.train = self.ratings[self.ratings['userId'].isin(train_users)]
        self.test = self.ratings[self.ratings['userId'].isin(test_users)]
        print(f" âœ… Train: {len(self.train):,}, Test: {len(self.test):,}")

        # User-Movie í–‰ë ¬ ìƒì„±
        self.um = self.train.pivot_table(
            index='userId',
            columns='movieId',
            values='rating'
        ).fillna(self.mean_rating)

        # SVD ë¶„í•´
        print(f" ğŸ”„ SVD ë¶„í•´ ì¤‘...")
        svd = TruncatedSVD(
            n_components=min(self.svd_dim, self.um.shape[0]-1, self.um.shape[1]-1),
            random_state=42
        )
        user_features = svd.fit_transform(self.um)
        movie_features = svd.components_.T

        # ì •ê·œí™”
        self.user_factors = {}
        for i, user_id in enumerate(self.um.index):
            uf = user_features[i]
            uf_norm = (uf - uf.mean()) / (uf.std() + 1e-8)
            self.user_factors[user_id] = uf_norm

        self.movie_factors = {}
        for i, movie_id in enumerate(self.um.columns):
            mf = movie_features[i]
            mf_norm = (mf - mf.mean()) / (mf.std() + 1e-8)
            self.movie_factors[movie_id] = mf_norm

        # Bias ê³„ì‚°
        print(f" ğŸ¯ Bias ê³„ì‚° ì¤‘...")
        for user_id in self.train['userId'].unique():
            user_ratings = self.train[self.train['userId'] == user_id]
            user_mean = user_ratings['rating'].mean()
            self.user_bias[user_id] = user_mean - self.mean_rating

        for movie_id in self.train['movieId'].unique():
            movie_ratings = self.train[self.train['movieId'] == movie_id]
            movie_mean = movie_ratings['rating'].mean()
            self.movie_bias[movie_id] = movie_mean - self.mean_rating

        # ì½˜í…ì¸  íŠ¹ì§• ìƒì„± (ëª¨ë“  ì˜í™” ì‚¬ìš© - Coverage ê°œì„ )
        print(f" ğŸ¬ ì½˜í…ì¸  íŠ¹ì§• ìƒì„± ì¤‘...")
        mlb = MultiLabelBinarizer()
        genres_matrix = mlb.fit_transform(
            self.movies['genres'].str.split('|')
        )

        self.movie_features = {}
        self.item_to_idx = {}
        for i, movie_id in enumerate(self.movies['movieId']):
            self.movie_features[movie_id] = genres_matrix[i]
            self.item_to_idx[movie_id] = i

        self.item_similarity = cosine_similarity(genres_matrix)

        # ì¸ê¸°ë„ ê³„ì‚°
        popularity_series = self.train.groupby('movieId')['rating'].count()
        max_count = popularity_series.max()
        min_count = popularity_series.min()
        
        self.popularity = {}
        for movie_id in self.movies['movieId']:
            if movie_id in popularity_series.index:
                norm_pop = (popularity_series[movie_id] - min_count) / (max_count - min_count + 1e-8)
                self.popularity[movie_id] = norm_pop
            else:
                self.popularity[movie_id] = 0.0

        print(f"âœ… {self.name} ì „ì²˜ë¦¬ ì™„ë£Œ")

    def predict_cf(self, user_id, movie_id):
        """í˜‘ì—… í•„í„°ë§ (SVD ê¸°ë°˜)"""
        if user_id not in self.user_factors or movie_id not in self.movie_factors:
            return self.mean_rating

        user_vec = self.user_factors[user_id]
        movie_vec = self.movie_factors[movie_id]
        latent_score = np.dot(user_vec, movie_vec)
        scaled_score = latent_score * self.std_rating
        ub = self.user_bias.get(user_id, 0)
        mb = self.movie_bias.get(movie_id, 0)
        pred = scaled_score + ub + mb + self.mean_rating
        return np.clip(pred, 1, 5)

    def predict_cb(self, user_id, movie_id):
        """ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ (ì¥ë¥´ ìœ ì‚¬ë„)"""
        user_ratings = self.train[self.train['userId'] == user_id]

        if user_ratings.empty:
            return self.mean_rating
        
        if not hasattr(self, 'item_to_idx') or self.item_to_idx is None:
            return self.mean_rating
        
        if movie_id not in self.item_to_idx:
            return self.mean_rating

        rated_movies = user_ratings['movieId'].values
        weighted_sum = 0.0
        similarity_sum = 0.0

        for rated_id in rated_movies:
            if rated_id not in self.item_to_idx or movie_id not in self.item_to_idx:
                continue
                
            idx_rated = self.item_to_idx[rated_id]
            idx_movie = self.item_to_idx[movie_id]
            
            try:
                similarity = self.item_similarity[idx_movie][idx_rated]
                rating = user_ratings[user_ratings['movieId'] == rated_id]['rating'].values[0]
                weighted_sum += similarity * rating
                similarity_sum += similarity
            except (IndexError, ValueError):
                continue

        if similarity_sum > 0:
            pred = weighted_sum / similarity_sum
            return np.clip(pred, 1, 5)
        else:
            return self.mean_rating

    def predict_weighted_avg(self, user_id, movie_id, alpha=0.4):
        """ê°€ì¤‘ í‰ê·  í•˜ì´ë¸Œë¦¬ë“œ: Î±Ã—CF + (1-Î±)Ã—CB"""
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        pred = alpha * cf + (1 - alpha) * cb
        return np.clip(pred, 1, 5)

    def predict_feature_combo(self, user_id, movie_id):
        """íŠ¹ì§• ê²°í•© í•˜ì´ë¸Œë¦¬ë“œ: 0.4Ã—CF + 0.4Ã—CB + 0.1Ã—ì‚¬ìš©ìí¸í–¥ + 0.1Ã—ì˜í™”í¸í–¥"""
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)

        ub = self.user_bias.get(user_id, 0)
        mb = self.movie_bias.get(movie_id, 0)

        ub_normalized = np.clip(ub / (self.std_rating + 1e-8), -1, 1)
        mb_normalized = np.clip(mb / (self.std_rating + 1e-8), -1, 1)

        pred = 0.4 * cf + 0.4 * cb + 0.1 * ub_normalized + 0.1 * mb_normalized
        return np.clip(pred, 1, 5)

    def predict_mixed(self, user_id, movie_id):
        """í˜¼í•© í•˜ì´ë¸Œë¦¬ë“œ: 0.5Ã—CF + 0.5Ã—CB"""
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        pred = 0.5 * cf + 0.5 * cb
        return np.clip(pred, 1, 5)

    def get_recommendations(self, user_id, n=10, method='weighted_avg'):
        """ì¶”ì²œ ìƒì„±"""
        watched = set(self.train[self.train['userId'] == user_id]['movieId'])
        predictions = []

        for movie_id in self.movies['movieId']:
            if not hasattr(self, 'item_to_idx') or self.item_to_idx is None:
                continue
            
            if movie_id not in self.item_to_idx:
                continue

            if movie_id not in watched:
                if method.lower() == 'cf':
                    pred = self.predict_cf(user_id, movie_id)
                elif method.lower() == 'cb':
                    pred = self.predict_cb(user_id, movie_id)
                elif method.lower() == 'weighted_avg':
                    pred = self.predict_weighted_avg(user_id, movie_id)
                elif method.lower() == 'feature_combo':
                    pred = self.predict_feature_combo(user_id, movie_id)
                elif method.lower() == 'mixed':
                    pred = self.predict_mixed(user_id, movie_id)
                else:
                    pred = self.predict_weighted_avg(user_id, movie_id)

                if 1 <= pred <= 5:
                    predictions.append((movie_id, pred))

        predictions.sort(key=lambda x: x[1], reverse=True)
        return [p[0] for p in predictions[:n]]

    def evaluate(self):
        """5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ í‰ê°€"""
        print(f"\nğŸ“ˆ {self.name} í‰ê°€ ì¤‘...")

        results = {}
        methods = {
            'CF': 'cf',
            'CB': 'cb',
            'Weighted_Avg': 'weighted_avg',
            'Feature_Combo': 'feature_combo',
            'Mixed': 'mixed'
        }

        for method_display, method_lower in methods.items():
            print(f" ğŸ“Š {method_display} í‰ê°€ ì¤‘...")

            # ê¸°ë³¸ ì •í™•ë„ ì§€í‘œ
            actuals = []
            preds = []

            for _, row in self.test.iterrows():
                user_id = row['userId']
                movie_id = row['movieId']
                actual = row['rating']

                if method_lower == 'cf':
                    pred = self.predict_cf(user_id, movie_id)
                elif method_lower == 'cb':
                    pred = self.predict_cb(user_id, movie_id)
                elif method_lower == 'weighted_avg':
                    pred = self.predict_weighted_avg(user_id, movie_id)
                elif method_lower == 'feature_combo':
                    pred = self.predict_feature_combo(user_id, movie_id)
                elif method_lower == 'mixed':
                    pred = self.predict_mixed(user_id, movie_id)
                else:
                    pred = self.predict_weighted_avg(user_id, movie_id)

                actuals.append(actual)
                preds.append(pred)

            # Sparsity-Aware Score
            num_users = len(self.um.index)
            num_items = len(self.um.columns)
            sparsity_result = self.metrics.sparsity_aware_score(
                test_data=np.array(actuals),
                predictions=np.array(preds),
                num_users=num_users,
                num_items=num_items
            )

            # ì¶”ì²œ ì§€í‘œ
            test_users = list(self.test['userId'].unique())
            precisions = []
            recalls = []
            f1s = []
            maps = []
            mrrs = []
            ndcgs = []

            for user_id in test_users:
                user_test = self.test[self.test['userId'] == user_id]
                if len(user_test) < 1:
                    continue

                recs = self.get_recommendations(user_id, n=10, method=method_lower)
                relevant = user_test[user_test['rating'] >= 4]['movieId'].tolist()

                if recs and relevant:
                    precisions.append(self.metrics.precision_at_k(recs, relevant, 10))
                    recalls.append(self.metrics.recall_at_k(recs, relevant, 10))
                    f1s.append(self.metrics.f1_at_k(recs, relevant, 10))
                    maps.append(self.metrics.map_at_k(recs, relevant, 10))
                    mrrs.append(self.metrics.mrr_at_k(recs, relevant, 10))
                    relevance = [1 if m in relevant else 0 for m in recs]
                    ndcgs.append(self.metrics.ndcg_at_k(relevance, 10))

            # ë‹¤ì–‘ì„± ì§€í‘œ
            all_recs = []
            for user_id in test_users:
                recs = self.get_recommendations(user_id, n=10, method=method_lower)
                if recs:
                    all_recs.append(recs)

            diversities = []
            for recs in all_recs:
                if len(recs) >= 2:
                    div = self.metrics.intra_list_diversity(
                        recs,
                        self.item_similarity,
                        self.item_to_idx
                    )
                    if div > 0:
                        diversities.append(div)
            
            diversity = np.mean(diversities) if diversities else 0.0
            coverage = self.metrics.coverage(all_recs, len(self.movies))

            all_recommended_items = []
            for recs in all_recs:
                all_recommended_items.extend(recs)

            novelty = self.metrics.novelty(all_recommended_items, self.popularity)
            pop_bias = self.metrics.popularity_bias(all_recommended_items, self.popularity)

            # ê²°ê³¼ ì €ì¥
            results[method_display] = {
                'RMSE': np.sqrt(mean_squared_error(actuals, preds)),
                'MAE': mean_absolute_error(actuals, preds),
                'Sparsity': sparsity_result['Sparsity'],
                'Adjusted_RMSE': sparsity_result['Adjusted_RMSE'],
                'Adjusted_MAE': sparsity_result['Adjusted_MAE'],
                'Precision@10': np.mean(precisions) if precisions else 0,
                'Recall@10': np.mean(recalls) if recalls else 0,
                'F1@10': np.mean(f1s) if f1s else 0,
                'MAP@10': np.mean(maps) if maps else 0,
                'MRR@10': np.mean(mrrs) if mrrs else 0,
                'NDCG@10': np.mean(ndcgs) if ndcgs else 0,
                'Diversity': diversity,
                'Coverage': coverage,
                'Novelty': novelty,
                'PopularityBias': pop_bias,
                'Num_Samples': len(precisions)
            }

            print(f" âœ… {method_display}: RMSE={results[method_display]['RMSE']:.4f}")

        return results


# =====================================================================================================
# ì„¹ì…˜ 4: ë©”ì¸ ì‹¤í–‰
# =====================================================================================================

def main():
    """MovieLens Small/1M ë°ì´í„°ì…‹ì—ì„œ 5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ í‰ê°€"""
    datasets_to_test = ['Small', '1M']
    all_results = []

    for dataset in datasets_to_test:
        print(f"\n{'='*100}")
        print(f"ğŸ“Š {dataset} ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*100}")

        ratings, movies = load_movielens(dataset)
        if ratings is None or movies is None:
            print(f"âŒ {dataset} ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
            continue

        model = OptimizedHybridRecommender(ratings, movies, name=dataset, svd_dim=200)
        model._prepare()
        results = model.evaluate()

        for method, metrics in results.items():
            metrics['Dataset'] = dataset
            metrics['Method'] = method
            all_results.append(metrics)

    # ê²°ê³¼ ì¶œë ¥
    if all_results:
        print(f"\n{'='*100}")
        print("ğŸ“ˆ ìµœì¢… ê²°ê³¼ (5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ Ã— 2 ë°ì´í„°ì…‹)")
        print(f"{'='*100}")

        results_df = pd.DataFrame(all_results)

        print("\nâœ… ê¸°ë³¸ ì •í™•ë„ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'RMSE', 'MAE', 'Sparsity', 'Adjusted_RMSE']].to_string(index=False))

        print("\nâœ… ì¶”ì²œ ì •í™•ì„± ì§€í‘œ (Ranking Metrics):")
        print(results_df[['Dataset', 'Method', 'Precision@10', 'Recall@10', 'F1@10', 'NDCG@10']].to_string(index=False))

        print("\nâœ… ìˆœìœ„ ê¸°ë°˜ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'MAP@10', 'MRR@10', 'NDCG@10']].to_string(index=False))

        print("\nâœ… ë‹¤ì–‘ì„± ë° í’ˆì§ˆ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'Diversity', 'Coverage', 'Novelty', 'PopularityBias']].to_string(index=False))

        # CSV ì €ì¥
        output_filename = 'hybrid_results_small_1m_ver9_corrected.csv'
        results_df.to_csv(output_filename, index=False)
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_filename}")

        # ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥
        print("\nğŸ“Š ë°ì´í„°ì…‹ë³„ ë¹„êµ:")
        print("-" * 100)

        for dataset in datasets_to_test:
            dataset_results = results_df[results_df['Dataset'] == dataset]
            if len(dataset_results) > 0:
                best_method = dataset_results.loc[dataset_results['RMSE'].idxmin(), 'Method']
                best_rmse = dataset_results['RMSE'].min()
                best_adj_rmse = dataset_results['Adjusted_RMSE'].min()
                best_precision = dataset_results['Precision@10'].max()
                best_novelty = dataset_results['Novelty'].max()

                print(f"\n{dataset}:")
                print(f" ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ (RMSE): {best_method}")
                print(f" ìµœì € RMSE: {best_rmse:.4f}")
                print(f" ìµœì € Adjusted_RMSE: {best_adj_rmse:.4f}")
                print(f" ìµœê³  Precision@10: {best_precision:.4f}")
                print(f" ìµœê³  Novelty: {best_novelty:.4f}")
    else:
        print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤")


if __name__ == "__main__":
    main()
