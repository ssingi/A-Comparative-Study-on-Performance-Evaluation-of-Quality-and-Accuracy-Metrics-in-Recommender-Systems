# ìˆ˜ì •ëœ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 7 (UTF-8 ì¸ì½”ë”© ì˜¤ë¥˜ ìˆ˜ì •)
# Small (100K) + 1M ë°ì´í„°ì…‹ ëª¨ë‘ ì§€ì›
# â­ í•µì‹¬ ìˆ˜ì •: encoding='iso-8859-1' ë˜ëŠ” 'latin-1' ì¶”ê°€

import os
import io
import requests
import zipfile
import pandas as pd
import numpy as np
import math
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import MultiLabelBinarizer
import warnings
warnings.filterwarnings('ignore')

print("=" * 100)
print("ğŸ¬ MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Small & 1M ë°ì´í„°ì…‹ ì§€ì› (Ver 7 - ì¸ì½”ë”© ìˆ˜ì •)")
print("=" * 100)

# ================================
# ê³ ê¸‰ í‰ê°€ ì§€í‘œ í´ë˜ìŠ¤
# ================================

class AdvancedMetrics:
    """12ê°œ í‰ê°€ ì§€í‘œ + Sparsity-Aware Score"""
    
    @staticmethod
    def precision_at_k(recommended, relevant, k=10):
        """ì¶”ì²œ ì¤‘ ë§ì€ ë¹„ìœ¨"""
        if k == 0 or not recommended:
            return 0.0
        rec_k = set(recommended[:k])
        rel_set = set(relevant)
        return len(rec_k & rel_set) / k if len(rec_k) > 0 else 0.0
    
    @staticmethod
    def recall_at_k(recommended, relevant, k=10):
        """ì°¾ì€ ì¢‹ì€ ì˜í™”ì˜ ë¹„ìœ¨"""
        if not relevant:
            return 0.0
        rec_k = set(recommended[:k])
        rel_set = set(relevant)
        return len(rec_k & rel_set) / len(rel_set)
    
    @staticmethod
    def f1_at_k(recommended, relevant, k=10):
        """F1 ìŠ¤ì½”ì–´"""
        p = AdvancedMetrics.precision_at_k(recommended, relevant, k)
        r = AdvancedMetrics.recall_at_k(recommended, relevant, k)
        if p + r == 0:
            return 0.0
        return 2 * (p * r) / (p + r)
    
    @staticmethod
    def ndcg_at_k(relevance, k=10):
        """ìˆœìœ„ë³„ í• ì¸ ëˆ„ì  ì´ë“"""
        if not relevance:
            return 0.0
        rel = relevance[:k]
        dcg = sum(r / math.log2(i + 2) for i, r in enumerate(rel))
        ideal = sorted(relevance, reverse=True)[:k]
        idcg = sum(r / math.log2(i + 2) for i, r in enumerate(ideal))
        return dcg / idcg if idcg > 0 else 0.0
    
    @staticmethod
    def map_at_k(recommended, relevant, k=10):
        """í‰ê·  ì •í™•ë„"""
        if not relevant:
            return 0.0
        rec_k = recommended[:k]
        rel_set = set(relevant)
        score = 0.0
        num_hits = 0
        for i, rec in enumerate(rec_k):
            if rec in rel_set:
                num_hits += 1
                score += num_hits / (i + 1)
        return score / min(len(rel_set), k)
    
    @staticmethod
    def mrr_at_k(recommended, relevant, k=10):
        """í‰ê·  ì—­ìˆœìœ„"""
        if not relevant:
            return 0.0
        rec_k = recommended[:k]
        rel_set = set(relevant)
        for i, rec in enumerate(rec_k):
            if rec in rel_set:
                return 1.0 / (i + 1)
        return 0.0
    
    @staticmethod
    def intra_list_diversity(recs, sim_matrix, item_to_idx):
        """ë¦¬ìŠ¤íŠ¸ ë‚´ ë‹¤ì–‘ì„±"""
        if len(recs) < 2:
            return 0.0
        dists = []
        for i in range(len(recs)):
            for j in range(i + 1, len(recs)):
                if recs[i] in item_to_idx and recs[j] in item_to_idx:
                    idx_i = item_to_idx[recs[i]]
                    idx_j = item_to_idx[recs[j]]
                    similarity = sim_matrix[idx_i][idx_j]
                    dists.append(1 - similarity)
        return np.mean(dists) if dists else 0.0
    
    @staticmethod
    def coverage(all_recs, total_items):
        """ì¹´íƒˆë¡œê·¸ ì»¤ë²„ë¦¬ì§€"""
        unique_recs = set()
        for recs in all_recs:
            unique_recs.update(recs)
        return len(unique_recs) / total_items if total_items > 0 else 0.0
    
    @staticmethod
    def novelty(recs, popularity):
        """ìƒˆë¡œì›€ ì •ë„"""
        if not recs:
            return 0.0
        novelty_scores = []
        for rec in recs:
            pop = popularity.get(rec, 0.5)
            pop = max(pop, 0.001)
            novelty_scores.append(-math.log2(pop))
        return np.mean(novelty_scores) if novelty_scores else 0.0
    
    @staticmethod
    def popularity_bias(recs, popularity):
        """ì¸ê¸°ë„ í¸í–¥"""
        if not recs:
            return 0.0
        pop_scores = [popularity.get(rec, 0.5) for rec in recs]
        return np.mean(pop_scores)
    
    @staticmethod
    def sparsity_aware_score(test_data, predictions, num_users, num_items):
        """í¬ì†Œì„±ì„ ê³ ë ¤í•œ ì„±ëŠ¥ ì ìˆ˜"""
        rmse = np.sqrt(mean_squared_error(test_data, predictions))
        mae = mean_absolute_error(test_data, predictions)
        total_possible_pairs = num_users * num_items
        actual_pairs = len(test_data)
        sparsity = 1 - (actual_pairs / total_possible_pairs) if total_possible_pairs > 0 else 1.0
        adjusted_rmse = rmse / (1 + sparsity)
        adjusted_mae = mae / (1 + sparsity)
        
        return {
            'RMSE': rmse,
            'MAE': mae,
            'Sparsity': sparsity,
            'Adjusted_RMSE': adjusted_rmse,
            'Adjusted_MAE': adjusted_mae
        }

# ================================
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (Small & 1M ì§€ì›)
# ================================

def load_movielens(dataset_type='Small'):
    """MovieLens ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ (Small ë˜ëŠ” 1M)"""
    datasets_info = {
        'Small': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip',
            'extract_dir': 'movielens_data/ml-latest-small',
            'folder_name': 'ml-latest-small',
            'encoding': 'utf-8'  # Smallì€ UTF-8
        },
        '1M': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-1m.zip',
            'extract_dir': 'movielens_data/ml-1m',
            'folder_name': 'ml-1m',
            'encoding': 'iso-8859-1'  # 1Mì€ Latin-1 ì¸ì½”ë”©
        }
    }
    
    if dataset_type not in datasets_info:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_type}")
        print(f"   ì§€ì› ë°ì´í„°ì…‹: {list(datasets_info.keys())}")
        return None, None
    
    info = datasets_info[dataset_type]
    os.makedirs('movielens_data', exist_ok=True)
    
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì²˜ìŒë§Œ)
    if not os.path.exists(info['extract_dir']):
        print(f"ğŸ“¥ {dataset_type} ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        try:
            response = requests.get(info['url'], timeout=30)
            response.raise_for_status()
            
            # io.BytesIO ì‚¬ìš©
            with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:
                zip_ref.extractall('movielens_data')
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   ì´ìœ : {type(e).__name__}")
            return None, None
    
    # ë°ì´í„° ë¡œë“œ
    try:
        extract_path = info['extract_dir']
        encoding = info['encoding']
        
        # â­ Smallê³¼ 1Mì˜ ë‹¤ë¥¸ í¬ë§· + ì¸ì½”ë”© ì²˜ë¦¬
        if dataset_type == 'Small':
            ratings = pd.read_csv(f'{extract_path}/ratings.csv', encoding=encoding)
            movies = pd.read_csv(f'{extract_path}/movies.csv', encoding=encoding)
            
        elif dataset_type == '1M':
            # 1M ë°ì´í„°ëŠ” êµ¬ë¶„ìê°€ ë‹¤ë¦„ (::) + ì¸ì½”ë”© ì§€ì •
            ratings = pd.read_csv(
                f'{extract_path}/ratings.dat',
                sep='::',
                header=None,
                engine='python',
                encoding=encoding,  # â­ ì¸ì½”ë”© ì§€ì •
                names=['userId', 'movieId', 'rating', 'timestamp']
            )
            movies = pd.read_csv(
                f'{extract_path}/movies.dat',
                sep='::',
                header=None,
                engine='python',
                encoding=encoding,  # â­ ì¸ì½”ë”© ì§€ì •
                names=['movieId', 'title', 'genres']
            )
        
        print(f"âœ… {dataset_type} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
        print(f"   ì‚¬ìš©ì: {ratings['userId'].nunique():,}ëª…")
        print(f"   ì˜í™”: {movies['movieId'].nunique():,}ê°œ")
        print(f"   í‰ì : {len(ratings):,}ê°œ")
        
        # í†µê³„ ì •ë³´
        sparsity = 1 - (len(ratings) / (ratings['userId'].nunique() * movies['movieId'].nunique()))
        print(f"   í¬ì†Œì„±: {sparsity:.4f} ({sparsity*100:.2f}%)")
        
        return ratings, movies
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"   ì´ìœ : {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return None, None

# ================================
# í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ í´ë˜ìŠ¤
# ================================

class OptimizedHybridRecommender:
    """ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ (CF/CB/Hybrid) ì„±ëŠ¥ ì¸¡ì •"""
    
    def __init__(self, ratings, movies, name='recommender', svd_dim=200):
        self.name = name
        self.ratings = ratings
        self.movies = movies
        self.svd_dim = svd_dim
        self.metrics = AdvancedMetrics()
        self.mean_rating = ratings['rating'].mean()
        self.std_rating = ratings['rating'].std()
        self.user_factors = None
        self.movie_factors = None
        self.user_bias = {}
        self.movie_bias = {}
        self.movie_features = None
        self.item_similarity = None
        self.item_to_idx = None
        self.train = None
        self.test = None
        self.um = None
        self.popularity = {}
        
        print(f"ğŸš€ {name} ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _prepare(self):
        """ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì¤€ë¹„"""
        print(f"\nğŸ“Š {self.name} ì „ì²˜ë¦¬ ì¤‘...")
        
        # Train/Test ë¶„í• 
        unique_users = self.ratings['userId'].unique()
        train_users, test_users = train_test_split(
            unique_users, test_size=0.2, random_state=42
        )
        
        self.train = self.ratings[self.ratings['userId'].isin(train_users)]
        self.test = self.ratings[self.ratings['userId'].isin(test_users)]
        
        print(f"  âœ… Train: {len(self.train):,}, Test: {len(self.test):,}")
        
        # ì‚¬ìš©ì-ì˜í™” í–‰ë ¬
        self.um = self.train.pivot_table(
            index='userId',
            columns='movieId',
            values='rating'
        ).fillna(self.mean_rating)
        
        # SVD ë¶„í•´
        print(f"  ğŸ”„ SVD ë¶„í•´ ì¤‘...")
        svd = TruncatedSVD(
            n_components=min(self.svd_dim, self.um.shape[0]-1, self.um.shape[1]-1),
            random_state=42
        )
        user_features = svd.fit_transform(self.um)
        movie_features = svd.components_.T
        
        # ì •ê·œí™”
        self.user_factors = {}
        for i, user_id in enumerate(self.um.index):
            uf = user_features[i]
            uf_norm = (uf - uf.mean()) / (uf.std() + 1e-8)
            self.user_factors[user_id] = uf_norm
        
        self.movie_factors = {}
        for i, movie_id in enumerate(self.um.columns):
            mf = movie_features[i]
            mf_norm = (mf - mf.mean()) / (mf.std() + 1e-8)
            self.movie_factors[movie_id] = mf_norm
        
        # Bias ê³„ì‚°
        print(f"  ğŸ¯ Bias ê³„ì‚° ì¤‘...")
        for user_id in self.train['userId'].unique():
            user_ratings = self.train[self.train['userId'] == user_id]
            user_mean = user_ratings['rating'].mean()
            self.user_bias[user_id] = user_mean - self.mean_rating
        
        for movie_id in self.train['movieId'].unique():
            movie_ratings = self.train[self.train['movieId'] == movie_id]
            movie_mean = movie_ratings['rating'].mean()
            self.movie_bias[movie_id] = movie_mean - self.mean_rating
        
        # ì½˜í…ì¸  íŠ¹ì§•
        print(f"  ğŸ¬ ì½˜í…ì¸  íŠ¹ì§• ìƒì„± ì¤‘...")
        movies_in_train = self.movies[self.movies['movieId'].isin(
            self.train['movieId'].unique()
        )].copy()
        
        mlb = MultiLabelBinarizer()
        genres_matrix = mlb.fit_transform(
            movies_in_train['genres'].str.split('|')
        )
        
        self.movie_features = {}
        self.item_to_idx = {}
        for i, movie_id in enumerate(movies_in_train['movieId']):
            self.movie_features[movie_id] = genres_matrix[i]
            self.item_to_idx[movie_id] = i
        
        self.item_similarity = cosine_similarity(genres_matrix)
        
        # ì¸ê¸°ë„
        popularity_series = self.train.groupby('movieId')['rating'].count()
        popularity_normalized = (popularity_series - popularity_series.min()) / (
            popularity_series.max() - popularity_series.min() + 1e-8
        )
        self.popularity = popularity_normalized.to_dict()
        
        print(f"âœ… {self.name} ì „ì²˜ë¦¬ ì™„ë£Œ")
    
    # ===== ì˜ˆì¸¡ ë©”ì„œë“œ =====
    
    def predict_cf(self, user_id, movie_id):
        """í˜‘ì—… í•„í„°ë§"""
        if user_id not in self.user_factors or movie_id not in self.movie_factors:
            return self.mean_rating
        
        user_vec = self.user_factors[user_id]
        movie_vec = self.movie_factors[movie_id]
        
        latent_score = np.dot(user_vec, movie_vec)
        scaled_score = latent_score * self.std_rating
        ub = self.user_bias.get(user_id, 0)
        mb = self.movie_bias.get(movie_id, 0)
        
        pred = scaled_score + ub + mb + self.mean_rating
        return np.clip(pred, 1, 5)
    
    def predict_cb(self, user_id, movie_id):
        """ì½˜í…ì¸  ê¸°ë°˜"""
        user_ratings = self.train[self.train['userId'] == user_id]
        
        if user_ratings.empty or movie_id not in self.movie_features:
            return self.mean_rating
        
        rated_movies = user_ratings['movieId'].values
        weighted_sum = 0.0
        similarity_sum = 0.0
        
        for rated_id in rated_movies:
            if rated_id in self.item_to_idx and movie_id in self.item_to_idx:
                idx_rated = self.item_to_idx[rated_id]
                idx_movie = self.item_to_idx[movie_id]
                similarity = self.item_similarity[idx_movie][idx_rated]
                rating = user_ratings[user_ratings['movieId'] == rated_id]['rating'].values[0]
                weighted_sum += similarity * rating
                similarity_sum += similarity
        
        if similarity_sum > 0:
            return weighted_sum / similarity_sum
        else:
            return self.mean_rating
    
    def predict_weighted_avg(self, user_id, movie_id, alpha=0.4):
        """ê°€ì¤‘ í‰ê· """
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        pred = alpha * cf + (1 - alpha) * cb
        return np.clip(pred, 1, 5)
    
    def predict_feature_combo(self, user_id, movie_id):
        """íŠ¹ì§• ê²°í•©"""
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        ub = self.user_bias.get(user_id, 0)
        mb = self.movie_bias.get(movie_id, 0)
        pred = 0.5*cf + 0.3*cb + 0.1*ub + 0.1*mb
        return np.clip(pred, 1, 5)
    
    def predict_mixed(self, user_id, movie_id):
        """í˜¼í•©"""
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        confidence = 0.5
        pred = confidence * cf + (1 - confidence) * cb
        return np.clip(pred, 1, 5)
    
    def get_recommendations(self, user_id, n=10, method='weighted_avg'):
        """ì¶”ì²œ ìƒì„±"""
        watched = set(self.train[self.train['userId'] == user_id]['movieId'])
        predictions = []
        
        for movie_id in self.movies['movieId']:
            if movie_id not in self.movie_features:
                continue
            
            if movie_id not in watched:
                if method == 'cf':
                    pred = self.predict_cf(user_id, movie_id)
                elif method == 'cb':
                    pred = self.predict_cb(user_id, movie_id)
                elif method == 'weighted_avg':
                    pred = self.predict_weighted_avg(user_id, movie_id)
                elif method == 'feature_combo':
                    pred = self.predict_feature_combo(user_id, movie_id)
                elif method == 'mixed':
                    pred = self.predict_mixed(user_id, movie_id)
                else:
                    pred = self.predict_weighted_avg(user_id, movie_id)
                
                if 1 <= pred <= 5:
                    predictions.append((movie_id, pred))
        
        predictions.sort(key=lambda x: x[1], reverse=True)
        return [p[0] for p in predictions[:n]]
    
    def evaluate(self):
        """ì¢…í•© í‰ê°€ - ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ í¬í•¨"""
        print(f"\nğŸ“ˆ {self.name} í‰ê°€ ì¤‘...")
        
        results = {}
        methods = {
            'CF': self.predict_cf,
            'CB': self.predict_cb,
            'Weighted_Avg': self.predict_weighted_avg,
            'Feature_Combo': self.predict_feature_combo,
            'Mixed': self.predict_mixed
        }
        
        for method_name, predict_func in methods.items():
            print(f"  ğŸ“Š {method_name} í‰ê°€ ì¤‘...")
            
            # Step 1: ê¸°ë³¸ ì •í™•ë„ ì§€í‘œ
            actuals = []
            preds = []
            
            for _, row in self.test.iterrows():
                user_id = row['userId']
                movie_id = row['movieId']
                actual = row['rating']
                pred = predict_func(user_id, movie_id)
                actuals.append(actual)
                preds.append(pred)
            
            # Step 2: Sparsity-Aware Score
            num_users = len(self.um.index)
            num_items = len(self.um.columns)
            sparsity_result = self.metrics.sparsity_aware_score(
                test_data=np.array(actuals),
                predictions=np.array(preds),
                num_users=num_users,
                num_items=num_items
            )
            
            # Step 3: ì¶”ì²œ ì§€í‘œ ê³„ì‚°
            test_users = list(self.test['userId'].unique())
            precisions = []
            recalls = []
            f1s = []
            maps = []
            mrrs = []
            ndcgs = []
            
            for user_id in test_users:
                user_test = self.test[self.test['userId'] == user_id]
                if len(user_test) < 1:
                    continue
                
                # ì¶”ì²œ ìƒì„±
                if method_name == 'CF':
                    recs = self.get_recommendations(user_id, n=10, method='cf')
                elif method_name == 'CB':
                    recs = self.get_recommendations(user_id, n=10, method='cb')
                elif method_name == 'Weighted_Avg':
                    recs = self.get_recommendations(user_id, n=10, method='weighted_avg')
                elif method_name == 'Feature_Combo':
                    recs = self.get_recommendations(user_id, n=10, method='feature_combo')
                elif method_name == 'Mixed':
                    recs = self.get_recommendations(user_id, n=10, method='mixed')
                else:
                    recs = []
                
                relevant = user_test[user_test['rating'] >= 4]['movieId'].tolist()
                
                if recs and relevant:
                    precisions.append(self.metrics.precision_at_k(recs, relevant, 10))
                    recalls.append(self.metrics.recall_at_k(recs, relevant, 10))
                    f1s.append(self.metrics.f1_at_k(recs, relevant, 10))
                    maps.append(self.metrics.map_at_k(recs, relevant, 10))
                    mrrs.append(self.metrics.mrr_at_k(recs, relevant, 10))
                    relevance = [1 if m in relevant else 0 for m in recs]
                    ndcgs.append(self.metrics.ndcg_at_k(relevance, 10))
            
            # Step 4: ë‹¤ì–‘ì„± ì§€í‘œ
            all_recs = []
            for user_id in test_users:
                if method_name == 'CF':
                    recs = self.get_recommendations(user_id, n=10, method='cf')
                elif method_name == 'CB':
                    recs = self.get_recommendations(user_id, n=10, method='cb')
                elif method_name == 'Weighted_Avg':
                    recs = self.get_recommendations(user_id, n=10, method='weighted_avg')
                elif method_name == 'Feature_Combo':
                    recs = self.get_recommendations(user_id, n=10, method='feature_combo')
                elif method_name == 'Mixed':
                    recs = self.get_recommendations(user_id, n=10, method='mixed')
                else:
                    recs = []
                
                if recs:
                    all_recs.append(recs)
            
            diversity = self.metrics.intra_list_diversity(
                [r for recs in all_recs for r in recs],
                self.item_similarity,
                self.item_to_idx
            )
            
            coverage = self.metrics.coverage(all_recs, len(self.movies))
            
            novelty = self.metrics.novelty(
                [r for recs in all_recs for r in recs],
                self.popularity
            )
            
            pop_bias = self.metrics.popularity_bias(
                [r for recs in all_recs for r in recs],
                self.popularity
            )
            
            # Step 5: ê²°ê³¼ ì €ì¥
            results[method_name] = {
                'RMSE': np.sqrt(mean_squared_error(actuals, preds)),
                'MAE': mean_absolute_error(actuals, preds),
                'Sparsity': sparsity_result['Sparsity'],
                'Adjusted_RMSE': sparsity_result['Adjusted_RMSE'],
                'Adjusted_MAE': sparsity_result['Adjusted_MAE'],
                'Precision@10': np.mean(precisions) if precisions else 0,
                'Recall@10': np.mean(recalls) if recalls else 0,
                'F1@10': np.mean(f1s) if f1s else 0,
                'MAP@10': np.mean(maps) if maps else 0,
                'MRR@10': np.mean(mrrs) if mrrs else 0,
                'NDCG@10': np.mean(ndcgs) if ndcgs else 0,
                'Diversity': diversity,
                'Coverage': coverage,
                'Novelty': novelty,
                'PopularityBias': pop_bias,
                'Num_Samples': len(precisions)
            }
            
            print(f"    âœ… {method_name}: RMSE={results[method_name]['RMSE']:.4f}")
        
        return results

# ================================
# ë©”ì¸ ì‹¤í–‰
# ================================

def main():
    # â­ ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ ì²˜ë¦¬
    datasets_to_test = ['Small', '1M']
    all_results = []
    
    for dataset in datasets_to_test:
        print(f"\n{'='*100}")
        print(f"ğŸ“Š {dataset} ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*100}")
        
        # ë°ì´í„° ë¡œë“œ
        ratings, movies = load_movielens(dataset)
        
        if ratings is None or movies is None:
            print(f"âŒ {dataset} ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
            continue
        
        # ëª¨ë¸ ìƒì„± ë° í‰ê°€
        model = OptimizedHybridRecommender(ratings, movies, name=dataset, svd_dim=200)
        model._prepare()
        results = model.evaluate()
        
        # ê²°ê³¼ ì €ì¥
        for method, metrics in results.items():
            metrics['Dataset'] = dataset
            metrics['Method'] = method
            all_results.append(metrics)
    
    # ê²°ê³¼ ì¶œë ¥
    if all_results:
        print(f"\n{'='*100}")
        print("ğŸ“ˆ ìµœì¢… ê²°ê³¼ (5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ Ã— 2 ë°ì´í„°ì…‹)")
        print(f"{'='*100}")
        
        results_df = pd.DataFrame(all_results)
        
        print("\nâœ… ê¸°ë³¸ ì •í™•ë„ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'RMSE', 'MAE', 'Sparsity', 'Adjusted_RMSE']].to_string(index=False))
        
        print("\nâœ… ì¶”ì²œ ì •í™•ì„± ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'Precision@10', 'Recall@10', 'F1@10', 'NDCG@10']].to_string(index=False))
        
        print("\nâœ… ìˆœìœ„ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'MAP@10', 'MRR@10', 'NDCG@10']].to_string(index=False))
        
        print("\nâœ… ë‹¤ì–‘ì„± ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'Diversity', 'Coverage', 'Novelty', 'PopularityBias']].to_string(index=False))
        
        # CSV ì €ì¥
        output_filename = 'hybrid_results_small_1m_ver7.csv'
        results_df.to_csv(output_filename, index=False)
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_filename}")
        
        # ë°ì´í„°ì…‹ë³„ ë¹„êµ
        print("\nğŸ“Š ë°ì´í„°ì…‹ë³„ ë¹„êµ:")
        print("-" * 100)
        
        for dataset in datasets_to_test:
            dataset_results = results_df[results_df['Dataset'] == dataset]
            
            if len(dataset_results) > 0:
                best_method = dataset_results.loc[dataset_results['RMSE'].idxmin(), 'Method']
                best_rmse = dataset_results['RMSE'].min()
                best_adj_rmse = dataset_results['Adjusted_RMSE'].min()
                best_precision = dataset_results['Precision@10'].max()
                
                print(f"\n{dataset}:")
                print(f"  ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ (RMSE): {best_method}")
                print(f"  ìµœì € RMSE: {best_rmse:.4f}")
                print(f"  ìµœì € Adjusted_RMSE: {best_adj_rmse:.4f}")
                print(f"  ìµœê³  Precision@10: {best_precision:.4f}")
    else:
        print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤")

if __name__ == "__main__":
    main()
