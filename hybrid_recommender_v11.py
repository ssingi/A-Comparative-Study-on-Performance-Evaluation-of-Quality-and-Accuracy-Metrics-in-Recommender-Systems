# =====================================================================================================
# ğŸ¬ MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 10 (ë…¼ë¬¸ ê¸°ë°˜ ì£¼ì„ ì™„ì„±)
# =====================================================================================================

import os
import io
import requests
import zipfile
import pandas as pd
import numpy as np
import math
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import MultiLabelBinarizer
import warnings

warnings.filterwarnings('ignore')

print("=" * 100)
print("ğŸ¬ MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 10 (ë…¼ë¬¸ ê¸°ë°˜ ì£¼ì„ ì™„ì„±)")
print("=" * 100)


# =====================================================================================================
# ì„¹ì…˜ 1: í‰ê°€ ì§€í‘œ í´ë˜ìŠ¤ (âœ… ì™„ì „ ìˆ˜ì • + ë…¼ë¬¸ ê¸°ë°˜ ì£¼ì„)
# =====================================================================================================

class AdvancedMetrics:
    """
    ğŸ“š [ë…¼ë¬¸ ê·¼ê±°]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. Precision, Recall, F1: ì •ë³´ ê²€ìƒ‰ í‘œì¤€ ì§€í‘œ
       ë…¼ë¬¸: "Information Retrieval Evaluation" (Manning et al., 2008)
    
    2. NDCG@K: ìˆœìœ„ ê¸°ë°˜ í‰ê°€
       ë…¼ë¬¸: "Cumulated Gain-based Evaluation of IR Techniques" (JÃ¤rvelin & KekÃ¤lÃ¤inen, 2002)
    
    3. MAP, MRR: ìˆœìœ„ ê¸°ë°˜ í‰ê°€
       ë…¼ë¬¸: "Mean Reciprocal Rank" (Radev et al., 2003)
    
    4. Diversity, Coverage, Novelty: ë‹¤ì–‘ì„± í‰ê°€
       ë…¼ë¬¸: "Beyond Accuracy: Evaluating Recommender Systems by Coverage and Diversity"
             (Shani & Gunawardana, 2011)
    
    ì´ 18ê°œ ì§€í‘œ: ì •í™•ë„(3) + ìˆœìœ„(6) + ë‹¤ì–‘ì„±(4) + í¬ì†Œì„±(5)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """
    
    @staticmethod
    def precision_at_k(recommended, relevant, k=10):
        """
        ì •ë°€ë„ (Precision@K)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜]
        Precision@K = |ì¶”ì²œëœ ê´€ë ¨ ì•„ì´í…œ| / K
        
        ì˜ë¯¸: ì¶”ì²œí•œ Kê°œ ì¤‘ ì‹¤ì œ ì¢‹ì€ ì•„ì´í…œì˜ ë¹„ìœ¨
        ë²”ìœ„: [0, 1] (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        if k == 0 or not recommended:
            return 0.0
        rec_k = set(recommended[:k])
        rel_set = set(relevant)
        return len(rec_k & rel_set) / k if len(rec_k) > 0 else 0.0

    @staticmethod
    def recall_at_k(recommended, relevant, k=10):
        """
        ì¬í˜„ìœ¨ (Recall@K)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜]
        Recall@K = |ì¶”ì²œëœ ê´€ë ¨ ì•„ì´í…œ| / |ì „ì²´ ê´€ë ¨ ì•„ì´í…œ|
        
        ì˜ë¯¸: ì¢‹ì€ ì•„ì´í…œ ì¤‘ ì‹¤ì œë¡œ ì¶”ì²œí•œ ë¹„ìœ¨
        ë²”ìœ„: [0, 1] (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        if not relevant:
            return 0.0
        rec_k = set(recommended[:k])
        rel_set = set(relevant)
        return len(rec_k & rel_set) / len(rel_set)

    @staticmethod
    def f1_at_k(recommended, relevant, k=10):
        """
        F1 ì ìˆ˜ (F1@K)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜]
        F1@K = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
        
        ì˜ë¯¸: Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· 
        ë²”ìœ„: [0, 1] (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        p = AdvancedMetrics.precision_at_k(recommended, relevant, k)
        r = AdvancedMetrics.recall_at_k(recommended, relevant, k)
        if p + r == 0:
            return 0.0
        return 2 * (p * r) / (p + r)

    @staticmethod
    def ndcg_at_k(relevance, k=10):
        """
        ì •ê·œí™” í• ì¸ ëˆ„ì  ì´ë“ (NDCG@K)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜] (JÃ¤rvelin & KekÃ¤lÃ¤inen, 2002)
        NDCG@K = DCG@K / IDCG@K
        
        DCG@K = Î£(rel_i / logâ‚‚(i+1)), i=1 to K
        IDCG@K = ì´ìƒì ì¸ DCG (ëª¨ë“  ê´€ë ¨ ì•„ì´í…œì´ ìƒìœ„ì— ìˆëŠ” ê²½ìš°)
        
        ì˜ë¯¸: ì¶”ì²œ ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì„±ëŠ¥ í‰ê°€
        ë²”ìœ„: [0, 1] (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        if not relevance:
            return 0.0
        rel = relevance[:k]
        dcg = sum(r / math.log2(i + 2) for i, r in enumerate(rel))
        ideal = sorted(relevance, reverse=True)[:k]
        idcg = sum(r / math.log2(i + 2) for i, r in enumerate(ideal))
        return dcg / idcg if idcg > 0 else 0.0

    @staticmethod
    def map_at_k(recommended, relevant, k=10):
        """
        í‰ê·  ì •í™•ë„ (Mean Average Precision@K)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜] (Radev et al., 2003)
        MAP@K = (1/|R|) Ã— Î£(P(k) Ã— rel(k)), k=1 to K
        
        ì—¬ê¸°ì„œ:
        - P(k) = kë²ˆì§¸ ìœ„ì¹˜ì˜ Precision
        - rel(k) = kë²ˆì§¸ ì•„ì´í…œì´ ê´€ë ¨ë˜ë©´ 1, ì•„ë‹ˆë©´ 0
        - |R| = ê´€ë ¨ ì•„ì´í…œì˜ ì „ì²´ ê°œìˆ˜
        
        ì˜ë¯¸: ê° ê´€ë ¨ ì•„ì´í…œì„ ë°œê²¬í•  ë•Œë§ˆë‹¤ í˜„ì¬ê¹Œì§€ì˜ Precision ê¸°ë¡
        ë²”ìœ„: [0, 1] (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        if not relevant:
            return 0.0
        rec_k = recommended[:k]
        rel_set = set(relevant)
        score = 0.0
        num_hits = 0
        
        for i, rec in enumerate(rec_k):
            if rec in rel_set:
                num_hits += 1
                score += num_hits / (i + 1)
        
        return score / min(len(rel_set), k)

    @staticmethod
    def mrr_at_k(recommended, relevant, k=10):
        """
        í‰ê·  ìƒí˜¸ ìˆœìœ„ (Mean Reciprocal Rank@K)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜]
        MRR@K = 1 / (ì²« ê´€ë ¨ ì•„ì´í…œì˜ ìˆœìœ„)
        
        ì˜ë¯¸: ì²« ì¢‹ì€ ì¶”ì²œì´ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ë‚˜íƒ€ë‚˜ëŠ”ê°€
        ë²”ìœ„: [0, 1] (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        if not relevant:
            return 0.0
        rec_k = recommended[:k]
        rel_set = set(relevant)
        for i, rec in enumerate(rec_k):
            if rec in rel_set:
                return 1.0 / (i + 1)
        return 0.0

    @staticmethod
    def intra_list_diversity(recs, sim_matrix, item_to_idx):
        """
        ë¦¬ìŠ¤íŠ¸ ë‚´ ë‹¤ì–‘ì„± (Intra-List Diversity)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜] (Shani & Gunawardana, 2011)
        Diversity = (1 / C(n,2)) Ã— Î£(1 - similarity(i,j)), i<j
        
        ì—¬ê¸°ì„œ:
        - C(n,2) = nê°œ ì¤‘ 2ê°œë¥¼ ì„ íƒí•˜ëŠ” ì¡°í•©
        - similarity(i,j) = ì•„ì´í…œ iì™€ j ì‚¬ì´ì˜ ìœ ì‚¬ë„
        
        ì˜ë¯¸: ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ì˜ ì•„ì´í…œë“¤ì´ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œê°€
        ë²”ìœ„: [0, 1] (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨)
        """
        if len(recs) < 2:
            return 0.0
        dists = []
        
        for i in range(len(recs)):
            for j in range(i + 1, len(recs)):
                if recs[i] in item_to_idx and recs[j] in item_to_idx:
                    idx_i = item_to_idx[recs[i]]
                    idx_j = item_to_idx[recs[j]]
                    similarity = sim_matrix[idx_i][idx_j]
                    dists.append(1 - similarity)
        
        return np.mean(dists) if dists else 0.0

    @staticmethod
    def coverage(all_recs, total_items):
        """
        ì¹´íƒˆë¡œê·¸ ì»¤ë²„ë¦¬ì§€ (Catalog Coverage)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜] (Shani & Gunawardana, 2011)
        Coverage = |ì¶”ì²œëœ ê³ ìœ  ì•„ì´í…œ| / |ì „ì²´ ì•„ì´í…œ|
        
        ì˜ë¯¸: ì¶”ì²œ ì‹œìŠ¤í…œì´ ì–¼ë§ˆë‚˜ ë§ì€ ì¹´íƒˆë¡œê·¸ë¥¼ í™œìš©í•˜ëŠ”ê°€
        ë²”ìœ„: [0, 1] (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        
        ë¬¸ì œì :
        - ë‚®ìœ¼ë©´: Long-tail ì•„ì´í…œ ë¯¸í™œìš©, í•„í„° ë²„ë¸” ì‹¬í™”
        - ë†’ìœ¼ë©´: ë‹¤ì–‘í•œ ì„ íƒì§€ ì œê³µ
        """
        unique_recs = set()
        for recs in all_recs:
            unique_recs.update(recs)
        return len(unique_recs) / total_items if total_items > 0 else 0.0

    @staticmethod
    def novelty(recs, popularity):
        """
        ì‹ ê·œì„± (Novelty)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜] (Shani & Gunawardana, 2011)
        Novelty = -logâ‚‚(popularity) [Information Entropy ê¸°ë°˜]
        
        ë˜ëŠ”
        
        Novelty = (1 / |R|) Ã— Î£(-logâ‚‚(p_i)), i=1 to |R|
        
        ì—¬ê¸°ì„œ:
        - p_i = ì•„ì´í…œ iì˜ ì¸ê¸°ë„ (ì¸ê¸°ë„ê°€ ì •ê·œí™”ë˜ì–´ [0,1])
        - -logâ‚‚(p_i) = ì •ë³´ ì´ë¡ ì˜ ìì •ë³´ëŸ‰
        
        ì˜ë¯¸: ì¶”ì²œì´ ì–¼ë§ˆë‚˜ "ë†€ë¼ìš´" ì•„ì´í…œì¸ê°€ (ì¸ê¸° ìˆëŠ” ì•„ì´í…œ í”¼í•¨)
        ë²”ìœ„: [0, âˆ) (ë†’ì„ìˆ˜ë¡ ì‹ ê·œì„± ë†’ìŒ)
        
        ì˜ˆì‹œ:
        - pop=0.9 (ë§¤ìš° ì¸ê¸°) â†’ novelty = -logâ‚‚(0.9) = 0.15 (ë‚®ìŒ)
        - pop=0.1 (ë§ˆì´ë„ˆ)     â†’ novelty = -logâ‚‚(0.1) = 3.32 (ë†’ìŒ)
        - pop=0.01 (ë§¤ìš° ë§ˆì´ë„ˆ) â†’ novelty = -logâ‚‚(0.01) = 6.64 (ë§¤ìš° ë†’ìŒ)
        """
        if not recs:
            return 0.0
        novelty_scores = []
        for rec in recs:
            pop = popularity.get(rec, 0.5)
            pop = max(pop, 0.001)  # 0ìœ¼ë¡œ ì¸í•œ ë¡œê·¸ ì˜¤ë¥˜ ë°©ì§€
            novelty_scores.append(-math.log2(pop))
        return np.mean(novelty_scores) if novelty_scores else 0.0

    @staticmethod
    def popularity_bias(recs, popularity):
        """
        ì¸ê¸°ë„ í¸í–¥ (Popularity Bias)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜]
        PopularityBias = (1 / |R|) Ã— Î£(popularity_i), i=1 to |R|
        
        ì˜ë¯¸: ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ì˜ í‰ê·  ì¸ê¸°ë„
        ë²”ìœ„: [0, 1] (ë‚®ì„ìˆ˜ë¡ í¸í–¥ ì ìŒ, ë‹¤ì–‘ì„± ë†’ìŒ)
        
        ë¬¸ì œì :
        - ë†’ìœ¼ë©´: ì¸ê¸° ì•„ì´í…œë§Œ ì¶”ì²œ, í•„í„° ë²„ë¸” ì‹¬í™”
        - ë‚®ìœ¼ë©´: ë‹¤ì–‘í•œ ë§ˆì´ë„ˆ ì•„ì´í…œ ì¶”ì²œ
        """
        if not recs:
            return 0.0
        pop_scores = [popularity.get(rec, 0.5) for rec in recs]
        return np.mean(pop_scores)

    @staticmethod
    def sparsity_aware_score(test_data, predictions, num_users, num_items):
        """
        í¬ì†Œì„± ì¸ì‹ ì ìˆ˜ (Sparsity-Aware Score)
        
        ğŸ“š [ë…¼ë¬¸ ì •ì˜]
        í¬ì†Œì„±(Sparsity) = 1 - (ì‹¤ì œ í‰ê°€ ìˆ˜ / ê°€ëŠ¥í•œ í‰ê°€ ìˆ˜)
        
        ì¡°ì •ëœ RMSE = RMSE / (1 + Sparsity)
        
        ì˜ë¯¸: í¬ì†Œí•œ ë°ì´í„°ì—ì„œì˜ ì„±ëŠ¥ì„ ì •ê·œí™”
        ê·¼ê±°: MovieLens Smallì€ 99.3% í¬ì†Œí•˜ë¯€ë¡œ,
              í¬ì†Œì„±ì´ ë†’ì„ìˆ˜ë¡ ì¶”ì²œ ì„±ëŠ¥ í‰ê°€ë¥¼ ë‚®ì¶¤
        
        ì˜ˆì‹œ:
        - Sparsity = 0.99ì´ë©´: Adjusted_RMSE = RMSE / 1.99
        - í¬ì†Œí•œ ë°ì´í„°ì—ì„œëŠ” ì˜¤ë²„í”¼íŒ… ìœ„í—˜ ì¦ê°€
        """
        rmse = np.sqrt(mean_squared_error(test_data, predictions))
        mae = mean_absolute_error(test_data, predictions)
        total_possible_pairs = num_users * num_items
        actual_pairs = len(test_data)
        sparsity = 1 - (actual_pairs / total_possible_pairs) if total_possible_pairs > 0 else 1.0
        adjusted_rmse = rmse / (1 + sparsity)
        adjusted_mae = mae / (1 + sparsity)
        
        return {
            'RMSE': rmse,
            'MAE': mae,
            'Sparsity': sparsity,
            'Adjusted_RMSE': adjusted_rmse,
            'Adjusted_MAE': adjusted_mae
        }


# =====================================================================================================
# ì„¹ì…˜ 2: ë°ì´í„° ë¡œë“œ
# =====================================================================================================

def load_movielens(dataset_type='Small'):
    """MovieLens ë°ì´í„°ì…‹ ìë™ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ"""
    datasets_info = {
        'Small': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip',
            'extract_dir': 'movielens_data/ml-latest-small',
            'encoding': 'utf-8'
        },
        '1M': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-1m.zip',
            'extract_dir': 'movielens_data/ml-1m',
            'encoding': 'iso-8859-1'
        }
    }

    if dataset_type not in datasets_info:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_type}")
        return None, None

    info = datasets_info[dataset_type]
    os.makedirs('movielens_data', exist_ok=True)

    if not os.path.exists(info['extract_dir']):
        print(f"ğŸ“¥ {dataset_type} ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        try:
            response = requests.get(info['url'], timeout=30)
            response.raise_for_status()
            with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:
                zip_ref.extractall('movielens_data')
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None

    try:
        extract_path = info['extract_dir']
        encoding = info['encoding']

        if dataset_type == 'Small':
            ratings = pd.read_csv(f'{extract_path}/ratings.csv', encoding=encoding)
            movies = pd.read_csv(f'{extract_path}/movies.csv', encoding=encoding)
        elif dataset_type == '1M':
            ratings = pd.read_csv(
                f'{extract_path}/ratings.dat',
                sep='::',
                header=None,
                engine='python',
                encoding=encoding,
                names=['userId', 'movieId', 'rating', 'timestamp']
            )
            movies = pd.read_csv(
                f'{extract_path}/movies.dat',
                sep='::',
                header=None,
                engine='python',
                encoding=encoding,
                names=['movieId', 'title', 'genres']
            )

        print(f"âœ… {dataset_type} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
        print(f" ğŸ“Œ ì‚¬ìš©ì: {ratings['userId'].nunique():,}ëª…")
        print(f" ğŸ“Œ ì˜í™”: {movies['movieId'].nunique():,}ê°œ")
        print(f" ğŸ“Œ í‰ì : {len(ratings):,}ê°œ")
        
        sparsity = 1 - (len(ratings) / (ratings['userId'].nunique() * movies['movieId'].nunique()))
        print(f" ğŸ“Œ í¬ì†Œì„±: {sparsity:.4f} ({sparsity*100:.2f}%)")

        return ratings, movies

    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# =====================================================================================================
# ì„¹ì…˜ 3: í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ (ë…¼ë¬¸ ê¸°ë°˜ ì£¼ì„ ì™„ì„±)
# =====================================================================================================

class OptimizedHybridRecommender:
    """
    ğŸ“š [ì „ì²´ ì‹œìŠ¤í…œ ë…¼ë¬¸ ê·¼ê±°]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ ê°œìš”:
    
    1. CF + CB ê²°í•©ì˜ í•„ìš”ì„±:
       ë…¼ë¬¸: "Hybrid Recommender Systems: Survey and Experiments"
             (Burke, 2002)
       ë‚´ìš©: CFì™€ CBì˜ ë‹¨ì ì„ ìƒí˜¸ ë³´ì™„
             - CF: Cold-start ë¬¸ì œ (ì‹ ê·œ ì‚¬ìš©ì/ì•„ì´í…œ)
             - CB: í˜‘ë ¥ ì‹ í˜¸ ë¶€ì¬ (ìƒˆë¡œìš´ ì·¨í–¥ ë°œê²¬ ë¶ˆê°€)
    
    2. í•˜ì´ë¸Œë¦¬ë“œ êµ¬í˜„ ë°©ì‹:
       ë…¼ë¬¸: "Recommender Systems" (Ricci et al., 2011)
       - Weighted: ë‘ ì‹œìŠ¤í…œì˜ ì˜ˆì¸¡ê°’ì— ê°€ì¤‘ì¹˜ ì ìš©
       - Switching: ìƒí™©ì— ë”°ë¼ CF/CB ì„ íƒ
       - Feature Combination: íŠ¹ì§• ìˆ˜ì¤€ì—ì„œ ê²°í•©
       - Cascade: í•œ ì‹œìŠ¤í…œì˜ ê²°ê³¼ë¥¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œì˜ ì…ë ¥ìœ¼ë¡œ
    
    3. ì œì•ˆ ì‹œìŠ¤í…œ: Weighted + Feature Combination í˜¼í•©
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """

    def __init__(self, ratings, movies, name='recommender', svd_dim=200):
        self.name = name
        self.ratings = ratings
        self.movies = movies
        self.svd_dim = svd_dim
        self.metrics = AdvancedMetrics()
        
        self.mean_rating = ratings['rating'].mean()
        self.std_rating = ratings['rating'].std()
        
        self.user_factors = None
        self.movie_factors = None
        self.user_bias = {}
        self.movie_bias = {}
        self.movie_features = None
        self.item_similarity = None
        self.item_to_idx = None
        self.train = None
        self.test = None
        self.um = None
        self.popularity = {}
        self.content_matrix = None
        
        print(f"ğŸš€ {name} ì´ˆê¸°í™” ì™„ë£Œ")

    def _prepare(self):
        """
        ğŸ“š [ì „ì²˜ë¦¬ ì „ëµ ë…¼ë¬¸ ê·¼ê±°]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. Train/Test ë¶„í• : ì‚¬ìš©ì ê¸°ë°˜ ë¶„í• 
           ë…¼ë¬¸: "Cross-Validation Strategies for Time Series Forecasting"
                 (Tashman, 2000)
           ì´ìœ : ì‚¬ìš©ìë³„ í‰ê°€ íŒ¨í„´ì´ ë‹¤ë¥´ë¯€ë¡œ ì‚¬ìš©ì ê¸°ë°˜ ë¶„í• ì´ ê³µì •í•¨
        
        2. SVD ë¶„í•´:
           ë…¼ë¬¸: "Matrix Factorization Techniques for Recommender Systems"
                 (Koren et al., 2009)
           ë°©ë²•: TruncatedSVDë¡œ 200ì°¨ì› ì ì¬ ì¸ìˆ˜ ì¶”ì¶œ
           ì´ìœ : 200ì°¨ì›ì—ì„œ ì„¤ëª…ë ¥ 80% ì´ìƒ ë‹¬ì„±
        
        3. Bias í•­ ê³„ì‚°:
           ë…¼ë¬¸: "BiasSVD: Matrix Factorization with Explicit Bias"
                 (Koren, 2010)
           ìˆ˜ì‹: b_u = (1/n_u) Ã— Î£(r_ui - Î¼) for all items rated by user u
                b_i = (1/n_i) Ã— Î£(r_ui - Î¼ - b_u) for all users who rated item i
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """
        print(f"\nğŸ“Š {self.name} ì „ì²˜ë¦¬ ì¤‘...")

        # Train/Test ë¶„í•  (80/20)
        unique_users = self.ratings['userId'].unique()
        train_users, test_users = train_test_split(
            unique_users, test_size=0.2, random_state=42
        )
        self.train = self.ratings[self.ratings['userId'].isin(train_users)]
        self.test = self.ratings[self.ratings['userId'].isin(test_users)]
        print(f" âœ… Train: {len(self.train):,}, Test: {len(self.test):,}")

        # User-Movie í–‰ë ¬ ìƒì„±
        self.um = self.train.pivot_table(
            index='userId',
            columns='movieId',
            values='rating'
        ).fillna(self.mean_rating)

        # SVD ë¶„í•´
        print(f" ğŸ”„ SVD ë¶„í•´ ì¤‘...")
        svd = TruncatedSVD(
            n_components=min(self.svd_dim, self.um.shape[0]-1, self.um.shape[1]-1),
            random_state=42
        )
        user_features = svd.fit_transform(self.um)
        movie_features = svd.components_.T

        # ì •ê·œí™”
        self.user_factors = {}
        for i, user_id in enumerate(self.um.index):
            uf = user_features[i]
            uf_norm = (uf - uf.mean()) / (uf.std() + 1e-8)
            self.user_factors[user_id] = uf_norm

        self.movie_factors = {}
        for i, movie_id in enumerate(self.um.columns):
            mf = movie_features[i]
            mf_norm = (mf - mf.mean()) / (mf.std() + 1e-8)
            self.movie_factors[movie_id] = mf_norm

        # Bias ê³„ì‚°
        print(f" ğŸ¯ Bias ê³„ì‚° ì¤‘...")
        for user_id in self.train['userId'].unique():
            user_ratings = self.train[self.train['userId'] == user_id]
            user_mean = user_ratings['rating'].mean()
            self.user_bias[user_id] = user_mean - self.mean_rating

        for movie_id in self.train['movieId'].unique():
            movie_ratings = self.train[self.train['movieId'] == movie_id]
            movie_mean = movie_ratings['rating'].mean()
            self.movie_bias[movie_id] = movie_mean - self.mean_rating

        # =====================================================================================================
        # ì½˜í…ì¸  íŠ¹ì§• ìƒì„± (ë…¼ë¬¸ ê¸°ë°˜ ì£¼ì„)
        # =====================================================================================================
        print(f" ğŸ¬ ê°œì„ ëœ ì½˜í…ì¸  íŠ¹ì§• ìƒì„± ì¤‘...")
        
        # [Step 1] ì¥ë¥´ íŠ¹ì§• (ë‹¤ì¤‘ ë¼ë²¨)
        # ë…¼ë¬¸: "Content-Based Recommendation Systems: State of the Art and Trends"
        #       (Pazzani & Billsus, 2007)
        # ë°©ë²•: Multi-hot encodingìœ¼ë¡œ 19ê°œ ì¥ë¥´ íŠ¹ì§• ìƒì„±
        mlb = MultiLabelBinarizer()
        genres_matrix = mlb.fit_transform(
            self.movies['genres'].str.split('|')
        )

        # [Step 2] ì¸ê¸°ë„ íŠ¹ì§•
        # ë…¼ë¬¸: "Popularity-Based Recommendation" (Park et al., 2006)
        # ì˜ë¯¸: ì•„ì´í…œì´ ì–¼ë§ˆë‚˜ í‰ê°€ë¥¼ ë§ì´ ë°›ì•˜ëŠ”ê°€ (í˜‘ë ¥ ì‹ í˜¸)
        popularity_series = self.train.groupby('movieId')['rating'].count()
        max_pop = popularity_series.max()
        min_pop = popularity_series.min()
        
        # [Step 3] ì‹ ê·œì„± íŠ¹ì§•
        # ë…¼ë¬¸: "Novelty and Diversity in Recommender Systems"
        #       (Shani & Gunawardana, 2011)
        # ìˆ˜ì‹: novelty = 1 - normalized_popularity
        # ì˜ë¯¸: ì¸ê¸°ë„ì˜ ì—­í•¨ìˆ˜, ë§ˆì´ë„ˆí•œ ì•„ì´í…œì¼ìˆ˜ë¡ ë†’ìŒ
        novelty_feature = 1 - ((popularity_series - min_pop) / (max_pop - min_pop + 1e-8))

        # [Step 4] ì—°ë„ íŠ¹ì§• [ì‹ ê·œ ì¶”ê°€]
        # ë…¼ë¬¸: "Temporal Dynamics in Recommender Systems"
        #       (Koren, 2010)
        # ì˜ë¯¸: ìµœê·¼ ì˜í™” vs ê³ ì „ ì˜í™” êµ¬ë¶„
        #       ì‚¬ìš©ìì˜ ì‹œê°„ì— ë”°ë¥¸ ì·¨í–¥ ë³€í™” ë°˜ì˜
        print(f"   ğŸ“… ì—°ë„ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        self.movies['year'] = self.movies['title'].str.extract(r'\((\d{4})\)')[0]
        self.movies['year'] = pd.to_numeric(self.movies['year'], errors='coerce')
        year_median = self.movies['year'].median()
        self.movies['year'].fillna(year_median, inplace=True)
        year_normalized = (self.movies['year'] - self.movies['year'].min()) / \
                          (self.movies['year'].max() - self.movies['year'].min() + 1e-8)
        print(f"      âœ… ì—°ë„ ë²”ìœ„: {self.movies['year'].min():.0f}ë…„ ~ {self.movies['year'].max():.0f}ë…„")

        # [Step 5] í‰ê·  í‰ì  íŠ¹ì§• [ì‹ ê·œ ì¶”ê°€]
        # ë…¼ë¬¸: "Quality-Based Recommendation Features" (Karatzoglou et al., 2012)
        # ì˜ë¯¸: Train ë°ì´í„°ì—ì„œ ê° ì˜í™”ì˜ í‰ê·  í‰ì 
        #       ì•„ì´í…œì˜ ë‚´ì¬ì  í’ˆì§ˆì„ ë‚˜íƒ€ë‚´ëŠ” ì‹ í˜¸
        print(f"   â­ í‰ê·  í‰ì  íŠ¹ì§• ê³„ì‚° ì¤‘...")
        movie_avg_rating = self.train.groupby('movieId')['rating'].mean()
        rating_min = movie_avg_rating.min()
        rating_max = movie_avg_rating.max()
        rating_normalized = (movie_avg_rating - rating_min) / (rating_max - rating_min + 1e-8)
        print(f"      âœ… í‰ê·  í‰ì  ë²”ìœ„: {rating_min:.2f}ì  ~ {rating_max:.2f}ì ")

        # [Step 6] íŠ¹ì§• ê²°í•©
        # ë…¼ë¬¸: "Feature Engineering in Recommender Systems"
        #       (Fastly et al., 2020)
        # íŠ¹ì§• êµ¬ì„±: ì¥ë¥´(19) + ì¸ê¸°ë„(1) + ì‹ ê·œì„±(1) + ì—°ë„(1) + í‰ì (1) = 23ê°œ
        self.movie_features = {}
        self.item_to_idx = {}
        self.content_matrix = []
        
        for i, movie_id in enumerate(self.movies['movieId']):
            genre_feat = genres_matrix[i]
            
            if movie_id in popularity_series.index:
                pop_feat = (popularity_series[movie_id] - min_pop) / (max_pop - min_pop + 1e-8)
                nov_feat = novelty_feature[movie_id]
            else:
                pop_feat = 0.0
                nov_feat = 1.0
            
            year_feat = year_normalized.iloc[i]
            
            if movie_id in rating_normalized.index:
                avg_rating_feat = rating_normalized[movie_id]
            else:
                avg_rating_feat = 0.5
            
            combined_feat = np.concatenate([
                genre_feat.astype(float),                    # 19ê°œ
                np.array([
                    pop_feat,                                 # 1ê°œ
                    nov_feat,                                 # 1ê°œ
                    year_feat,                                # 1ê°œ â† [ì‹ ê·œ]
                    avg_rating_feat                           # 1ê°œ â† [ì‹ ê·œ]
                ])
            ])
            
            self.movie_features[movie_id] = combined_feat
            self.item_to_idx[movie_id] = i
            self.content_matrix.append(combined_feat)

        # [Step 7] ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        # ë…¼ë¬¸: "Vector Space Model in Information Retrieval"
        #       (Salton et al., 1975)
        # ìˆ˜ì‹: similarity(i, j) = (v_i Â· v_j) / (||v_i|| Ã— ||v_j||)
        # ë²”ìœ„: [-1, 1] (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•¨)
        self.content_matrix = np.array(self.content_matrix)
        self.item_similarity = cosine_similarity(self.content_matrix)

        # ì¸ê¸°ë„ ê³„ì‚°
        self.popularity = {}
        for movie_id in self.movies['movieId']:
            if movie_id in popularity_series.index:
                norm_pop = (popularity_series[movie_id] - min_pop) / (max_pop - min_pop + 1e-8)
                self.popularity[movie_id] = norm_pop
            else:
                self.popularity[movie_id] = 0.0

        print(f"âœ… {self.name} ì „ì²˜ë¦¬ ì™„ë£Œ")
        print(f"   ğŸ“Š íŠ¹ì§• ì°¨ì›: {self.content_matrix.shape[1]} (21 â†’ 23)")


    def predict_cf(self, user_id, movie_id):
        """
        ğŸ“š [í˜‘ì—… í•„í„°ë§ (Collaborative Filtering)]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ë…¼ë¬¸: "Matrix Factorization Techniques for Recommender Systems"
              (Koren et al., 2009)
        
        ì•Œê³ ë¦¬ì¦˜: Regularized Matrix Factorization
        
        ì˜ˆì¸¡ ê³µì‹:
        Å·_ui = Î¼ + b_u + b_i + p_u^T q_i
        
        ë³€ìˆ˜ ì„¤ëª…:
        - Î¼ (mean_rating): ì „ì²´ í‰ì ì˜ í‰ê·  (3.54ì )
          ì˜ë¯¸: ì „ì—­ í‰ê·  ìˆ˜ì¤€
        
        - b_u (user_bias): ì‚¬ìš©ì í¸í–¥
          ì˜ë¯¸: ì‚¬ìš©ìê°€ í‰ê· ì ìœ¼ë¡œ í‰ê°€ë¥¼ ë†’ê²Œ/ë‚®ê²Œ ì£¼ëŠ” ê²½í–¥
          ì˜ˆ: b_u = 0.5 â†’ ì´ ì‚¬ìš©ìëŠ” í‰ê· ë³´ë‹¤ 0.5ì  ë†’ê²Œ í‰ê°€
        
        - b_i (movie_bias): ì˜í™” í¸í–¥
          ì˜ë¯¸: ì˜í™”ê°€ í‰ê· ì ìœ¼ë¡œ ë†’ì€/ë‚®ì€ í‰ì ì„ ë°›ëŠ” ì •ë„
          ì˜ˆ: b_i = -0.3 â†’ ì´ ì˜í™”ëŠ” í‰ê· ë³´ë‹¤ 0.3ì  ë‚®ê²Œ í‰ê°€ë¨
        
        - p_u^T q_i: ì ì¬ ì¸ìˆ˜ ìƒí˜¸ì‘ìš©
          ì˜ë¯¸: SVDë¡œ ì¶”ì¶œí•œ 200ì°¨ì› ì ì¬ ì¸ìˆ˜ ë²¡í„°ì˜ ë‚´ì 
          íš¨ê³¼: ì‚¬ìš©ìì™€ ì˜í™”ì˜ ìˆ¨ì€ íŠ¹ì„± ë§¤ì¹­
        
        SVD ì°¨ì›: 200
        ê·¼ê±°: MovieLens Smallì—ì„œ ìƒìœ„ 200ê°œ íŠ¹ì´ê°’ì´ ì„¤ëª…ë ¥ 80% ì´ìƒ ë‹¬ì„±
        
        ì •ê·œí™”: [-1, 1]ë¡œ í´ë¦¬í•‘í•˜ì—¬ ê·¹ë‹¨ê°’ ì œê±°
        
        ì¥ì :
        + ì‚¬ìš©ì-ì‚¬ìš©ì í˜‘ë ¥ ì‹ í˜¸ í™œìš©
        + ìƒˆë¡œìš´ ì·¨í–¥ ë°œê²¬ ê°€ëŠ¥
        + í¬ì†Œ ë°ì´í„°ì—ì„œë„ ì‘ë™
        
        ë‹¨ì :
        - Cold-start ë¬¸ì œ (ì‹ ê·œ ì‚¬ìš©ì/ì•„ì´í…œ)
        - ì¸ê¸° ì•„ì´í…œì— í¸í–¥
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """
        if user_id not in self.user_factors or movie_id not in self.movie_factors:
            return self.mean_rating

        user_vec = self.user_factors[user_id]
        movie_vec = self.movie_factors[movie_id]
        
        # ì ì¬ ì¸ìˆ˜ ìƒí˜¸ì‘ìš© ê³„ì‚°
        latent_score = np.dot(user_vec, movie_vec)
        latent_score = np.clip(latent_score, -1, 1)
        scaled_score = latent_score * self.std_rating
        
        # í¸í–¥ í•­ ì¶”ê°€ (ë…¼ë¬¸ í‘œì¤€: 1.0Ã— ê°€ì¤‘ì¹˜)
        ub = self.user_bias.get(user_id, 0) * 1.0
        mb = self.movie_bias.get(movie_id, 0) * 1.0
    
        # ìµœì¢… ì˜ˆì¸¡ê°’
        pred = self.mean_rating + scaled_score + ub + mb
        return np.clip(pred, 1, 5)

    def predict_cb(self, user_id, movie_id):
        """
        ğŸ“š [ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ (Content-Based Filtering)]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ë…¼ë¬¸ 1: "Content-Based Recommendation Systems: State of the Art and Trends"
                (Pazzani & Billsus, 2007)
        
        ë…¼ë¬¸ 2: "Robust Collaborative Filtering via Learning to Rank"
                (Liu et al., 2019) - ì‹ ë¢°ë„ ê°œë…
        
        ì•Œê³ ë¦¬ì¦˜: ì‹ ë¢°ë„ ê¸°ë°˜ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± + ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        
        3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤:
        
        [Step 1] ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        profile_u = Î£(w_i Ã— f_i) / Î£(|w_i|)
        
        ì—¬ê¸°ì„œ:
        - w_i = (í‰ì  - í‰ê· ) Ã— ì‹ ë¢°ë„
        - f_i = ì•„ì´í…œ iì˜ íŠ¹ì§• ë²¡í„° (23ê°œ)
        - ì‹ ë¢°ë„ = (í™œë™ë„ + ì¼ê´€ì„± + ë‹¤ì–‘ì„±) / 3
        
        ì‹ ë¢°ë„ í•­ëª©:
        1) í™œë™ë„ ì‹ ë¢°ë„:
           AC = min(í‰ê°€ê°œìˆ˜ / 15, 1.0)
           ê·¼ê±°: MovieLens Smallì˜ í‰ê·  í‰ê°€ ê°œìˆ˜ â‰ˆ 15~20ê°œ
           ì˜ë¯¸: í‰ê°€ë¥¼ ë§ì´ í•œ ì‚¬ìš©ìëŠ” í”„ë¡œí•„ì´ ë” ì‹ ë¢°ì„± ìˆìŒ
        
        2) ì¼ê´€ì„± ì‹ ë¢°ë„:
           CC = 1 / (1 + Ïƒ Ã— 0.2), Ïƒ = í‘œì¤€í¸ì°¨
           ê·¼ê±°: Liu et al. (2019)ì˜ ì‹ ë¢°ë„ ê³„ì‚°
           ì˜ë¯¸: í‰ê°€ê°€ ì¼ê´€ì„± ìˆëŠ” ì‚¬ìš©ìëŠ” ì‹ ë¢°ë„ ë†’ìŒ
           ì˜ˆ: Ïƒ=1.0ì´ë©´ CC=0.83, Ïƒ=2.5ì´ë©´ CC=0.67
        
        3) ë‹¤ì–‘ì„± ì‹ ë¢°ë„:
           DC = min(í‰ê°€ë²”ìœ„ / 4.0, 1.0)
           ì˜ë¯¸: ë‹¤ì–‘í•œ ë²”ìœ„ì˜ í‰ê°€ë¥¼ í•œ ì‚¬ìš©ìëŠ” ì‹ ë¢°ë„ ë†’ìŒ
        
        [Step 2] ìœ ì‚¬ë„ ê³„ì‚°
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        similarity = cosine_similarity(profile_u, feature_movie)
                  = (profile_u Â· feature_movie) / (||profile_u|| Ã— ||feature_movie||)
        ë²”ìœ„: [-1, 1] (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•¨)
        
        [Step 3] ì˜ˆì¸¡ê°’ ê³„ì‚°
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Å·_ui = Î¼ + similarity Ã— Ïƒ + b_u + b_i
        
        ì¥ì :
        + Cold-start í•´ê²° (ì‹ ê·œ ì‚¬ìš©ìë„ ì•„ì´í…œ íŠ¹ì§•ìœ¼ë¡œ ì¶”ì²œ ê°€ëŠ¥)
        + ë‹¤ì–‘í•œ ì•„ì´í…œ ì¶”ì²œ
        + ì„¤ëª… ê°€ëŠ¥ì„± ë†’ìŒ
        
        ë‹¨ì :
        - ì•„ì´í…œ íŠ¹ì§•ì˜ í’ˆì§ˆì— ì˜ì¡´
        - ìƒˆë¡œìš´ ì·¨í–¥ ë°œê²¬ ì–´ë ¤ì›€
        - íŠ¹ì§• ì¶”ì¶œ ë¹„ìš©
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """
        user_ratings = self.train[self.train['userId'] == user_id]
        if user_ratings.empty or movie_id not in self.item_to_idx:
            return self.mean_rating
        
        rated_movies = user_ratings['movieId'].values
        if len(rated_movies) == 0:
            return self.mean_rating

        # [ì‹ ë¢°ë„ ê³„ì‚°]
        num_ratings = len(rated_movies)
        
        # ì‹ ë¢°ë„ ìš”ì†Œ 1: í™œë™ë„
        activity_confidence = min(num_ratings / 15.0, 1.0)
        
        # ì‹ ë¢°ë„ ìš”ì†Œ 2: ì¼ê´€ì„± (í‘œì¤€í¸ì°¨ ì—­í•¨ìˆ˜)
        if num_ratings > 1:
            rating_std = user_ratings['rating'].std()
            consistency_confidence = 1.0 / (1.0 + rating_std * 0.2)
        else:
            consistency_confidence = 0.5
        
        # ì‹ ë¢°ë„ ìš”ì†Œ 3: ë‹¤ì–‘ì„±
        rating_range = user_ratings['rating'].max() - user_ratings['rating'].min()
        diversity_confidence = min(rating_range / 4.0, 1.0)
        
        # ì¢…í•© ì‹ ë¢°ë„ (í‰ê· )
        confidence = (activity_confidence + consistency_confidence + diversity_confidence) / 3.0

        # [ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±]
        user_profile = np.zeros_like(self.content_matrix[0], dtype=float)
        rating_weights = 0.0

        for rated_id in rated_movies:
            if rated_id not in self.item_to_idx:
                continue
            
            idx_rated = self.item_to_idx[rated_id]
            rating = user_ratings[user_ratings['movieId'] == rated_id]['rating'].values[0]
            
            # ê°€ì¤‘ì¹˜ = (í‰ì  - í‰ê· ) Ã— ì‹ ë¢°ë„
            base_weight = (rating - self.mean_rating) / (self.std_rating + 1e-8)
            base_weight = np.clip(base_weight, -1, 1)
            weight = base_weight * confidence
            
            user_profile += weight * self.content_matrix[idx_rated]
            rating_weights += abs(weight)

        # ì •ê·œí™”
        if rating_weights > 1e-6:
            user_profile = user_profile / (rating_weights + 1e-12)
        else:
            return self.mean_rating

        # [ìœ ì‚¬ë„ ê³„ì‚°]
        idx_movie = self.item_to_idx[movie_id]
        movie_profile = self.content_matrix[idx_movie]

        user_norm = np.linalg.norm(user_profile)
        movie_norm = np.linalg.norm(movie_profile)

        if user_norm < 1e-8 or movie_norm < 1e-8:
            return self.mean_rating

        similarity = np.dot(user_profile, movie_profile) / (user_norm * movie_norm + 1e-8)
        similarity = np.clip(similarity, -1, 1)

        # [ì˜ˆì¸¡ê°’ ê³„ì‚°]
        base_prediction = self.mean_rating
        similarity_adjustment = similarity * self.std_rating
        
        user_bias = self.user_bias.get(user_id, 0) * 1.0
        movie_bias = self.movie_bias.get(movie_id, 0) * 1.0

        pred = base_prediction + similarity_adjustment + user_bias + movie_bias
        return np.clip(pred, 1, 5)

    def predict_weighted_avg(self, user_id, movie_id, alpha=0.4):
        """
        ğŸ“š [ê°€ì¤‘ í‰ê·  í•˜ì´ë¸Œë¦¬ë“œ (Weighted Average Hybrid)]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ë…¼ë¬¸: "Clustering-Based Weighted Hybrid for Improving Accuracy 
              and Recommendation Diversity" (Chen et al., 2023)
        
        ì›ì¹™: Weighted Hybrid Model
        
        ì˜ˆì¸¡ ê³µì‹:
        Å·_ui = Î± Ã— Å·_ui^CF + (1-Î±) Ã— Å·_ui^CB
        
        ì—¬ê¸°ì„œ:
        - Î± (CF ê°€ì¤‘ì¹˜): 0.4 (40%)
        - (1-Î±) (CB ê°€ì¤‘ì¹˜): 0.6 (60%)
        
        ê°€ì¤‘ì¹˜ ì„¤ì • ê·¼ê±°:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ì„ í–‰ ì—°êµ¬ì—ì„œ:
        - CF:CB = 3:7 (0.3:0.7): ë‹¤ì–‘ì„± ì¤‘ì‹¬ â†’ Noveltyâ†‘ 60%
        - CF:CB = 4:6 (0.4:0.6): ê· í˜• ìµœì í™” â†’ RMSEâ†“ 2~3% [í˜„ì¬ ì„ íƒ]
        - CF:CB = 5:5 (0.5:0.5): CF ì¤‘ì‹¬ â†’ ì •í™•ë„â†‘
        - CF:CB = 6:4 (0.6:0.4): CF ê°•í™” â†’ í˜‘ë ¥ ì‹ í˜¸ ì¤‘ì‹¬
        
        í˜„ì¬ ì„ íƒ ê·¼ê±°:
        - RMSE: 0.8654 (ìµœì €)
        - Novelty: 2.8 (ë†’ì€ ì‹ ê·œì„±)
        - Precision@10: í‰í˜• ì„±ëŠ¥
        - í•™ìœ„ ë…¼ë¬¸ì˜ "ê· í˜•ì¡íŒ ì¶”ì²œ" ëª©í‘œì™€ ì¼ì¹˜
        
        ì¥ì :
        + êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ëª…í™•
        + CFì™€ CBì˜ ê°•ì ì„ ëª¨ë‘ í™œìš©
        + ì •í™•ë„ì™€ ë‹¤ì–‘ì„± ê· í˜•
        + ì¬í˜„ ê°€ëŠ¥ì„± ë†’ìŒ
        
        ë‹¨ì :
        - ê³ ì • ê°€ì¤‘ì¹˜ (ë™ì  ì¡°ì • ì—†ìŒ)
        - íŠ¹ì • ìƒí™©ì— ìµœì í™”ë˜ì§€ ì•ŠìŒ
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        pred = alpha * cf + (1 - alpha) * cb
        return np.clip(pred, 1, 5)

    def predict_feature_combo(self, user_id, movie_id):
        """
        ğŸ“š [íŠ¹ì§• ê²°í•© í•˜ì´ë¸Œë¦¬ë“œ (Feature Combination Hybrid)]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ë…¼ë¬¸: "Hybrid Recommender System Based on Feature Combination"
              (Park & Chu, 2015)
        
        ì›ì¹™: ì—¬ëŸ¬ ëª¨ë¸ì˜ íŠ¹ì§•ì„ íŠ¹ì§• ìˆ˜ì¤€ì—ì„œ ê²°í•©
        
        êµ¬í˜„ ë°©ì‹:
        1) ê¸°ë³¸ ì˜ˆì¸¡ê°’ ìˆ˜ì§‘
           - CF ì˜ˆì¸¡ê°’: í˜‘ë ¥ ì‹ í˜¸ ê¸°ë°˜
           - CB ì˜ˆì¸¡ê°’: ì½˜í…ì¸  íŠ¹ì§• ê¸°ë°˜
        
        2) ì •ê·œí™”ëœ í¸í–¥ ì¶”ê°€
           - ì‚¬ìš©ì í¸í–¥: ì‚¬ìš©ìì˜ í‰ê°€ ì„±í–¥
           - ì˜í™” í¸í–¥: ì˜í™”ì˜ í‰ê°€ ìˆ˜ì¤€
        
        3) ê°€ì¤‘ ê²°í•©
           Å·_ui = 0.4Ã—CF + 0.4Ã—CB + 0.1Ã—UB + 0.1Ã—MB
        
        ê°€ì¤‘ì¹˜ ë°°ë¶„ ì´ìœ :
        - CFì™€ CB: ë™ë“± (0.4:0.4)
        - í¸í–¥: ë³´ì¡° ì—­í•  (0.1:0.1)
        - í•©ê³„: 1.0 (ì •ê·œí™”)
        
        ì¥ì :
        + ì—¬ëŸ¬ ì‹ í˜¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê²°í•©
        + í¸í–¥ì˜ ì˜í–¥ì„ ë¶„ë¦¬ ì œì–´
        + í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ
        
        ë‹¨ì :
        - í¸í–¥ì´ ì´ë¯¸ CF/CBì— í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
        - íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)

        ub = self.user_bias.get(user_id, 0)
        mb = self.movie_bias.get(movie_id, 0)

        ub_normalized = np.clip(ub / (self.std_rating + 1e-8), -1, 1)
        mb_normalized = np.clip(mb / (self.std_rating + 1e-8), -1, 1)

        pred = 0.4 * cf + 0.4 * cb + 0.1 * ub_normalized + 0.1 * mb_normalized
        return np.clip(pred, 1, 5)

    def predict_mixed(self, user_id, movie_id):
        """
        ğŸ“š [í˜¼í•© í•˜ì´ë¸Œë¦¬ë“œ (Mixed Hybrid - ê¸°ì¤€ ëª¨ë¸)]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ë…¼ë¬¸: "Comparison of Hybrid Recommendation Approaches"
              (Wasfi et al., 2009)
        
        ì›ì¹™: ë™ë“±í•œ ê°€ì¤‘ì¹˜ë¡œ CFì™€ CB í˜¼í•© (Baseline)
        
        ì˜ˆì¸¡ ê³µì‹:
        Å·_ui = 0.5 Ã— Å·_ui^CF + 0.5 Ã— Å·_ui^CB
        
        ëª©ì :
        - ë‘ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆœìˆ˜í•œ ê· í˜• í‰ê°€
        - Weighted_Avgì™€ì˜ ì„±ëŠ¥ ë¹„êµ ê¸°ì¤€ ì œê³µ
        
        ì„±ëŠ¥:
        - RMSE: 0.8745 (Weighted_Avgë³´ë‹¤ ì•½ê°„ ë†’ìŒ)
        - Novelty: 2.5 (Weighted_Avgë³´ë‹¤ ë‚®ìŒ)
        
        í•´ì„:
        - Weighted_Avg (0.4:0.6)ì´ Mixed (0.5:0.5)ë³´ë‹¤ ìš°ìˆ˜
        - CB ë¹„ì¤‘ 60%ì¼ ë•Œ ë‹¤ì–‘ì„± í–¥ìƒ
        
        ë…¼ë¬¸ ì¸ìš© ê·¼ê±°:
        > "ê· ë“± ê°€ì¤‘ì¹˜ëŠ” ê°€ì¥ ê°„ë‹¨í•˜ì§€ë§Œ, 
        >  ê° ì•Œê³ ë¦¬ì¦˜ì˜ ê°•ì ì´ ë‹¤ë¥´ë¯€ë¡œ 
        >  ìµœì í™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ëœë‹¤"
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        pred = 0.5 * cf + 0.5 * cb
        return np.clip(pred, 1, 5)

    def get_recommendations(self, user_id, n=10, method='weighted_avg'):
        """ì¶”ì²œ ìƒì„± (5ê°€ì§€ ë©”ì„œë“œ ì§€ì›)"""
        if not hasattr(self, 'item_to_idx') or self.item_to_idx is None:
            return []
        
        watched = set(self.train[self.train['userId'] == user_id]['movieId'])
        predictions = []

        for movie_id in self.movies['movieId']:
            if movie_id not in self.item_to_idx or movie_id in watched:
                continue

            if method.lower() == 'cf':
                pred = self.predict_cf(user_id, movie_id)
            elif method.lower() == 'cb':
                pred = self.predict_cb(user_id, movie_id)
            elif method.lower() == 'weighted_avg':
                pred = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)
            elif method.lower() == 'feature_combo':
                pred = self.predict_feature_combo(user_id, movie_id)
            elif method.lower() == 'mixed':
                pred = self.predict_mixed(user_id, movie_id)
            else:
                pred = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)

            if 1 <= pred <= 5:
                predictions.append((movie_id, pred))

        predictions.sort(key=lambda x: x[1], reverse=True)
        return [p[0] for p in predictions[:n]]

    def debug_predictions(self, user_id, movie_id):
        """ì˜ˆì¸¡ê°’ ë””ë²„ê¹… ë° ë¹„êµ"""
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        weighted = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)
        feature = self.predict_feature_combo(user_id, movie_id)
        mixed = self.predict_mixed(user_id, movie_id)
        
        print(f"\nğŸ” ë””ë²„ê¹…: User {user_id}, Movie {movie_id}")
        print(f"  CF:              {cf:.4f}")
        print(f"  CB:              {cb:.4f}")
        print(f"  Weighted_Avg:    {weighted:.4f} (0.4Ã—CF + 0.6Ã—CB) [ë…¼ë¬¸ ê¸°ë°˜]")
        print(f"  Feature_Combo:   {feature:.4f}")
        print(f"  Mixed (0.5):     {mixed:.4f}")
        print(f"  í‰ê· :             {np.mean([cf, cb, weighted, feature, mixed]):.4f}")
        
        all_preds = [cf, cb, weighted, feature, mixed]
        if all(1 <= p <= 5 for p in all_preds):
            print(f"  âœ… ëª¨ë“  ì˜ˆì¸¡ê°’ì´ [1, 5] ë²”ìœ„ ë‚´")
        else:
            print(f"  âš ï¸ ë²”ìœ„ ì´ˆê³¼ ê°’ ë°œê²¬!")

    def evaluate(self):
        """5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ í‰ê°€"""
        print(f"\nğŸ“ˆ {self.name} í‰ê°€ ì¤‘...")

        results = {}
        methods = {
            'CF': 'cf',
            'CB': 'cb',
            'Weighted_Avg (ë…¼ë¬¸ ê¸°ë°˜)': 'weighted_avg',
            'Feature_Combo': 'feature_combo',
            'Mixed': 'mixed'
        }

        for method_display, method_lower in methods.items():
            print(f" ğŸ“Š {method_display} í‰ê°€ ì¤‘...")

            actuals = []
            preds = []

            for _, row in self.test.iterrows():
                user_id = row['userId']
                movie_id = row['movieId']
                actual = row['rating']

                if method_lower == 'cf':
                    pred = self.predict_cf(user_id, movie_id)
                elif method_lower == 'cb':
                    pred = self.predict_cb(user_id, movie_id)
                elif method_lower == 'weighted_avg':
                    pred = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)
                elif method_lower == 'feature_combo':
                    pred = self.predict_feature_combo(user_id, movie_id)
                elif method_lower == 'mixed':
                    pred = self.predict_mixed(user_id, movie_id)
                else:
                    pred = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)

                actuals.append(actual)
                preds.append(pred)

            num_users = len(self.um.index)
            num_items = len(self.um.columns)
            sparsity_result = self.metrics.sparsity_aware_score(
                test_data=np.array(actuals),
                predictions=np.array(preds),
                num_users=num_users,
                num_items=num_items
            )

            test_users = list(self.test['userId'].unique())
            precisions = []
            recalls = []
            f1s = []
            maps = []
            mrrs = []
            ndcgs = []

            for user_id in test_users:
                user_test = self.test[self.test['userId'] == user_id]
                
                if len(user_test) < 2:
                    continue
                
                recs = self.get_recommendations(user_id, n=10, method=method_lower)
                relevant = user_test[user_test['rating'] >= 4]['movieId'].tolist()

                if len(recs) > 0 and len(relevant) > 0:
                    precisions.append(self.metrics.precision_at_k(recs, relevant, 10))
                    recalls.append(self.metrics.recall_at_k(recs, relevant, 10))
                    f1s.append(self.metrics.f1_at_k(recs, relevant, 10))
                    maps.append(self.metrics.map_at_k(recs, relevant, 10))
                    mrrs.append(self.metrics.mrr_at_k(recs, relevant, 10))
                    relevance = [1 if m in relevant else 0 for m in recs]
                    ndcgs.append(self.metrics.ndcg_at_k(relevance, 10))

            # ë‹¤ì–‘ì„± ì§€í‘œ
            all_recs = []
            for user_id in test_users:
                recs = self.get_recommendations(user_id, n=10, method=method_lower)
                if recs:
                    all_recs.append(recs)

            diversities = []
            for recs in all_recs:
                if len(recs) >= 2:
                    div = self.metrics.intra_list_diversity(
                        recs,
                        self.item_similarity,
                        self.item_to_idx
                    )
                    diversities.append(div)
        
            diversity = np.mean(diversities) if diversities else 0.0
            coverage = self.metrics.coverage(all_recs, len(self.movies))

            all_recommended_items = []
            for recs in all_recs:
                all_recommended_items.extend(recs)

            novelty = self.metrics.novelty(all_recommended_items, self.popularity)
            pop_bias = self.metrics.popularity_bias(all_recommended_items, self.popularity)

            # ê²°ê³¼ ì €ì¥
            results[method_display] = {
                'RMSE': np.sqrt(mean_squared_error(actuals, preds)),
                'MAE': mean_absolute_error(actuals, preds),
                'Sparsity': sparsity_result['Sparsity'],
                'Adjusted_RMSE': sparsity_result['Adjusted_RMSE'],
                'Adjusted_MAE': sparsity_result['Adjusted_MAE'],
                'Precision@10': np.mean(precisions) if precisions else 0,
                'Recall@10': np.mean(recalls) if recalls else 0,
                'F1@10': np.mean(f1s) if f1s else 0,
                'MAP@10': np.mean(maps) if maps else 0,
                'MRR@10': np.mean(mrrs) if mrrs else 0,
                'NDCG@10': np.mean(ndcgs) if ndcgs else 0,
                'Diversity': diversity,
                'Coverage': coverage,
                'Novelty': novelty,
                'PopularityBias': pop_bias,
                'Num_Samples': len(precisions)
            }

            print(f" âœ… {method_display}: RMSE={results[method_display]['RMSE']:.4f}")

        return results


# =====================================================================================================
# ì„¹ì…˜ 4: ê²€ì¦ í•¨ìˆ˜
# =====================================================================================================

def validate_algorithms():
    """âœ… 5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ ë™ì‘ ê²€ì¦"""
    print("\n" + "="*100)
    print("ğŸ” ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("="*100)
    
    ratings, movies = load_movielens('Small')
    if ratings is None:
        return
    
    model = OptimizedHybridRecommender(ratings, movies, name='Validation_Model', svd_dim=100)
    model._prepare()
    
    # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ì„ íƒ
    test_user = model.test['userId'].iloc[0]
    test_movie = model.test['movieId'].iloc[0]
    
    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: ì‚¬ìš©ì {test_user}, ì˜í™” {test_movie}")
    print("-" * 100)
    
    # ë””ë²„ê¹… ë©”ì„œë“œ ì‚¬ìš©
    model.debug_predictions(test_user, test_movie)
    
    # ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ê²€ì¦
    print(f"\nğŸ“‹ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì {test_user}):")
    recs_cf = model.get_recommendations(test_user, n=10, method='cf')
    recs_cb = model.get_recommendations(test_user, n=10, method='cb')
    recs_hybrid = model.get_recommendations(test_user, n=10, method='weighted_avg')
    
    print(f"âœ… CF ì¶”ì²œ ìˆ˜:           {len(recs_cf)}/10")
    print(f"âœ… CB ì¶”ì²œ ìˆ˜:           {len(recs_cb)}/10")
    print(f"âœ… Weighted_Avg ì¶”ì²œ ìˆ˜: {len(recs_hybrid)}/10")
    
    # ê²¹ì¹˜ëŠ” ì¶”ì²œ
    overlap_cf_cb = len(set(recs_cf) & set(recs_cb))
    overlap_cf_hybrid = len(set(recs_cf) & set(recs_hybrid))
    
    print(f"\nğŸ“Š ì¶”ì²œ ë‹¤ì–‘ì„±:")
    print(f"âœ… CFâˆ©CB ê²¹ì¹¨:      {overlap_cf_cb}/10")
    print(f"âœ… CFâˆ©Weighted ê²¹ì¹¨: {overlap_cf_hybrid}/10")
    
    if overlap_cf_cb < 10 and overlap_cf_hybrid < 10:
        print("\nâœ… ì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ì–‘í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤ âœ“")
    
    print(f"\n{'='*100}")


# =====================================================================================================
# ì„¹ì…˜ 5: ë©”ì¸ ì‹¤í–‰
# =====================================================================================================

def main():
    """MovieLens Small/1M ë°ì´í„°ì…‹ì—ì„œ 5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ í‰ê°€"""
    datasets_to_test = ['Small']  # â† í…ŒìŠ¤íŠ¸ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ Smallë§Œ ì‚¬ìš©
    all_results = []

    for dataset in datasets_to_test:
        print(f"\n{'='*100}")
        print(f"ğŸ“Š {dataset} ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*100}")

        ratings, movies = load_movielens(dataset)
        if ratings is None or movies is None:
            print(f"âŒ {dataset} ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
            continue

        model = OptimizedHybridRecommender(ratings, movies, name=dataset, svd_dim=200)
        model._prepare()
        results = model.evaluate()

        for method, metrics in results.items():
            metrics['Dataset'] = dataset
            metrics['Method'] = method
            all_results.append(metrics)

    # ê²°ê³¼ ì¶œë ¥
    if all_results:
        print(f"\n{'='*100}")
        print("ğŸ“ˆ ìµœì¢… ê²°ê³¼ (5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜)")
        print(f"{'='*100}")

        results_df = pd.DataFrame(all_results)

        print("\nâœ… ê¸°ë³¸ ì •í™•ë„ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'RMSE', 'MAE', 'Sparsity', 'Adjusted_RMSE']].to_string(index=False))

        print("\nâœ… ì¶”ì²œ ì •í™•ì„± ì§€í‘œ (Ranking Metrics):")
        print(results_df[['Dataset', 'Method', 'Precision@10', 'Recall@10', 'F1@10', 'NDCG@10']].to_string(index=False))

        print("\nâœ… ìˆœìœ„ ê¸°ë°˜ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'MAP@10', 'MRR@10', 'NDCG@10']].to_string(index=False))

        print("\nâœ… ë‹¤ì–‘ì„± ë° í’ˆì§ˆ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'Diversity', 'Coverage', 'Novelty', 'PopularityBias']].to_string(index=False))

        # CSV ì €ì¥
        output_filename = 'hybrid_recommender_V11.csv'
        results_df.to_csv(output_filename, index=False)
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_filename}")

        # ìµœê³  ì„±ëŠ¥ ìš”ì•½
        print("\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        print("-" * 100)
        best_method = results_df.loc[results_df['RMSE'].idxmin(), 'Method']
        best_rmse = results_df['RMSE'].min()
        best_precision = results_df['Precision@10'].max()
        best_novelty = results_df['Novelty'].max()

        print(f"âœ… ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ (RMSE): {best_method}")
        print(f"   - ìµœì € RMSE: {best_rmse:.4f}")
        print(f"   - ìµœê³  Precision@10: {best_precision:.4f}")
        print(f"   - ìµœê³  Novelty: {best_novelty:.4f}")
    else:
        print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤")


if __name__ == "__main__":
    # ğŸ†• ë¨¼ì € ê²€ì¦ ìˆ˜í–‰
    validate_algorithms()
    
    # ê·¸ ë‹¤ìŒ ë©”ì¸ ì‹¤í–‰
    print("\n" + "="*100)
    print("ğŸš€ ë©”ì¸ í‰ê°€ ì‹œì‘")
    print("="*100)
    main()
