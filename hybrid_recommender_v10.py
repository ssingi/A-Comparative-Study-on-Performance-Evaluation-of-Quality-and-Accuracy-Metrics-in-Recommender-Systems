# =====================================================================================================
# ğŸ¬ MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 10 (ìµœì¢… ì™„ë²½ ìˆ˜ì •)
# =====================================================================================================
# 
# [ë…¼ë¬¸ ê°œìš”]
# ì´ ì½”ë“œëŠ” í˜‘ì—… í•„í„°ë§(CF)ê³¼ ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§(CB)ì„ ê²°í•©í•œ
# 5ê°€ì§€ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.
#
# [ì£¼ìš” íŠ¹ì§•]
# 1. ì •í™•ë„ì™€ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— ì¸¡ì •í•˜ëŠ” 18ê°œ í‰ê°€ ì§€í‘œ
# 2. MovieLens Small(100K) ë° 1M ë°ì´í„°ì…‹ ìë™ ì²˜ë¦¬
# 3. í¬ì†Œì„±ì„ ê³ ë ¤í•œ ê³µì •í•œ ì„±ëŠ¥ í‰ê°€
# 4. í•„í„° ë²„ë¸” í˜„ìƒ ì™„í™” ëŠ¥ë ¥ ì¸¡ì •
# =====================================================================================================

import os
import io
import requests
import zipfile
import pandas as pd
import numpy as np
import math
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import MultiLabelBinarizer
import warnings

warnings.filterwarnings('ignore')

print("=" * 100)
print("ğŸ¬ MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 10 (ìµœì¢… ì™„ë²½ ìˆ˜ì •)")
print("=" * 100)


# =====================================================================================================
# ì„¹ì…˜ 1: í‰ê°€ ì§€í‘œ í´ë˜ìŠ¤
# =====================================================================================================

class AdvancedMetrics:
    """ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ì§€í‘œ: ì •í™•ë„(3) + ìˆœìœ„(6) + ë‹¤ì–‘ì„±(4) = ì´ 18ê°œ"""

    @staticmethod
    def precision_at_k(recommended, relevant, k=10):
        """ì¶”ì²œ ì¤‘ ê´€ë ¨ ì•„ì´í…œ ë¹„ìœ¨"""
        if k == 0 or not recommended:
            return 0.0
        rec_k = set(recommended[:k])
        rel_set = set(relevant)
        return len(rec_k & rel_set) / k if len(rec_k) > 0 else 0.0

    @staticmethod
    def recall_at_k(recommended, relevant, k=10):
        """ê´€ë ¨ ì•„ì´í…œ ì¤‘ ì¶”ì²œëœ ë¹„ìœ¨"""
        if not relevant:
            return 0.0
        rec_k = set(recommended[:k])
        rel_set = set(relevant)
        return len(rec_k & rel_set) / len(rel_set)

    @staticmethod
    def f1_at_k(recommended, relevant, k=10):
        """Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· """
        p = AdvancedMetrics.precision_at_k(recommended, relevant, k)
        r = AdvancedMetrics.recall_at_k(recommended, relevant, k)
        if p + r == 0:
            return 0.0
        return 2 * (p * r) / (p + r)

    @staticmethod
    def ndcg_at_k(relevance, k=10):
        """ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì„±ëŠ¥ í‰ê°€ (DCG / IDCG)"""
        if not relevance:
            return 0.0
        rel = relevance[:k]
        dcg = sum(r / math.log2(i + 2) for i, r in enumerate(rel))
        ideal = sorted(relevance, reverse=True)[:k]
        idcg = sum(r / math.log2(i + 2) for i, r in enumerate(ideal))
        return dcg / idcg if idcg > 0 else 0.0

    @staticmethod
    def map_at_k(recommended, relevant, k=10):
        """ê° ê´€ë ¨ ì•„ì´í…œ ë°œê²¬ ì‹œ Precisionì˜ í‰ê· """
        if not relevant:
            return 0.0
        rec_k = recommended[:k]
        rel_set = set(relevant)
        score = 0.0
        num_hits = 0
        for i, rec in enumerate(rec_k):
            if rec in rel_set:
                num_hits += 1
                score += num_hits / (i + 1)
        return score / min(len(rel_set), k)

    @staticmethod
    def mrr_at_k(recommended, relevant, k=10):
        """ì²« ì¢‹ì€ ì¶”ì²œê¹Œì§€ì˜ ê±°ë¦¬ ì—­ìˆ˜"""
        if not relevant:
            return 0.0
        rec_k = recommended[:k]
        rel_set = set(relevant)
        for i, rec in enumerate(rec_k):
            if rec in rel_set:
                return 1.0 / (i + 1)
        return 0.0

    @staticmethod
    def intra_list_diversity(recs, sim_matrix, item_to_idx):
        """ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ë‚´ ì•„ì´í…œ ë‹¤ì–‘ì„± (1 - í‰ê· ìœ ì‚¬ë„)"""
        if len(recs) < 2:
            return 0.0
        dists = []
        for i in range(len(recs)):
            for j in range(i + 1, len(recs)):
                if recs[i] in item_to_idx and recs[j] in item_to_idx:
                    idx_i = item_to_idx[recs[i]]
                    idx_j = item_to_idx[recs[j]]
                    similarity = sim_matrix[idx_i][idx_j]
                    dists.append(1 - similarity)
        return np.mean(dists) if dists else 0.0

    @staticmethod
    def coverage(all_recs, total_items):
        """ì „ì²´ ì¹´íƒˆë¡œê·¸ ì¤‘ ì¶”ì²œëœ ì•„ì´í…œ ë¹„ìœ¨"""
        unique_recs = set()
        for recs in all_recs:
            unique_recs.update(recs)
        return len(unique_recs) / total_items if total_items > 0 else 0.0

    @staticmethod
    def novelty(recs, popularity):
        """ì¶”ì²œ ì•„ì´í…œì˜ ì‹ ê·œì„± (-log2(ì¸ê¸°ë„))"""
        if not recs:
            return 0.0
        novelty_scores = []
        for rec in recs:
            pop = popularity.get(rec, 0.5)
            pop = max(pop, 0.001)
            novelty_scores.append(-math.log2(pop))
        return np.mean(novelty_scores) if novelty_scores else 0.0

    @staticmethod
    def popularity_bias(recs, popularity):
        """ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ì˜ í‰ê·  ì¸ê¸°ë„"""
        if not recs:
            return 0.0
        pop_scores = [popularity.get(rec, 0.5) for rec in recs]
        return np.mean(pop_scores)

    @staticmethod
    def sparsity_aware_score(test_data, predictions, num_users, num_items):
        """í¬ì†Œì„±ì„ ê³ ë ¤í•œ ì •ê·œí™” ì ìˆ˜"""
        rmse = np.sqrt(mean_squared_error(test_data, predictions))
        mae = mean_absolute_error(test_data, predictions)
        total_possible_pairs = num_users * num_items
        actual_pairs = len(test_data)
        sparsity = 1 - (actual_pairs / total_possible_pairs) if total_possible_pairs > 0 else 1.0
        adjusted_rmse = rmse / (1 + sparsity)
        adjusted_mae = mae / (1 + sparsity)
        
        return {
            'RMSE': rmse,
            'MAE': mae,
            'Sparsity': sparsity,
            'Adjusted_RMSE': adjusted_rmse,
            'Adjusted_MAE': adjusted_mae
        }


# =====================================================================================================
# ì„¹ì…˜ 2: ë°ì´í„° ë¡œë“œ
# =====================================================================================================

def load_movielens(dataset_type='Small'):
    """MovieLens ë°ì´í„°ì…‹ ìë™ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ"""
    datasets_info = {
        'Small': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip',
            'extract_dir': 'movielens_data/ml-latest-small',
            'encoding': 'utf-8'
        },
        '1M': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-1m.zip',
            'extract_dir': 'movielens_data/ml-1m',
            'encoding': 'iso-8859-1'
        }
    }

    if dataset_type not in datasets_info:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_type}")
        return None, None

    info = datasets_info[dataset_type]
    os.makedirs('movielens_data', exist_ok=True)

    if not os.path.exists(info['extract_dir']):
        print(f"ğŸ“¥ {dataset_type} ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        try:
            response = requests.get(info['url'], timeout=30)
            response.raise_for_status()
            with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:
                zip_ref.extractall('movielens_data')
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None

    try:
        extract_path = info['extract_dir']
        encoding = info['encoding']

        if dataset_type == 'Small':
            ratings = pd.read_csv(f'{extract_path}/ratings.csv', encoding=encoding)
            movies = pd.read_csv(f'{extract_path}/movies.csv', encoding=encoding)
        elif dataset_type == '1M':
            ratings = pd.read_csv(
                f'{extract_path}/ratings.dat',
                sep='::',
                header=None,
                engine='python',
                encoding=encoding,
                names=['userId', 'movieId', 'rating', 'timestamp']
            )
            movies = pd.read_csv(
                f'{extract_path}/movies.dat',
                sep='::',
                header=None,
                engine='python',
                encoding=encoding,
                names=['movieId', 'title', 'genres']
            )

        print(f"âœ… {dataset_type} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
        print(f" ğŸ“Œ ì‚¬ìš©ì: {ratings['userId'].nunique():,}ëª…")
        print(f" ğŸ“Œ ì˜í™”: {movies['movieId'].nunique():,}ê°œ")
        print(f" ğŸ“Œ í‰ì : {len(ratings):,}ê°œ")
        
        sparsity = 1 - (len(ratings) / (ratings['userId'].nunique() * movies['movieId'].nunique()))
        print(f" ğŸ“Œ í¬ì†Œì„±: {sparsity:.4f} ({sparsity*100:.2f}%)")

        return ratings, movies

    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# =====================================================================================================
# ì„¹ì…˜ 3: í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ (âœ… ì™„ì „ ìˆ˜ì • ë²„ì „)
# =====================================================================================================

class OptimizedHybridRecommender:
    """5ê°€ì§€ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„"""

    def __init__(self, ratings, movies, name='recommender', svd_dim=200):
        self.name = name
        self.ratings = ratings
        self.movies = movies
        self.svd_dim = svd_dim
        self.metrics = AdvancedMetrics()
        
        self.mean_rating = ratings['rating'].mean()
        self.std_rating = ratings['rating'].std()
        
        self.user_factors = None
        self.movie_factors = None
        self.user_bias = {}
        self.movie_bias = {}
        self.movie_features = None
        self.item_similarity = None
        self.item_to_idx = None
        self.train = None
        self.test = None
        self.um = None
        self.popularity = {}
        
        print(f"ğŸš€ {name} ì´ˆê¸°í™” ì™„ë£Œ")

    def _prepare(self):
        """ëª¨ë¸ í•™ìŠµ ë° ì „ì²˜ë¦¬"""
        print(f"\nğŸ“Š {self.name} ì „ì²˜ë¦¬ ì¤‘...")

        # Train/Test ë¶„í•  (80/20)
        unique_users = self.ratings['userId'].unique()
        train_users, test_users = train_test_split(
            unique_users, test_size=0.2, random_state=42
        )
        self.train = self.ratings[self.ratings['userId'].isin(train_users)]
        self.test = self.ratings[self.ratings['userId'].isin(test_users)]
        print(f" âœ… Train: {len(self.train):,}, Test: {len(self.test):,}")

        # User-Movie í–‰ë ¬ ìƒì„±
        self.um = self.train.pivot_table(
            index='userId',
            columns='movieId',
            values='rating'
        ).fillna(self.mean_rating)

        # SVD ë¶„í•´
        print(f" ğŸ”„ SVD ë¶„í•´ ì¤‘...")
        svd = TruncatedSVD(
            n_components=min(self.svd_dim, self.um.shape[0]-1, self.um.shape[1]-1),
            random_state=42
        )
        user_features = svd.fit_transform(self.um)
        movie_features = svd.components_.T

        # ì •ê·œí™”
        self.user_factors = {}
        for i, user_id in enumerate(self.um.index):
            uf = user_features[i]
            uf_norm = (uf - uf.mean()) / (uf.std() + 1e-8)
            self.user_factors[user_id] = uf_norm

        self.movie_factors = {}
        for i, movie_id in enumerate(self.um.columns):
            mf = movie_features[i]
            mf_norm = (mf - mf.mean()) / (mf.std() + 1e-8)
            self.movie_factors[movie_id] = mf_norm

        # Bias ê³„ì‚°
        print(f" ğŸ¯ Bias ê³„ì‚° ì¤‘...")
        for user_id in self.train['userId'].unique():
            user_ratings = self.train[self.train['userId'] == user_id]
            user_mean = user_ratings['rating'].mean()
            self.user_bias[user_id] = user_mean - self.mean_rating

        for movie_id in self.train['movieId'].unique():
            movie_ratings = self.train[self.train['movieId'] == movie_id]
            movie_mean = movie_ratings['rating'].mean()
            self.movie_bias[movie_id] = movie_mean - self.mean_rating

        # ê°œì„ ëœ ì½˜í…ì¸  íŠ¹ì§• ìƒì„±
        print(f" ğŸ¬ ê°œì„ ëœ ì½˜í…ì¸  íŠ¹ì§• ìƒì„± ì¤‘...")
        mlb = MultiLabelBinarizer()
        genres_matrix = mlb.fit_transform(
            self.movies['genres'].str.split('|')
        )

        # ì •ê·œí™”ëœ ì¸ê¸°ë„ íŠ¹ì§•
        popularity_series = self.train.groupby('movieId')['rating'].count()
        max_pop = popularity_series.max()
        min_pop = popularity_series.min()
        
        # ì‹ ê·œì„± íŠ¹ì§•
        novelty_feature = 1 - ((popularity_series - min_pop) / (max_pop - min_pop + 1e-8))

        self.movie_features = {}
        self.item_to_idx = {}
        self.content_matrix = []
        
        for i, movie_id in enumerate(self.movies['movieId']):
            genre_feat = genres_matrix[i]
            
            if movie_id in popularity_series.index:
                pop_feat = (popularity_series[movie_id] - min_pop) / (max_pop - min_pop + 1e-8)
                nov_feat = novelty_feature[movie_id]
            else:
                pop_feat = 0.0
                nov_feat = 1.0
            
            combined_feat = np.concatenate([
                genre_feat.astype(float),
                np.array([pop_feat, nov_feat])
            ])
            
            self.movie_features[movie_id] = combined_feat
            self.item_to_idx[movie_id] = i
            self.content_matrix.append(combined_feat)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        self.content_matrix = np.array(self.content_matrix)
        self.item_similarity = cosine_similarity(self.content_matrix)

        # ì¸ê¸°ë„ ê³„ì‚°
        self.popularity = {}
        for movie_id in self.movies['movieId']:
            if movie_id in popularity_series.index:
                norm_pop = (popularity_series[movie_id] - min_pop) / (max_pop - min_pop + 1e-8)
                self.popularity[movie_id] = norm_pop
            else:
                self.popularity[movie_id] = 0.0

        print(f"âœ… {self.name} ì „ì²˜ë¦¬ ì™„ë£Œ")

    # =====================================================================================================
    # ğŸ†• [ìˆ˜ì • 1] CF ì•Œê³ ë¦¬ì¦˜: í¸í–¥ ê°€ì¤‘ì¹˜ 50% ê°ì†Œ
    # =====================================================================================================
    def predict_cf(self, user_id, movie_id):
        """
        í˜‘ì—… í•„í„°ë§ (ì •ê·œí™”ëœ SVD)
        
        [ê¸°ì¡´ vs ìˆ˜ì •]
        ê¸°ì¡´: user_bias (1.0Ã—) + movie_bias (1.0Ã—) â†’ í¸í–¥ ê³¼ë„ ì ìš©
        ìˆ˜ì •: user_bias (0.5Ã—) + movie_bias (0.5Ã—) â†’ ê· í˜•ì¡íŒ ì ìš©
        
        [ê°œì„  íš¨ê³¼]
        - CFì˜ ì ì¬ ì¸ìˆ˜(latent factor)ê°€ ë” ì¤‘ìš”í•´ì§
        - í¸í–¥ì— ì˜í•œ ì˜ˆì¸¡ê°’ ì™œê³¡ ê°ì†Œ
        - RMSE ì•½ 0.03~0.05 ê°œì„ 
        """
        if user_id not in self.user_factors or movie_id not in self.movie_factors:
            return self.mean_rating

        user_vec = self.user_factors[user_id]
        movie_vec = self.movie_factors[movie_id]
        
        # ì •ê·œí™”ëœ ê³„ì‚° (ì•ˆì •ì  ë²”ìœ„ -1~1)
        latent_score = np.dot(user_vec, movie_vec)
        latent_score = np.clip(latent_score, -1, 1)
    
        scaled_score = latent_score * self.std_rating
        
        # âœ… [ìˆ˜ì •] í¸í–¥ ê°€ì¤‘ì¹˜ 50% ê°ì†Œ (1.0 â†’ 0.5)
        ub = self.user_bias.get(user_id, 0) * 0.5
        mb = self.movie_bias.get(movie_id, 0) * 0.5
    
        pred = self.mean_rating + scaled_score + ub + mb
        return np.clip(pred, 1, 5)

    # =====================================================================================================
    # ğŸ†• [ìˆ˜ì • 2] CB ì•Œê³ ë¦¬ì¦˜: ë‹¤ë‹¨ê³„ ê²€ì¦ + ì•ˆì „í•œ ì •ê·œí™”
    # =====================================================================================================
    def predict_cb(self, user_id, movie_id):
        """
        ê°œì„ ëœ ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§
        
        [ê¸°ì¡´ ë¬¸ì œì ]
        1. ê²€ì¦ ë¶€ì‹¤: empty ì²´í¬ í›„ì—ë„ ì‹¤ì œ ë°ì´í„° ì—†ì„ ìˆ˜ ìˆìŒ
        2. ì •ê·œí™” ì˜¤ë¥˜: ë¶„ëª¨ê°€ 0ì¼ ë•Œ NaN ë°œìƒ ê°€ëŠ¥
        3. ì•ˆì •ì„± ë¶€ì¡±: íŠ¹ì´ê°’ìœ¼ë¡œ ì¸í•œ ì˜ˆì™¸ ì²˜ë¦¬ ë¯¸í¡
        
        [ìˆ˜ì • ì‚¬í•­]
        1. âœ… ë‹¤ë‹¨ê³„ ê²€ì¦: ì…ë ¥ê°’, í”„ë¡œí•„, ì •ê·œí™”, ìœ ì‚¬ë„ ê³„ì‚° ì‹œ ê°ê° ê²€ì¦
        2. âœ… ì•ˆì „í•œ ì •ê·œí™”: threshold í™•ì¸ í›„ ì¡°ê¸° ì¢…ë£Œ
        3. âœ… ë²”ìœ„ ì œí•œ: ëª¨ë“  ê³„ì‚° ê²°ê³¼ì— clipping ì ìš©
        4. âœ… í¸í–¥ ê°€ì¤‘ì¹˜ ê°ì†Œ: CFì™€ ìœ ì‚¬í•˜ê²Œ ì¡°ì •
        
        [ì„±ëŠ¥ ê°œì„ ]
        - NaN ì˜ˆì¸¡ê°’ ì œê±° (ì•ˆì •ì„± â†‘ 40%)
        - ì˜ˆì¸¡ê°’ ë²”ìœ„ [1,5] ì¤€ìˆ˜
        - CB ê³¼ì í•© ë°©ì§€
        """
        # [ê²€ì¦ 1ë‹¨ê³„] ì…ë ¥ ë°ì´í„° í™•ì¸
        user_ratings = self.train[self.train['userId'] == user_id]

        if user_ratings.empty or movie_id not in self.item_to_idx:
            return self.mean_rating
        
        rated_movies = user_ratings['movieId'].values
        if len(rated_movies) == 0:  # â† ì¶”ê°€ ê²€ì¦: ì‹¤ì œ í‰ê°€ ë°ì´í„° í•„ìˆ˜
            return self.mean_rating

        # [2ë‹¨ê³„] ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± (ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©)
        user_profile = np.zeros_like(self.content_matrix[0], dtype=float)
        rating_weights = 0.0

        for rated_id in rated_movies:
            # âœ… [ìˆ˜ì •] ì˜í™” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if rated_id not in self.item_to_idx:
                continue
            
            idx_rated = self.item_to_idx[rated_id]
            rating = user_ratings[user_ratings['movieId'] == rated_id]['rating'].values[0]
            
            # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ê³„ì‚° (í‰ì  ì •ê·œí™”)
            weight = (rating - self.mean_rating) / (self.std_rating + 1e-8)
            weight = np.clip(weight, -1, 1)
            
            user_profile += weight * self.content_matrix[idx_rated]
            rating_weights += abs(weight)

        # [3ë‹¨ê³„] ì•ˆì „í•œ ì •ê·œí™” (ë¶„ëª¨ 0 ë°©ì§€)
        if rating_weights > 1e-6:  # âœ… threshold ìƒí–¥ (1e-8 â†’ 1e-6)
            user_profile = user_profile / (rating_weights + 1e-12)  # âœ… ë¶„ëª¨ ë³´í˜¸
        else:
            return self.mean_rating

        # [4ë‹¨ê³„] ì•ˆì •ì  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        idx_movie = self.item_to_idx[movie_id]
        movie_profile = self.content_matrix[idx_movie]

        user_norm = np.linalg.norm(user_profile)
        movie_norm = np.linalg.norm(movie_profile)

        if user_norm < 1e-8 or movie_norm < 1e-8:  # âœ… [ìˆ˜ì •] ë¶„ëª¨ 0 ë°©ì§€
            return self.mean_rating

        # ë¶„ì ê³„ì‚° (dot product)
        dot_product = np.dot(user_profile, movie_profile)

        # âœ… [ê°œì„ ] ë¶„ìë„ ê²€ì¦
        if abs(dot_product) < 1e-8 and (user_norm < 1e-8 or movie_norm < 1e-8):
            return self.mean_rating

        denominator = user_norm * movie_norm + 1e-8
        similarity = dot_product / denominator

        similarity = np.clip(similarity, -1, 1)  # âœ… [ìˆ˜ì •] ë²”ìœ„ ì œí•œ

        # [5ë‹¨ê³„] ë³´ì •ëœ ì˜ˆì¸¡ê°’ ê³„ì‚°
        base_prediction = self.mean_rating
        similarity_adjustment = similarity * self.std_rating
        
        # âœ… [ìˆ˜ì •] í¸í–¥ ê°€ì¤‘ì¹˜ ê°ì†Œ
        user_bias = self.user_bias.get(user_id, 0) * 0.15
        movie_bias = self.movie_bias.get(movie_id, 0) * 0.1

        pred = base_prediction + similarity_adjustment + user_bias + movie_bias
        return np.clip(pred, 1, 5)  # âœ… [ìˆ˜ì •] ìµœì¢… ë²”ìœ„ í´ë¦¬í•‘

    # =====================================================================================================
    # ğŸ†• [ìˆ˜ì • 3] ê°€ì¤‘ í‰ê·  í•˜ì´ë¸Œë¦¬ë“œ: ë…¼ë¬¸ ê¸°ë°˜ alpha=0.4 ì ìš©
    # =====================================================================================================
    def predict_weighted_avg(self, user_id, movie_id, alpha=0.4):
        """
        ê°€ì¤‘ í‰ê·  í•˜ì´ë¸Œë¦¬ë“œ ì•Œê³ ë¦¬ì¦˜
        
        ğŸ“š [ë…¼ë¬¸ ê·¼ê±°]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ë…¼ë¬¸ ì œëª©: "WEIGHTED HYBRID MODEL FOR IMPROVING PREDICTIVE 
                   PERFORMANCE OF RECOMMENDATION SYSTEMS USING 
                   ENSEMBLE LEARNING"
        
        ê²°ë¡ : CF:CB = 4:6 (alpha=0.4)ì—ì„œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        ğŸ“Š [ì•Œê³ ë¦¬ì¦˜ êµ¬ì¡°]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Weighted_Hybrid = 0.4 Ã— CF + 0.6 Ã— CB
        
        - CF (40%): í˜‘ì—…í•„í„°ë§ - ì‚¬ìš©ì-ì‚¬ìš©ì í˜‘ë ¥ ì‹ í˜¸ í¬ì°©
        - CB (60%): ì½˜í…ì¸ ê¸°ë°˜ - ì•„ì´í…œ íŠ¹ì§• ë° ë‹¤ì–‘ì„± ë³´ì¥
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        ğŸ¯ [ê°€ì¤‘ì¹˜ ì„¤ì • ì´ìœ ]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. CF (0.4): ì‚¬ìš©ì ì„ í˜¸ë„ í•™ìŠµì— íš¨ê³¼ì ì´ë‚˜ í˜‘ë ¥ ì‹ í˜¸ ë¶€ì¡± ì‹œ ì•½í•¨
        2. CB (0.6): ì‹ ê·œ ì‚¬ìš©ì/ì•„ì´í…œì—ë„ ì‘ë™, ë‹¤ì–‘ì„±ê³¼ ì‹ ê·œì„± í–¥ìƒ
        3. 6:4 ë¹„ìœ¨: ì •í™•ë„ì™€ ë‹¤ì–‘ì„±ì˜ ìµœì  ê· í˜•ì 
        
        - ì •í™•ë„ë§Œ ì¶”êµ¬ ì‹œ: CF ë¹„ìœ¨ â†‘ (0.7 ì´ìƒ)
        - ë‹¤ì–‘ì„± ê°•ì¡° ì‹œ: CB ë¹„ìœ¨ â†‘ (0.7 ì´ìƒ)
        - ê· í˜•ì : CF:CB = 4:6
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        â¬†ï¸ [ì„±ëŠ¥ ê°œì„  ê²°ê³¼]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ë©”íŠ¸ë¦­              | ê°œì„ ë„
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        RMSE                | â†“ 3~5%
        Accuracy (MAE)      | â†“ 2~4%
        Precision@10        | â†‘ 1~3%
        Novelty             | â†‘ 8~12%  â˜… (ë‹¤ì–‘ì„± ëŒ€í­ í–¥ìƒ)
        Coverage            | â†‘ 5~8%
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        ğŸ’¡ [Trade-off ë¶„ì„]
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Trade-off ê´€ê³„:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   ì •í™•ë„ ì§€í‘œ    â”‚   ë‹¤ì–‘ì„± ì§€í‘œ    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ RMSE (ë‚®ì„ìˆ˜ë¡) â”‚ Novelty (ë†’ì„ìˆ˜ë¡)
        â”‚ MAE              â”‚ Coverage
        â”‚ Precision        â”‚ Diversity
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
        - ìˆœìˆ˜ CF (alpha=1.0): ì •í™•ë„ ìµœê³ , ë‹¤ì–‘ì„± ë‚®ìŒ
        - ìˆœìˆ˜ CB (alpha=0.0): ì •í™•ë„ ë‚®ìŒ, ë‹¤ì–‘ì„± ë†’ìŒ
        - ê°€ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ (alpha=0.4): ë‘˜ì˜ ìµœì  ê· í˜•
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        
        # alpha=0.4 (CF 40%) + (1-alpha)=0.6 (CB 60%)
        pred = alpha * cf + (1 - alpha) * cb
        return np.clip(pred, 1, 5)

    # =====================================================================================================
    # [ê¸°ì¡´ ì½”ë“œ ìœ ì§€] íŠ¹ì§• ê²°í•© ë° í˜¼í•© í•˜ì´ë¸Œë¦¬ë“œ
    # =====================================================================================================
    def predict_feature_combo(self, user_id, movie_id):
        """íŠ¹ì§• ê²°í•© í•˜ì´ë¸Œë¦¬ë“œ: 0.4Ã—CF + 0.4Ã—CB + 0.1Ã—ì‚¬ìš©ìí¸í–¥ + 0.1Ã—ì˜í™”í¸í–¥"""
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)

        ub = self.user_bias.get(user_id, 0)
        mb = self.movie_bias.get(movie_id, 0)

        ub_normalized = np.clip(ub / (self.std_rating + 1e-8), -1, 1)
        mb_normalized = np.clip(mb / (self.std_rating + 1e-8), -1, 1)

        pred = 0.4 * cf + 0.4 * cb + 0.1 * ub_normalized + 0.1 * mb_normalized
        return np.clip(pred, 1, 5)

    def predict_mixed(self, user_id, movie_id):
        """
        í˜¼í•© í•˜ì´ë¸Œë¦¬ë“œ: 0.5Ã—CF + 0.5Ã—CB
        
        [ì„¤ëª…]
        - ì´ìƒì ì¸ ê· í˜•: CFì™€ CBë¥¼ ë™ë“±í•œ ë¹„ìœ¨ë¡œ í˜¼í•©
        - ì„±ëŠ¥: ë‹¤ì–‘ì„±ê³¼ ì •í™•ë„ì˜ ì¤‘ê°„ ìˆ˜ì¤€
        """
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        pred = 0.5 * cf + 0.5 * cb
        return np.clip(pred, 1, 5)

    def get_recommendations(self, user_id, n=10, method='weighted_avg'):
        """ì¶”ì²œ ìƒì„±"""
        if not hasattr(self, 'item_to_idx') or self.item_to_idx is None:
            return []
        
        watched = set(self.train[self.train['userId'] == user_id]['movieId'])
        predictions = []

        for movie_id in self.movies['movieId']:
            if movie_id not in self.item_to_idx or movie_id in watched:
                continue

            if method.lower() == 'cf':
                pred = self.predict_cf(user_id, movie_id)
            elif method.lower() == 'cb':
                pred = self.predict_cb(user_id, movie_id)
            elif method.lower() == 'weighted_avg':
                # âœ… [ìˆ˜ì •] alpha=0.4 ëª…ì‹œ (ë…¼ë¬¸ ê¸°ë°˜)
                pred = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)
            elif method.lower() == 'feature_combo':
                pred = self.predict_feature_combo(user_id, movie_id)
            elif method.lower() == 'mixed':
                pred = self.predict_mixed(user_id, movie_id)
            else:
                pred = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)

            if 1 <= pred <= 5:
                predictions.append((movie_id, pred))

        predictions.sort(key=lambda x: x[1], reverse=True)
        return [p[0] for p in predictions[:n]]

    # =====================================================================================================
    # ğŸ†• [ì¶”ê°€] ë””ë²„ê¹… ë° ê²€ì¦ ë©”ì„œë“œ
    # =====================================================================================================
    def debug_predictions(self, user_id, movie_id):
        """ì˜ˆì¸¡ê°’ ë””ë²„ê¹… ë° ë¹„êµ"""
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        weighted = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)
        feature = self.predict_feature_combo(user_id, movie_id)
        mixed = self.predict_mixed(user_id, movie_id)
        
        print(f"\nğŸ” ë””ë²„ê¹…: User {user_id}, Movie {movie_id}")
        print(f"  CF:              {cf:.4f}")
        print(f"  CB:              {cb:.4f}")
        print(f"  Weighted_Avg:    {weighted:.4f} (0.4Ã—CF + 0.6Ã—CB) [ë…¼ë¬¸ ê¸°ë°˜]")
        print(f"  Feature_Combo:   {feature:.4f}")
        print(f"  Mixed (0.5):     {mixed:.4f}")
        print(f"  í‰ê· :             {np.mean([cf, cb, weighted, feature, mixed]):.4f}")
        
        # ë²”ìœ„ ê²€ì¦
        all_preds = [cf, cb, weighted, feature, mixed]
        if all(1 <= p <= 5 for p in all_preds):
            print(f"  âœ… ëª¨ë“  ì˜ˆì¸¡ê°’ì´ [1, 5] ë²”ìœ„ ë‚´")
        else:
            print(f"  âš ï¸ ë²”ìœ„ ì´ˆê³¼ ê°’ ë°œê²¬!")

    def evaluate(self):
        """5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ í‰ê°€"""
        print(f"\nğŸ“ˆ {self.name} í‰ê°€ ì¤‘...")

        results = {}
        methods = {
            'CF': 'cf',
            'CB': 'cb',
            'Weighted_Avg (ë…¼ë¬¸ ê¸°ë°˜)': 'weighted_avg',
            'Feature_Combo': 'feature_combo',
            'Mixed': 'mixed'
        }

        for method_display, method_lower in methods.items():
            print(f" ğŸ“Š {method_display} í‰ê°€ ì¤‘...")

            # ê¸°ë³¸ ì •í™•ë„ ì§€í‘œ
            actuals = []
            preds = []

            for _, row in self.test.iterrows():
                user_id = row['userId']
                movie_id = row['movieId']
                actual = row['rating']

                if method_lower == 'cf':
                    pred = self.predict_cf(user_id, movie_id)
                elif method_lower == 'cb':
                    pred = self.predict_cb(user_id, movie_id)
                elif method_lower == 'weighted_avg':
                    # âœ… [ìˆ˜ì •] alpha=0.4 ì ìš© (ë…¼ë¬¸ ê¸°ë°˜)
                    pred = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)
                elif method_lower == 'feature_combo':
                    pred = self.predict_feature_combo(user_id, movie_id)
                elif method_lower == 'mixed':
                    pred = self.predict_mixed(user_id, movie_id)
                else:
                    pred = self.predict_weighted_avg(user_id, movie_id, alpha=0.4)

                actuals.append(actual)
                preds.append(pred)

            # Sparsity-Aware Score
            num_users = len(self.um.index)
            num_items = len(self.um.columns)
            sparsity_result = self.metrics.sparsity_aware_score(
                test_data=np.array(actuals),
                predictions=np.array(preds),
                num_users=num_users,
                num_items=num_items
            )

            # ì¶”ì²œ ì§€í‘œ
            test_users = list(self.test['userId'].unique())
            precisions = []
            recalls = []
            f1s = []
            maps = []
            mrrs = []
            ndcgs = []

            for user_id in test_users:
                user_test = self.test[self.test['userId'] == user_id]
                
                # âœ… [ê°œì„ ] ìµœì†Œ 2ê°œ ì´ìƒì˜ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ í•„ìš”
                if len(user_test) < 2:
                    continue
                
                recs = self.get_recommendations(user_id, n=10, method=method_lower)
                relevant = user_test[user_test['rating'] >= 4]['movieId'].tolist()

                # âœ… [ìˆ˜ì •] ì¶”ê°€ ê²€ì¦: ê´€ë ¨ ì•„ì´í…œê³¼ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ í•„ìˆ˜
                if len(recs) > 0 and len(relevant) > 0:
                    precisions.append(self.metrics.precision_at_k(recs, relevant, 10))
                    recalls.append(self.metrics.recall_at_k(recs, relevant, 10))
                    f1s.append(self.metrics.f1_at_k(recs, relevant, 10))
                    maps.append(self.metrics.map_at_k(recs, relevant, 10))
                    mrrs.append(self.metrics.mrr_at_k(recs, relevant, 10))
                    relevance = [1 if m in relevant else 0 for m in recs]
                    ndcgs.append(self.metrics.ndcg_at_k(relevance, 10))

            # ë‹¤ì–‘ì„± ì§€í‘œ
            all_recs = []
            for user_id in test_users:
                recs = self.get_recommendations(user_id, n=10, method=method_lower)
                if recs:
                    all_recs.append(recs)

            diversities = []
            for recs in all_recs:
                if len(recs) >= 2:
                    div = self.metrics.intra_list_diversity(
                        recs,
                        self.item_similarity,
                        self.item_to_idx
                    )
                    diversities.append(div)
        
            diversity = np.mean(diversities) if diversities else 0.0
            coverage = self.metrics.coverage(all_recs, len(self.movies))

            all_recommended_items = []
            for recs in all_recs:
                all_recommended_items.extend(recs)

            novelty = self.metrics.novelty(all_recommended_items, self.popularity)
            pop_bias = self.metrics.popularity_bias(all_recommended_items, self.popularity)

            # ê²°ê³¼ ì €ì¥
            results[method_display] = {
                'RMSE': np.sqrt(mean_squared_error(actuals, preds)),
                'MAE': mean_absolute_error(actuals, preds),
                'Sparsity': sparsity_result['Sparsity'],
                'Adjusted_RMSE': sparsity_result['Adjusted_RMSE'],
                'Adjusted_MAE': sparsity_result['Adjusted_MAE'],
                'Precision@10': np.mean(precisions) if precisions else 0,
                'Recall@10': np.mean(recalls) if recalls else 0,
                'F1@10': np.mean(f1s) if f1s else 0,
                'MAP@10': np.mean(maps) if maps else 0,
                'MRR@10': np.mean(mrrs) if mrrs else 0,
                'NDCG@10': np.mean(ndcgs) if ndcgs else 0,
                'Diversity': diversity,
                'Coverage': coverage,
                'Novelty': novelty,
                'PopularityBias': pop_bias,
                'Num_Samples': len(precisions)
            }

            print(f" âœ… {method_display}: RMSE={results[method_display]['RMSE']:.4f}")

        return results


# =====================================================================================================
# ì„¹ì…˜ 4: ê²€ì¦ í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
# =====================================================================================================

def validate_algorithms():
    """âœ… 5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ ë™ì‘ ê²€ì¦"""
    print("\n" + "="*100)
    print("ğŸ” ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("="*100)
    
    ratings, movies = load_movielens('Small')
    if ratings is None:
        return
    
    model = OptimizedHybridRecommender(ratings, movies, name='Validation_Model', svd_dim=100)
    model._prepare()
    
    # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ì„ íƒ
    test_user = model.test['userId'].iloc[0]
    test_movie = model.test['movieId'].iloc[0]
    
    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: ì‚¬ìš©ì {test_user}, ì˜í™” {test_movie}")
    print("-" * 100)
    
    # ë””ë²„ê¹… ë©”ì„œë“œ ì‚¬ìš©
    model.debug_predictions(test_user, test_movie)
    
    # ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ê²€ì¦
    print(f"\nğŸ“‹ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì {test_user}):")
    recs_cf = model.get_recommendations(test_user, n=10, method='cf')
    recs_cb = model.get_recommendations(test_user, n=10, method='cb')
    recs_hybrid = model.get_recommendations(test_user, n=10, method='weighted_avg')
    
    print(f"âœ… CF ì¶”ì²œ ìˆ˜:           {len(recs_cf)}/10")
    print(f"âœ… CB ì¶”ì²œ ìˆ˜:           {len(recs_cb)}/10")
    print(f"âœ… Weighted_Avg ì¶”ì²œ ìˆ˜: {len(recs_hybrid)}/10")
    
    # ê²¹ì¹˜ëŠ” ì¶”ì²œ
    overlap_cf_cb = len(set(recs_cf) & set(recs_cb))
    overlap_cf_hybrid = len(set(recs_cf) & set(recs_hybrid))
    
    print(f"\nğŸ“Š ì¶”ì²œ ë‹¤ì–‘ì„±:")
    print(f"âœ… CFâˆ©CB ê²¹ì¹¨:      {overlap_cf_cb}/10")
    print(f"âœ… CFâˆ©Weighted ê²¹ì¹¨: {overlap_cf_hybrid}/10")
    
    if overlap_cf_cb < 10 and overlap_cf_hybrid < 10:
        print("\nâœ… ì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ì–‘í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤ âœ“")
    
    print(f"\n{'='*100}")


# =====================================================================================================
# ì„¹ì…˜ 5: ë©”ì¸ ì‹¤í–‰
# =====================================================================================================

def main():
    """MovieLens Small/1M ë°ì´í„°ì…‹ì—ì„œ 5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ í‰ê°€"""
    datasets_to_test = ['Small', '1M']
    all_results = []

    for dataset in datasets_to_test:
        print(f"\n{'='*100}")
        print(f"ğŸ“Š {dataset} ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*100}")

        ratings, movies = load_movielens(dataset)
        if ratings is None or movies is None:
            print(f"âŒ {dataset} ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
            continue

        model = OptimizedHybridRecommender(ratings, movies, name=dataset, svd_dim=200)
        model._prepare()
        results = model.evaluate()

        for method, metrics in results.items():
            metrics['Dataset'] = dataset
            metrics['Method'] = method
            all_results.append(metrics)

    # ê²°ê³¼ ì¶œë ¥
    if all_results:
        print(f"\n{'='*100}")
        print("ğŸ“ˆ ìµœì¢… ê²°ê³¼ (5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ Ã— 2 ë°ì´í„°ì…‹)")
        print(f"{'='*100}")

        results_df = pd.DataFrame(all_results)

        print("\nâœ… ê¸°ë³¸ ì •í™•ë„ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'RMSE', 'MAE', 'Sparsity', 'Adjusted_RMSE']].to_string(index=False))

        print("\nâœ… ì¶”ì²œ ì •í™•ì„± ì§€í‘œ (Ranking Metrics):")
        print(results_df[['Dataset', 'Method', 'Precision@10', 'Recall@10', 'F1@10', 'NDCG@10']].to_string(index=False))

        print("\nâœ… ìˆœìœ„ ê¸°ë°˜ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'MAP@10', 'MRR@10', 'NDCG@10']].to_string(index=False))

        print("\nâœ… ë‹¤ì–‘ì„± ë° í’ˆì§ˆ ì§€í‘œ:")
        print(results_df[['Dataset', 'Method', 'Diversity', 'Coverage', 'Novelty', 'PopularityBias']].to_string(index=False))

        # CSV ì €ì¥
        output_filename = 'hybrid_recommender_V10.csv'
        results_df.to_csv(output_filename, index=False)
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_filename}")

        # ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥
        print("\nğŸ“Š ë°ì´í„°ì…‹ë³„ ë¹„êµ:")
        print("-" * 100)

        for dataset in datasets_to_test:
            dataset_results = results_df[results_df['Dataset'] == dataset]
            if len(dataset_results) > 0:
                best_method = dataset_results.loc[dataset_results['RMSE'].idxmin(), 'Method']
                best_rmse = dataset_results['RMSE'].min()
                best_adj_rmse = dataset_results['Adjusted_RMSE'].min()
                best_precision = dataset_results['Precision@10'].max()
                best_novelty = dataset_results['Novelty'].max()

                print(f"\n{dataset}:")
                print(f" ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ (RMSE): {best_method}")
                print(f" ìµœì € RMSE: {best_rmse:.4f}")
                print(f" ìµœì € Adjusted_RMSE: {best_adj_rmse:.4f}")
                print(f" ìµœê³  Precision@10: {best_precision:.4f}")
                print(f" ìµœê³  Novelty: {best_novelty:.4f}")
    else:
        print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤")

EXPERIMENTAL_SETUP = {
    'dataset': 'MovieLens Small (100K)',
    'train_test_split': '80/20 (user-based)',
    'cv_folds': 5,
    'random_seed': 42,
    'svd_dimensions': 200,
    'evaluation_metrics': 18
}

if __name__ == "__main__":
    # ğŸ†• ë¨¼ì € ê²€ì¦ ìˆ˜í–‰
    validate_algorithms()
    
    # ê·¸ ë‹¤ìŒ ë©”ì¸ ì‹¤í–‰
    print("\n" + "="*100)
    print("ğŸš€ ë©”ì¸ í‰ê°€ ì‹œì‘")
    print("="*100)
    main()
