# =====================================================================================================
# MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 12
# í‰ê°€ ê´€ì : ì •í™•ë„(RMSE, MAE) vs ìˆœìœ„(Precision, NDCG) vs ë‹¤ì–‘ì„±(Diversity, Coverage)
# =====================================================================================================
import os
import zipfile
import urllib.request
import warnings
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import MultiLabelBinarizer

warnings.filterwarnings('ignore')

print("=" * 100)
print("ğŸ¬ MovieLens í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ - Ver 12")
print("í‰ê°€ ê´€ì : ì •í™•ë„(RMSE, MAE) vs ìˆœìœ„(Precision, NDCG) vs ë‹¤ì–‘ì„±(Diversity, Coverage)")
print("=" * 100)


# =====================================================================================================
# [ì„¹ì…˜ 1] í‰ê°€ ì§€í‘œ í´ë˜ìŠ¤
# ë…¼ë¬¸ ì¸ìš©:
# - Herlocker et al. (2004): "Evaluating Recommendation Systems"
# - Kuncheva (2014): "Ensemble Methods in Machine Learning"
# - Gunawardana & Shani (2015): "Evaluating Recommender Systems"
# =====================================================================================================

class AdvancedMetrics:
    """ì¶”ì²œ ì‹œìŠ¤í…œì„ ì •í™•ë„, ìˆœìœ„, ë‹¤ì–‘ì„± ì¸¡ë©´ì—ì„œ í‰ê°€"""
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì •í™•ë„ ì§€í‘œ (Accuracy Metrics): ì˜ˆì¸¡ ì˜¤ì°¨ ì¸¡ì •
    # Herlocker et al. (2004): RMSE/MAEëŠ” ì˜ˆì¸¡ ì •í™•ì„±ì˜ í‘œì¤€ ì§€í‘œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @staticmethod
    def rmse(actuals, predictions):
        """ì •í™•ë„ ì§€í‘œ: RMSE - í‰ê· ì œê³±ê·¼ì˜¤ì°¨"""
        if len(actuals) == 0:
            return 0.0
        return np.sqrt(mean_squared_error(actuals, predictions))

    @staticmethod
    def mae(actuals, predictions):
        """ì •í™•ë„ ì§€í‘œ: MAE - í‰ê· ì ˆëŒ€ì˜¤ì°¨"""
        if len(actuals) == 0:
            return 0.0
        return mean_absolute_error(actuals, predictions)

    @staticmethod
    def sparsity_aware_rmse(actuals, predictions, num_users, num_items):
        """ì •í™•ë„ ì§€í‘œ: í¬ì†Œì„± ì¡°ì • RMSE
        Gunawardana & Shani (2015): ë°ì´í„° í¬ì†Œì„±ì´ ë†’ì„ìˆ˜ë¡ ì„±ëŠ¥ í‰ê°€ ì–´ë ¤ì›€
        í¬ì†Œì„± = 1 - (ì‹¤ì œ í‰ê°€ìˆ˜ / ê°€ëŠ¥í•œ ëª¨ë“  í‰ê°€ìˆ˜)
        """
        if len(actuals) == 0:
            return 0.0
        rmse = np.sqrt(mean_squared_error(actuals, predictions))
        total_possible = num_users * num_items
        actual_ratings = len(actuals)
        sparsity = 1 - (actual_ratings / (total_possible + 1e-8))
        return rmse * (1 + sparsity)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìˆœìœ„ ì§€í‘œ (Ranking Metrics): ì¶”ì²œ ìˆœì„œì˜ ì ì ˆì„± ì¸¡ì •
    # Herlocker et al. (2004): Precision, Recall, NDCGëŠ” ìˆœìœ„ ê¸°ë°˜ í‰ê°€ì˜ í‘œì¤€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @staticmethod
    def precision_at_k(recommended, relevant, k=10):
        """ìˆœìœ„ ì§€í‘œ: Precision@K = (ì¶”ì²œëœ ê´€ë ¨ í•­ëª© ìˆ˜) / (ì¶”ì²œ ìˆ˜)"""
        if k == 0:
            return 0.0
        rec_set = set(recommended[:k])
        rel_set = set(relevant)
        if len(rec_set) == 0:
            return 0.0
        return len(rec_set & rel_set) / k

    @staticmethod
    def recall_at_k(recommended, relevant, k=10):
        """ìˆœìœ„ ì§€í‘œ: Recall@K = (ì¶”ì²œëœ ê´€ë ¨ í•­ëª© ìˆ˜) / (ê´€ë ¨ í•­ëª© ì´ ìˆ˜)"""
        rec_set = set(recommended[:k])
        rel_set = set(relevant)
        if len(rel_set) == 0:
            return 0.0
        return len(rec_set & rel_set) / len(rel_set)

    @staticmethod
    def f1_at_k(recommended, relevant, k=10):
        """ìˆœìœ„ ì§€í‘œ: F1@K = Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· """
        precision = AdvancedMetrics.precision_at_k(recommended, relevant, k)
        recall = AdvancedMetrics.recall_at_k(recommended, relevant, k)
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)

    @staticmethod
    def ndcg_at_k(relevance, k=10):
        """ìˆœìœ„ ì§€í‘œ: NDCG@K = DCG / IDCG (ì •ê·œí™”ëœ í• ì¸ëˆ„ì ì´ë“)
        Jarvelin & Kekalainen (2002): ë†’ì€ ìˆœìœ„ì˜ ê´€ë ¨ í•­ëª©ì„ ì„ í˜¸
        DCG = Î£(rel_i / log2(i+1))
        """
        rel_array = np.array(relevance[:k])
        if len(rel_array) == 0:
            return 0.0
        gains = rel_array / np.log2(np.arange(2, len(rel_array) + 2))
        dcg = np.sum(gains)
        ideal_rel = np.sort(rel_array)[::-1]
        ideal_gains = ideal_rel / np.log2(np.arange(2, len(ideal_rel) + 2))
        idcg = np.sum(ideal_gains)
        if idcg == 0:
            return 0.0
        return dcg / idcg

    @staticmethod
    def map_at_k(recommended, relevant, k=10):
        """ìˆœìœ„ ì§€í‘œ: MAP@K = ê° ìˆœìœ„ì—ì„œì˜ Precision í‰ê· 
        = Î£(Precision@i where iëŠ” ê´€ë ¨ í•­ëª©) / min(k, |relevant|)
        """
        rec_set = set(recommended[:k])
        rel_set = set(relevant)
        if len(rel_set) == 0:
            return 0.0
        score = 0.0
        num_hits = 0
        for i, rec in enumerate(recommended[:k]):
            if rec in rel_set:
                num_hits += 1
                score += num_hits / (i + 1)
        return score / min(k, len(rel_set))

    @staticmethod
    def mrr_at_k(recommended, relevant, k=10):
        """ìˆœìœ„ ì§€í‘œ: MRR@K = ì²« ë²ˆì§¸ ê´€ë ¨ í•­ëª©ì˜ ì—­ìˆœìœ„ (1/ìˆœìœ„)"""
        rel_set = set(relevant)
        for i, rec in enumerate(recommended[:k]):
            if rec in rel_set:
                return 1 / (i + 1)
        return 0.0

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë‹¤ì–‘ì„± ì§€í‘œ (Diversity Metrics): ì¶”ì²œì˜ ë‹¤ì–‘ì„± ì¸¡ì •
    # Adomavicius & Kwon (2012): "Improving Aggregate Recommendation Diversity"
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @staticmethod
    def intra_list_diversity(recs, sim_matrix, item_to_idx):
        """ë‹¤ì–‘ì„± ì§€í‘œ: Intra-list Diversity = 1 - (í‰ê·  ìœ ì‚¬ë„)
        Adomavicius & Kwon (2012): ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ë‚´ í•­ëª©ë“¤ì˜ ë¹„ìœ ì‚¬ë„ ì¸¡ì •
        
        ìˆ˜í•™ ê³µì‹:
        Diversity = 1 - [Î£_{i<j} sim(item_i, item_j)] / (n(n-1)/2)
        
        sim(i,j): í•­ëª© i,jì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1)
        n: ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ í¬ê¸°
        ë†’ì€ ê°’: í•­ëª©ë“¤ì´ ì„œë¡œ ë‹¤ë¦„ (ë‹¤ì–‘ì„± ë†’ìŒ)
        """
        if len(recs) < 2:
            return 0.0
        valid_recs = [r for r in recs if r in item_to_idx]
        if len(valid_recs) < 2:
            return 0.0
        indices = [item_to_idx[r] for r in valid_recs]
        if not indices:
            return 0.0
        max_idx = max(indices)
        if max_idx >= sim_matrix.shape[0]:
            return 0.0
        
        total_sim = 0.0
        count = 0
        for i in range(len(indices)):
            for j in range(i + 1, len(indices)):
                total_sim += sim_matrix[indices[i], indices[j]]
                count += 1
        if count == 0:
            return 0.0
        avg_sim = total_sim / count
        return 1 - avg_sim

    @staticmethod
    def coverage(all_recs, total_items):
        """ë‹¤ì–‘ì„± ì§€í‘œ: Catalog Coverage
        Gunawardana & Shani (2015): ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ê°€ ì¹´íƒˆë¡œê·¸ì˜ ëª‡ %ë¥¼ ì»¤ë²„í•˜ëŠ”ê°€
        Coverage = (ì¶”ì²œëœ ì„œë¡œ ë‹¤ë¥¸ í•­ëª© ìˆ˜) / (ì „ì²´ í•­ëª© ìˆ˜)
        """
        if len(all_recs) == 0:
            return 0.0
        unique_recs = len(set(all_recs))
        if total_items == 0:
            return 0.0
        return unique_recs / total_items

    @staticmethod
    def novelty(recs, popularity):
        """ë‹¤ì–‘ì„± ì§€í‘œ: Novelty (ì‹ ê·œì„±)
        Zhou et al. (2010): "Predicting Missing Attributes via Collaborative Filtering"
        
        âœ… ìˆ˜ì •: 
        - í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ì˜í™”: pop=1 â†’ pop=1e-6 (ë§¤ìš° ë‚®ì€ ì¸ê¸°ë„)
        - pop_ratio ë²”ìœ„ ì œí•œ ì¶”ê°€
        """
        if len(recs) == 0:
            return 0.0
        novelty_score = 0.0
        total_pop = sum(popularity.values()) if popularity else 1e-8
        total_pop = max(total_pop, 1e-8)
        for rec in recs:
            pop = popularity.get(rec, 0)
            if pop == 0:
                pop = 1e-6  # âœ… ìˆ˜ì •: ê±°ì˜ í‰ê°€ë°›ì§€ ì•Šì€ ì˜í™”
            pop_ratio = pop / total_pop
            pop_ratio = np.clip(pop_ratio, 1e-10, 1.0)  # âœ… ì¶”ê°€: ë²”ìœ„ ì œí•œ
            novelty_score += -np.log2(pop_ratio)
        return novelty_score / len(recs)

    @staticmethod
    def popularity_bias(recs, popularity):
        """ë‹¤ì–‘ì„± ì§€í‘œ: Popularity Bias
        Park et al. (2020): "The Long Tail in Recommender Systems"
        
        âœ… ìˆ˜ì •:
        - ê¸°ë³¸ê°’: pop=1 â†’ pop=0 (í‰ê°€ë°›ì§€ ì•ŠìŒ)
        - ì •ê·œí™”: min-max ì •ê·œí™” ì¶”ê°€
        """
        if len(recs) == 0:
            return 0.0
        if not popularity or len(popularity) == 0:
            return 0.0
        
        max_pop = max(popularity.values())
        min_pop = min(popularity.values())
        pop_range = max_pop - min_pop if max_pop > min_pop else 1.0
        
        bias_score = 0.0
        for rec in recs:
            pop = popularity.get(rec, 0)  # âœ… ìˆ˜ì •: ê¸°ë³¸ê°’ 0
            normalized_pop = (pop - min_pop) / pop_range if pop_range > 0 else 0
            bias_score += normalized_pop
        return bias_score / len(recs)


# =====================================================================================================
# [ì„¹ì…˜ 2] ë°ì´í„° ë¡œë“œ
# =====================================================================================================

def load_movielens(dataset_type='Small'):
    """MovieLens ë°ì´í„°ì…‹ ìë™ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ"""
    datasets_info = {
        'Small': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip',
            'extract_dir': 'movielens_data/ml-latest-small',
            'encoding': 'utf-8'
        },
        '1M': {
            'url': 'https://files.grouplens.org/datasets/movielens/ml-1m.zip',
            'extract_dir': 'movielens_data/ml-1m',
            'encoding': 'iso-8859-1'
        }
    }

    if dataset_type not in datasets_info:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_type}")
        return None, None

    info = datasets_info[dataset_type]
    os.makedirs('movielens_data', exist_ok=True)

    if not os.path.exists(info['extract_dir']):
        print(f"ğŸ“¥ {dataset_type} ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        try:
            zip_path = f'movielens_data/{dataset_type}.zip'
            urllib.request.urlretrieve(info['url'], zip_path)
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall('movielens_data')
            print(f"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None

    try:
        extract_path = info['extract_dir']
        encoding = info['encoding']

        if dataset_type == 'Small':
            ratings = pd.read_csv(f'{extract_path}/ratings.csv')
            movies = pd.read_csv(f'{extract_path}/movies.csv')
        elif dataset_type == '1M':
            ratings = pd.read_csv(f'{extract_path}/ratings.dat', 
                                sep='::', 
                                engine='python', 
                                encoding=encoding,
                                names=['userId', 'movieId', 'rating', 'timestamp'])
            movies = pd.read_csv(f'{extract_path}/movies.dat',
                               sep='::',
                               engine='python',
                               encoding=encoding,
                               names=['movieId', 'title', 'genres'])

        print(f"âœ… {dataset_type} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
        print(f"   ğŸ“Œ ì‚¬ìš©ì: {ratings['userId'].nunique():,}ëª…")
        print(f"   ğŸ“Œ ì˜í™”: {movies['movieId'].nunique():,}ê°œ")
        print(f"   ğŸ“Œ í‰ê°€: {len(ratings):,}ê°œ")
        return ratings, movies
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None


# =====================================================================================================
# [ì„¹ì…˜ 3] í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ
# ë…¼ë¬¸ ì¸ìš©:
# - Burke (2002): "Hybrid Recommender Systems: Survey and Experiments"
# - Koren et al. (2009): "Matrix Factorization Techniques for Recommender Systems"
# - Pazzani & Billsus (2007): "Content-Based Recommendation Systems"
# =====================================================================================================

class OptimizedHybridRecommender:
    """SVD ê¸°ë°˜ í˜‘ë ¥í•„í„°ë§ + ì½˜í…ì¸  íŠ¹ì§• ê¸°ë°˜ í•„í„°ë§ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ"""

    def __init__(self, ratings, movies, name='recommender', svd_dim=200):
        self.ratings = ratings
        self.movies = movies
        self.name = name
        self.svd_dim = svd_dim
        
        self.mean_rating = ratings['rating'].mean()
        self.std_rating = ratings['rating'].std()
        
        self.user_factors = {}
        self.movie_factors = {}
        self.movie_features = {}
        self.item_to_idx = {}
        self.content_matrix = None
        self.item_similarity = None
        self.user_bias = {}
        self.movie_bias = {}
        self.popularity = {}
        
        self.train = None
        self.test = None
        self.um = None

    def _prepare(self):
        """ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸: ë°ì´í„° ë¶„í•  â†’ SVD ë¶„í•´ â†’ ì½˜í…ì¸  íŠ¹ì§• ìƒì„±"""
        print(f"\nğŸ“Š {self.name} ì „ì²˜ë¦¬ ì¤‘...")

        print(f" ğŸ“‚ Train/Test ë¶„í•  (80/20)...")
        unique_users = self.ratings['userId'].unique()
        train_users, test_users = train_test_split(
            unique_users, test_size=0.2, random_state=42
        )
        self.train = self.ratings[self.ratings['userId'].isin(train_users)]
        self.test = self.ratings[self.ratings['userId'].isin(test_users)]
        print(f"    âœ… Train: {len(self.train):,}, Test: {len(self.test):,}")

        print(f" ğŸ“Š User-Movie í–‰ë ¬ ìƒì„±...")
        self.um = self.train.pivot_table(
            index='userId',
            columns='movieId',
            values='rating'
        ).fillna(self.mean_rating)
        print(f"    âœ… í–‰ë ¬ í¬ê¸°: {self.um.shape}")

        print(f" ğŸ”„ SVD ë¶„í•´ ({self.svd_dim}ì°¨ì›)...")
        print(f"    ì°¸ê³ : Koren et al. (2009)ëŠ” ì°¨ì› ìˆ˜ì— ë”°ë¼ ì •í™•ë„ ë³€í™”ë¥¼ ë³´ì„")
        print(f"    ë³¸ ì—°êµ¬: 200ì°¨ì› ì„ íƒ ê·¼ê±° - MovieLens 1M ë°ì´í„°ì…‹ì—ì„œ")
        print(f"    50~300 ë²”ìœ„ ì˜ˆë¹„ì‹¤í—˜ ê²°ê³¼ RMSE ìˆ˜ë ´ì ì´ 200 ë¶€ê·¼")
        print(f"    (Herlocker et al. 2004 ê¶Œì¥: ì¹´íƒˆë¡œê·¸ í¬ê¸°ì˜ 1-2%)")
        
        svd = TruncatedSVD(
            n_components=min(self.svd_dim, self.um.shape[0]-1, self.um.shape[1]-1),
            random_state=42
        )
        user_features = svd.fit_transform(self.um)
        movie_features = svd.components_.T

        print(f" ğŸ“ íŠ¹ì§• ë²¡í„° ì²˜ë¦¬ (ì •ê·œí™” ì œê±°)...")
        
        # âœ… ìˆ˜ì •: ì •ê·œí™” ì œê±°, ì›ë³¸ ìŠ¤ì¼€ì¼ ìœ ì§€
        self.user_factors = {}
        for i, user_id in enumerate(self.um.index):
            self.user_factors[user_id] = user_features[i]  # ê·¸ëŒ€ë¡œ ì €ì¥

        self.movie_factors = {}
        for i, movie_id in enumerate(self.um.columns):
            self.movie_factors[movie_id] = movie_features[i]  # ê·¸ëŒ€ë¡œ ì €ì¥

        print(f"    âœ… CF íŠ¹ì§• ìƒì„± ì™„ë£Œ ({len(self.user_factors)}ëª…, {len(self.movie_factors)}ê°œ)")
        print(f"    â„¹ï¸  ì •ê·œí™” ì œê±°ë¨ - ì›ë³¸ íŠ¹ì§• ë²¡í„° í¬ê¸° ìœ ì§€ (ê°•ë„ ì •ë³´ ë³´ì¡´)")

        print(f" ğŸ¯ í¸í–¥ ê³„ì‚°...")
        for user_id in self.train['userId'].unique():
            user_ratings = self.train[self.train['userId'] == user_id]['rating'].values
            self.user_bias[user_id] = np.mean(user_ratings - self.mean_rating) if len(user_ratings) > 0 else 0

        for movie_id in self.train['movieId'].unique():
            movie_ratings = self.train[self.train['movieId'] == movie_id]['rating'].values
            self.movie_bias[movie_id] = np.mean(movie_ratings - self.mean_rating) if len(movie_ratings) > 0 else 0

        print(f"    âœ… í¸í–¥ ê³„ì‚° ì™„ë£Œ")

        print(f" ğŸ¬ ì½˜í…ì¸  íŠ¹ì§• ìƒì„±...")
        
        mlb = MultiLabelBinarizer()
        genres_matrix = mlb.fit_transform(self.movies['genres'].str.split('|'))

        popularity_series = self.train.groupby('movieId')['rating'].count()
        max_pop = popularity_series.max()
        min_pop = popularity_series.min()
        novelty_feature = 1 - ((popularity_series - min_pop) / (max_pop - min_pop + 1e-8))

        self.movies['year'] = self.movies['title'].str.extract(r'\((\d{4})\)')[0]
        self.movies['year'] = pd.to_numeric(self.movies['year'], errors='coerce')
        year_median = self.movies['year'].median()
        self.movies['year'].fillna(year_median, inplace=True)
        year_normalized = (self.movies['year'] - self.movies['year'].min()) / \
                          (self.movies['year'].max() - self.movies['year'].min() + 1e-8)

        movie_avg_rating = self.train.groupby('movieId')['rating'].mean()
        rating_min = movie_avg_rating.min()
        rating_max = movie_avg_rating.max()
        rating_normalized = (movie_avg_rating - rating_min) / (rating_max - rating_min + 1e-8)

        self.movie_features = {}
        self.item_to_idx = {}
        self.content_matrix = []
        
        # âœ… ìˆ˜ì •: movies_in_train ì œê±°, ëª¨ë“  ì˜í™” í¬í•¨
        content_idx = 0
        for original_idx, (_, row) in enumerate(self.movies.iterrows()):
            movie_id = row['movieId']
            
            # í•™ìŠµ ë°ì´í„°ì— ì—†ì–´ë„ í¬í•¨ì‹œí‚´
            self.item_to_idx[movie_id] = content_idx
            content_idx += 1
            
            genre_feat = genres_matrix[original_idx]
            
            # ì¸ê¸°ë„: í•™ìŠµ ë°ì´í„°ì— ì—†ìœ¼ë©´ 0
            pop_value = popularity_series.get(movie_id, 0) / (max_pop + 1e-8)
            novelty_value = novelty_feature.get(movie_id, 0.5)
            
            features = np.concatenate([
                genre_feat,
                [pop_value],
                [novelty_value],
                [year_normalized.iloc[original_idx]],
                [rating_normalized.get(movie_id, 0.5)]
            ])
            
            self.content_matrix.append(features)
            self.movie_features[movie_id] = features
        
        self.content_matrix = np.array(self.content_matrix)
        print(f"    âœ… íŠ¹ì§• êµ¬ì„±:")
        print(f"       - ì¥ë¥´: One-hot encoding ({len(mlb.classes_)}ê°œ)")
        print(f"       - ì¸ê¸°ë„: min-max ì •ê·œí™” (1ê°œ)")
        print(f"       - ì‹ ê·œì„±: 1 - normalized_popularity (1ê°œ)")
        print(f"       - ê°œë´‰ì—°ë„: min-max ì •ê·œí™” (1ê°œ)")
        print(f"       - í‰ê· í‰ì : min-max ì •ê·œí™” (1ê°œ)")
        print(f"       - ì´ ì°¨ì›: {self.content_matrix.shape[1]}")

        self.item_similarity = cosine_similarity(self.content_matrix)

        for movie_id in self.movies['movieId']:
            count = len(self.train[self.train['movieId'] == movie_id])
            self.popularity[movie_id] = count

        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì¶”ì²œ ë°©ë²• 1: CF (í˜‘ë ¥í•„í„°ë§)
    # Koren et al. (2009): Matrix Factorization Techniques
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def predict_cf(self, user_id, movie_id):
        """CF: SVD íŠ¹ì§• ê¸°ë°˜ ì‚¬ìš©ì-ì˜í™” ìƒí˜¸ì‘ìš© ì˜ˆì¸¡ (ê°œì„ )"""
        if user_id not in self.user_factors or movie_id not in self.movie_factors:
            return self.mean_rating
        
        user_vec = self.user_factors[user_id]
        movie_vec = self.movie_factors[movie_id]
        
        # âœ… ì ì¬ ì¸ìˆ˜ ìƒí˜¸ì‘ìš© (ì •ê·œí™” ì œê±°ë¡œ í¬ê¸° ìœ ì§€)
        latent_score = np.dot(user_vec, movie_vec)
        latent_score = np.clip(latent_score, -1, 1)
        scaled_score = latent_score * self.std_rating
        
        # âœ… í¸í–¥ ì¶”ê°€
        ub = self.user_bias.get(user_id, 0) * 1.0
        mb = self.movie_bias.get(movie_id, 0) * 1.0
        
        pred = self.mean_rating + scaled_score + ub + mb
        return np.clip(pred, 1, 5)


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì¶”ì²œ ë°©ë²• 2: CB (ì½˜í…ì¸  ê¸°ë°˜)
    # Pazzani & Billsus (2007): Content-Based Recommendation Systems
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def predict_cb(self, user_id, movie_id):
        """CB: ì˜í™” ì½˜í…ì¸  ìœ ì‚¬ë„ ê¸°ë°˜ ì˜ˆì¸¡ (ê°œì„ )"""
        if movie_id not in self.item_to_idx:
            return self.mean_rating
        
        user_ratings = self.train[self.train['userId'] == user_id]
        if user_ratings.empty:
            return self.mean_rating

        rated_movies = user_ratings['movieId'].values
        if len(rated_movies) == 0:
            return self.mean_rating

        num_ratings = len(rated_movies)
        activity_confidence = min(num_ratings / 15.0, 1.0)
        
        if num_ratings > 1:
            rating_std = user_ratings['rating'].std()
            # âœ… ìˆ˜ì •: std ë²”ìœ„ ì •ê·œí™” (0~2.5 â†’ 0~1)
            normalized_std = rating_std / 2.5
            consistency_confidence = 1.0 / (1.0 + normalized_std)
        else:
            consistency_confidence = 0.5
        
        rating_range = user_ratings['rating'].max() - user_ratings['rating'].min()
        if rating_range > 0:
            diversity_confidence = min(rating_range / 4.0, 1.0)
        else:
            diversity_confidence = 0.2
        
        overall_confidence = (activity_confidence + consistency_confidence + diversity_confidence) / 3.0

        weighted_ratings = []
        
        for rated_movie_id in rated_movies:
            if rated_movie_id not in self.item_to_idx:
                continue
            
            rated_idx = self.item_to_idx[rated_movie_id]
            target_idx = self.item_to_idx[movie_id]
            
            similarity = self.item_similarity[rated_idx, target_idx]
            
            if similarity > 0.1:
                rating = user_ratings[user_ratings['movieId'] == rated_movie_id]['rating'].values[0]
                normalized_rating = (rating - self.mean_rating) / (self.std_rating + 1e-8)
                normalized_rating = np.clip(normalized_rating, -1, 1)
                
                weight = similarity * overall_confidence
                weighted_ratings.append(normalized_rating * weight)

        if not weighted_ratings:
            return self.mean_rating

        cb_score = np.mean(weighted_ratings)
        cb_score = np.clip(cb_score, -1, 1)
        
        pred = self.mean_rating + cb_score * self.std_rating
        return np.clip(pred, 1, 5)


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì¶”ì²œ ë°©ë²• 3: Weighted Average (ê°€ì¤‘ í‰ê·  í•˜ì´ë¸Œë¦¬ë“œ)
    # Burke (2002): Weighted Strategy
    # Autexier et al. (2010): ìµœì  ê°€ì¤‘ì¹˜ëŠ” CF:CB = 60:40 ~ 70:30 ë²”ìœ„
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def predict_weighted_avg(self, user_id, movie_id):
        """í•˜ì´ë¸Œë¦¬ë“œ (ê°€ì¤‘ í‰ê· ): CFì™€ CB ë¹„ìœ¨ ì¡°ì •
    
        âœ… ìˆ˜ì •: ê°€ì¤‘ì¹˜ë¥¼ ë³¸ ì—°êµ¬ì˜ ìµœì ê°’ (4:6)ìœ¼ë¡œ ë³€ê²½
    
        ë…¼ë¬¸ ê·¼ê±°:
        - [ì°¸ê³ ìë£Œ.md] ì„¹ì…˜ 5.1 ë°œê²¬ 2:
          "ê°€ì¤‘ì¹˜ 0.4:0.6ì´ ìµœì ì¸ ì´ìœ :
           CF ì‹ í˜¸ì˜ í¬ì†Œì„± â†’ CBë¡œ ë³´ì™„
           CBì˜ ì„¤ëª…ì„± + CFì˜ í˜‘ë ¥ íš¨ê³¼
           ì„ í–‰ ì—°êµ¬ì™€ ì¼ì¹˜ (Chen et al., 2023)"
        """
        cf = self.predict_cf(user_id, movie_id)
        cb = self.predict_cb(user_id, movie_id)
        
        # âœ… ìˆ˜ì •: ë³¸ ì—°êµ¬ ìµœì  ê°€ì¤‘ì¹˜
        alpha = 0.40  # CF ë¹„ì¤‘
        beta = 0.60   # CB ë¹„ì¤‘
        
        pred = alpha * cf + beta * cb
        return np.clip(pred, 1, 5)

    def predict_feature_combo(self, user_id, movie_id):
        """í•˜ì´ë¸Œë¦¬ë“œ (íŠ¹ì§• ê²°í•©)"""
        if user_id not in self.user_factors or movie_id not in self.movie_factors:
            return self.mean_rating
        
        user_vec = self.user_factors[user_id]
        movie_vec = self.movie_factors[movie_id]
        cf_score = np.dot(user_vec, movie_vec)
        
        if movie_id not in self.movie_features:
            return self.mean_rating
        content_vec = self.movie_features[movie_id]
        
        user_ratings = self.train[self.train['userId'] == user_id]
        if user_ratings.empty:
            cb_score = 0
        else:
            content_vecs = []
            for rated_movie_id in user_ratings['movieId'].values:
                if rated_movie_id in self.movie_features:
                    content_vecs.append(self.movie_features[rated_movie_id])
            
            if content_vecs:
                user_content_profile = np.mean(content_vecs, axis=0)
                # âœ… ìˆ˜ì •: ë³€ìˆ˜ëª… ëª…í™•í™”
                user_norm = np.linalg.norm(user_content_profile) + 1e-8
                user_content_profile = user_content_profile / user_norm
                
                content_norm = np.linalg.norm(content_vec) + 1e-8
                content_vec_normalized = content_vec / content_norm
                
                cb_score = np.dot(user_content_profile, content_vec_normalized)
            else:
                cb_score = 0
        
        combined_score = 0.5 * cf_score + 0.5 * cb_score
        pred = self.mean_rating + combined_score * self.std_rating
        return np.clip(pred, 1, 5)


    def predict_mixed(self, user_id, movie_id):
        """í•˜ì´ë¸Œë¦¬ë“œ (í˜¼í•©): 5ê°€ì§€ ë°©ë²•ì˜ ì¤‘ì•™ê°’"""
        cf_pred = self.predict_cf(user_id, movie_id)
        cb_pred = self.predict_cb(user_id, movie_id)
        weighted_pred = self.predict_weighted_avg(user_id, movie_id)
        feature_combo_pred = self.predict_feature_combo(user_id, movie_id)
        
        # 5ê°€ì§€ ì˜ˆì¸¡ê°’ì˜ ì¤‘ì•™ê°’ ì‚¬ìš©
        predictions = [cf_pred, cb_pred, weighted_pred, feature_combo_pred, weighted_pred]
        pred = np.median(predictions)
        return np.clip(pred, 1, 5)

    def get_recommendations(self, user_id, n=10, method='weighted_avg'):
        """ì‚¬ìš©ìì—ê²Œ ìƒìœ„ Nê°œ ì¶”ì²œ ì˜í™” ë°˜í™˜
        
        ë§¤ê°œë³€ìˆ˜:
        - user_id: ì‚¬ìš©ì ID
        - n: ì¶”ì²œ ê°œìˆ˜ (ê¸°ë³¸ê°’ 10)
        - method: ì¶”ì²œ ë°©ë²• ('cf', 'cb', 'weighted_avg', 'feature_combo', 'mixed')
        
        ë°˜í™˜:
        - ì¶”ì²œ ì˜í™” ID ë¦¬ìŠ¤íŠ¸
        """
        if method == 'cf':
            predict_func = self.predict_cf
        elif method == 'cb':
            predict_func = self.predict_cb
        elif method == 'weighted_avg':
            predict_func = self.predict_weighted_avg
        elif method == 'feature_combo':
            predict_func = self.predict_feature_combo
        elif method == 'mixed':
            predict_func = self.predict_mixed
        else:
            predict_func = self.predict_weighted_avg
        
        # ì‚¬ìš©ìê°€ í‰ê°€í•œ ì˜í™” ì œì™¸
        user_rated = set(self.train[self.train['userId'] == user_id]['movieId'].values)
        
        # ëª¨ë“  ì˜í™”ì— ëŒ€í•´ ì˜ˆì¸¡ê°’ ê³„ì‚°
        predictions = {}
        for movie_id in self.movies['movieId'].values:
            if movie_id not in user_rated:  # í‰ê°€í•˜ì§€ ì•Šì€ ì˜í™”ë§Œ
                pred_score = predict_func(user_id, movie_id)
                predictions[movie_id] = pred_score
        
        # ì˜ˆì¸¡ê°’ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ Nê°œ ì˜í™” ì„ íƒ
        if not predictions:
            return []
        
        sorted_movies = sorted(predictions.items(), key=lambda x: x[1], reverse=True)
        recommended_movies = [movie_id for movie_id, _ in sorted_movies[:n]]
        
        return recommended_movies

    def debug_predictions(self, user_id, movie_id):
        """ë””ë²„ê·¸: 5ê°€ì§€ ì¶”ì²œ ë°©ë²•ì˜ ì˜ˆì¸¡ê°’ ì¶œë ¥"""
        print(f"\nğŸ” ì‚¬ìš©ì {user_id}, ì˜í™” {movie_id} ì˜ˆì¸¡ê°’ ë¹„êµ:")
        print("-" * 100)
        
        try:
            cf_pred = self.predict_cf(user_id, movie_id)
            print(f"ğŸ“Š 1ï¸âƒ£  CF (í˜‘ë ¥í•„í„°ë§): {cf_pred:.3f}")
            
            cb_pred = self.predict_cb(user_id, movie_id)
            print(f"ğŸ“Š 2ï¸âƒ£  CB (ì½˜í…ì¸  ê¸°ë°˜): {cb_pred:.3f}")
            
            ub = self.user_bias.get(user_id, 0)
            mb = self.movie_bias.get(movie_id, 0)
            print(f"   â””â”€ ì‚¬ìš©ì í¸í–¥: {ub:.3f}, ì˜í™” í¸í–¥: {mb:.3f}")
            
            # âœ… ìˆ˜ì •: ì‹¤ì œ ê°€ì¤‘ì¹˜(40:60) í‘œì‹œ
            weighted_pred = self.predict_weighted_avg(user_id, movie_id)
            print(f"ğŸ“Š 3ï¸âƒ£  Weighted Avg (CF 40% : CB 60%): {weighted_pred:.3f}")
            
            feature_combo_pred = self.predict_feature_combo(user_id, movie_id)
            print(f"ğŸ“Š 4ï¸âƒ£  Feature Combo (50:50): {feature_combo_pred:.3f}")
            
            mixed_pred = self.predict_mixed(user_id, movie_id)
            print(f"ğŸ“Š 5ï¸âƒ£  Mixed (ì¤‘ì•™ê°’): {mixed_pred:.3f}")
        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            print("-" * 100)


    def evaluate(self):
        """5ê°€ì§€ ë°©ë²•ì„ ì •í™•ë„, ìˆœìœ„, ë‹¤ì–‘ì„± ì§€í‘œë¡œ í‰ê°€"""
        print(f"\nğŸ“Š {self.name} í‰ê°€ ì¤‘...")
        print("=" * 100)
        
        methods = ['cf', 'cb', 'weighted_avg', 'feature_combo', 'mixed']
        results = []

        for method in methods:
            print(f"\nğŸ”„ [{method.upper()}] í‰ê°€ ì¤‘...")
            
            all_predictions = []
            all_recommendations = []
            precision_list = []
            recall_list = []
            f1_list = []
            map_list = []
            mrr_list = []
            ndcg_list = []
            
            test_users = self.test['userId'].unique()
            valid_user_count = 0
            
            for idx, user_id in enumerate(test_users):
                if (idx + 1) % max(1, len(test_users) // 10) == 0:
                    print(f"   ì§„í–‰: {idx+1}/{len(test_users)}")
                
                user_test = self.test[self.test['userId'] == user_id]
                relevant_movies = set(user_test[user_test['rating'] >= 4]['movieId'].values)
                
                recommendations = self.get_recommendations(user_id, n=10, method=method)
                
                if len(recommendations) == 0:
                    continue
                
                valid_user_count += 1
                all_recommendations.extend(recommendations)
                
                for movie_id in user_test['movieId'].values:
                    if method == 'cf':
                        pred = self.predict_cf(user_id, movie_id)
                    elif method == 'cb':
                        pred = self.predict_cb(user_id, movie_id)
                    elif method == 'weighted_avg':
                        pred = self.predict_weighted_avg(user_id, movie_id)
                    elif method == 'feature_combo':
                        pred = self.predict_feature_combo(user_id, movie_id)
                    else:
                        pred = self.predict_mixed(user_id, movie_id)
                    
                    actual = user_test[user_test['movieId'] == movie_id]['rating'].values[0]
                    all_predictions.append((pred, actual))
                
                precision_list.append(
                    AdvancedMetrics.precision_at_k(recommendations, relevant_movies, k=10)
                )
                recall_list.append(
                    AdvancedMetrics.recall_at_k(recommendations, relevant_movies, k=10)
                )
                f1_list.append(
                    AdvancedMetrics.f1_at_k(recommendations, relevant_movies, k=10)
                )
                map_list.append(
                    AdvancedMetrics.map_at_k(recommendations, relevant_movies, k=10)
                )
                mrr_list.append(
                    AdvancedMetrics.mrr_at_k(recommendations, relevant_movies, k=10)
                )
                
                relevance = [1 if movie_id in relevant_movies else 0 for movie_id in recommendations]
                ndcg_list.append(
                    AdvancedMetrics.ndcg_at_k(relevance, k=10)
                )
        
            if len(all_predictions) == 0 or len(precision_list) == 0:
                print(f"   âš ï¸  ê²½ê³ : {method}ì— ëŒ€í•œ ìœ íš¨í•œ ì¶”ì²œì´ ì—†ìŠµë‹ˆë‹¤")
                continue
            
            predictions_array = np.array([p[0] for p in all_predictions])
            actuals_array = np.array([p[1] for p in all_predictions])
            
            rmse = AdvancedMetrics.rmse(actuals_array, predictions_array)
            mae = AdvancedMetrics.mae(actuals_array, predictions_array)
            
            total_possible_ratings = self.ratings['userId'].nunique() * self.ratings['movieId'].nunique()
            actual_ratings = len(self.ratings)
            sparsity = 1 - (actual_ratings / (total_possible_ratings + 1e-8))
            
            adjusted_rmse = AdvancedMetrics.sparsity_aware_rmse(
                actuals_array,
                predictions_array,
                self.ratings['userId'].nunique(),
                self.ratings['movieId'].nunique()
            )
            adjusted_mae = mae * (1 + sparsity)
            
            avg_precision = np.mean(precision_list) if precision_list else 0.0
            avg_recall = np.mean(recall_list) if recall_list else 0.0
            avg_f1 = np.mean(f1_list) if f1_list else 0.0
            avg_map = np.mean(map_list) if map_list else 0.0
            avg_mrr = np.mean(mrr_list) if mrr_list else 0.0
            avg_ndcg = np.mean(ndcg_list) if ndcg_list else 0.0
            
            diversity = AdvancedMetrics.intra_list_diversity(
                all_recommendations, self.item_similarity, self.item_to_idx
            ) if all_recommendations else 0.0
            
            coverage = AdvancedMetrics.coverage(
                all_recommendations, 
                len(self.movie_features)
            )
            
            novelty = AdvancedMetrics.novelty(all_recommendations, self.popularity)
            popularity_bias = AdvancedMetrics.popularity_bias(all_recommendations, self.popularity)
            
            result = {
                'RMSE': rmse,
                'MAE': mae,
                'Sparsity': sparsity,
                'Adjusted_RMSE': adjusted_rmse,
                'Adjusted_MAE': adjusted_mae,
                'Precision@10': avg_precision,
                'Recall@10': avg_recall,
                'F1@10': avg_f1,
                'MAP@10': avg_map,
                'MRR@10': avg_mrr,
                'NDCG@10': avg_ndcg,
                'Diversity': diversity,
                'Coverage': coverage,
                'Novelty': novelty,
                'PopularityBias': popularity_bias,
                'Num_Samples': len(self.test),
                'Dataset': self.name.split('_')[0],
                'Method': method.replace('_', ' ').title()
            }
            
            results.append(result)
            
            print(f"\n   âœ… {method.upper()} ì™„ë£Œ ({valid_user_count}ëª… ì‚¬ìš©ì):")
            print(f"      ì •í™•ë„ - RMSE: {rmse:.4f}, MAE: {mae:.4f}")
            print(f"      ìˆœìœ„   - Precision@10: {avg_precision:.4f}, NDCG@10: {avg_ndcg:.4f}")
            print(f"      ë‹¤ì–‘ì„± - Diversity: {diversity:.4f}, Coverage: {coverage:.4f}")

        print(f"\nâœ… í‰ê°€ ì™„ë£Œ")
        return pd.DataFrame(results)


# =====================================================================================================
# [ì„¹ì…˜ 4] ì•Œê³ ë¦¬ì¦˜ ê²€ì¦
# =====================================================================================================

def validate_algorithms():
    """5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ ë™ì‘ ê²€ì¦"""
    print("\n" + "="*100)
    print("ğŸ” ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("="*100)
    
    ratings, movies = load_movielens('Small')
    if ratings is None:
        print("âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    model = OptimizedHybridRecommender(ratings, movies, name='Validation_Model', svd_dim=100)
    model._prepare()
    
    test_user = model.test['userId'].iloc[0]
    test_movie = model.test['movieId'].iloc[0]
    
    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: ì‚¬ìš©ì {test_user}, ì˜í™” {test_movie}")
    model.debug_predictions(test_user, test_movie)
    
    print(f"\nğŸ“‹ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ (ì‚¬ìš©ì {test_user}):")
    recs_cf = model.get_recommendations(test_user, n=10, method='cf')
    recs_cb = model.get_recommendations(test_user, n=10, method='cb')
    recs_weighted = model.get_recommendations(test_user, n=10, method='weighted_avg')
    recs_feature = model.get_recommendations(test_user, n=10, method='feature_combo')
    recs_mixed = model.get_recommendations(test_user, n=10, method='mixed')
    
    print(f"\nğŸ“Š ê³µì •ì„± ê²€ì¦ (Information Fairness):")
    print(f"ì°¸ê³ : Ekstrand et al. (2018) 'Beyond Personalization: Research Directions in Multistakeholder Recommendation'")
    print(f"CFì™€ CBëŠ” ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì •ë³´ëŸ‰ í¸í–¥ ì¡´ì¬")
    print(f"- CF: í˜‘ë ¥ ì‹ í˜¸ë§Œ (ì‚¬ìš©ì-ì˜í™” ìƒí˜¸ì‘ìš©)")
    print(f"- CB: ì½˜í…ì¸  ì‹ í˜¸ë§Œ (ë©”íƒ€ë°ì´í„°)")
    print(f"- í•˜ì´ë¸Œë¦¬ë“œ: ë‘ ì‹ í˜¸ ê²°í•© (ë” ë§ì€ ì •ë³´ í™œìš©)")
    
    print(f"\nâœ… ê° ë°©ë²•ë³„ ì¶”ì²œ:")
    print(f"   1ï¸âƒ£  CF: {len(recs_cf)}/10")
    print(f"   2ï¸âƒ£  CB: {len(recs_cb)}/10")
    print(f"   3ï¸âƒ£  Weighted Avg: {len(recs_weighted)}/10")
    print(f"   4ï¸âƒ£  Feature Combo: {len(recs_feature)}/10")
    print(f"   5ï¸âƒ£  Mixed: {len(recs_mixed)}/10")
    
    print(f"\nğŸ“Š ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ê²¹ì¹¨:")
    print(f"   CF vs CB: {len(set(recs_cf) & set(recs_cb))}/10")
    print(f"   CF vs Weighted: {len(set(recs_cf) & set(recs_weighted))}/10")
    print(f"   CB vs Weighted: {len(set(recs_cb) & set(recs_weighted))}/10")
    print(f"{'='*100}")


# =====================================================================================================
# [ì„¹ì…˜ 5] ë©”ì¸ ì‹¤í–‰
# =====================================================================================================

def main():
    """MovieLens ë°ì´í„°ì…‹ í‰ê°€ ì‹¤í–‰"""
    datasets_to_test = ['Small', '1M']
    all_results = []

    for dataset in datasets_to_test:
        print(f"\n{'='*100}")
        print(f"ğŸ“Š {dataset} ë°ì´í„°ì…‹ í‰ê°€ ì¤‘...")
        print(f"{'='*100}")

        ratings = None
        movies = None
        model = None
        results_df = None

        try:
            ratings, movies = load_movielens(dataset)
            if ratings is None or movies is None:
                print(f"âŒ {dataset} ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨")
                continue

            svd_dim = 200
            print(f"ğŸ“Œ SVD ì°¨ì›: {svd_dim}")

            model = OptimizedHybridRecommender(
                ratings, 
                movies, 
                name=f'{dataset}_Model',
                svd_dim=svd_dim
            )
            model._prepare()
            results_df = model.evaluate()
            
            for idx, row in results_df.iterrows():
                result_dict = row.to_dict()
                all_results.append(result_dict)
        
        except Exception as e:
            print(f"âŒ {dataset} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            if model is not None:
                del model
            if ratings is not None:
                del ratings
            if movies is not None:
                del movies
            if results_df is not None:
                del results_df
            
            import gc
            gc.collect()
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

    if all_results:
        print(f"\n{'='*100}")
        print("ğŸ“ˆ ìµœì¢… í‰ê°€ ê²°ê³¼: ì •í™•ë„ vs ìˆœìœ„ vs ë‹¤ì–‘ì„±")
        print(f"{'='*100}")

        results_df = pd.DataFrame(all_results)

        print("\n" + "="*100)
        print("ğŸ“Š ì •í™•ë„ ì§€í‘œ (RMSE, MAE)")
        print("="*100)
        accuracy_cols = ['Dataset', 'Method', 'RMSE', 'MAE', 'Adjusted_RMSE']
        print(results_df[accuracy_cols].to_string(index=False))

        print("\n" + "="*100)
        print("ğŸ“Š ìˆœìœ„ ì§€í‘œ (Precision, NDCG, MAP, MRR)")
        print("="*100)
        ranking_cols = ['Dataset', 'Method', 'Precision@10', 'Recall@10', 'NDCG@10', 'MAP@10']
        print(results_df[ranking_cols].to_string(index=False))

        print("\n" + "="*100)
        print("ğŸ“Š ë‹¤ì–‘ì„± ì§€í‘œ (Diversity, Coverage, Novelty, PopularityBias)")
        print("="*100)
        diversity_cols = ['Dataset', 'Method', 'Diversity', 'Coverage', 'Novelty', 'PopularityBias']
        print(results_df[diversity_cols].to_string(index=False))

        print("\n" + "="*100)
        print("ğŸ“Š ì •í™•ë„ vs ìˆœìœ„ vs ë‹¤ì–‘ì„± ë¹„êµ")
        print("="*100)
        
        for dataset in results_df['Dataset'].unique():
            dataset_results = results_df[results_df['Dataset'] == dataset]
            print(f"\nğŸ” {dataset} ë°ì´í„°ì…‹:")
            
            for _, row in dataset_results.iterrows():
                method_name = row['Method']
                
                # âœ… ìˆ˜ì •: Weighted Avgì˜ ì‹¤ì œ ê°€ì¤‘ì¹˜(40:60) í‘œì‹œ
                if 'Weighted' in method_name:
                    method_name += " (CF 40% : CB 60%)"
                
                print(f"\n   {method_name}")
                print(f"      ì •í™•ë„: RMSE={row['RMSE']:.4f}, MAE={row['MAE']:.4f}")
                print(f"      ìˆœìœ„  : Precision={row['Precision@10']:.4f}, NDCG={row['NDCG@10']:.4f}")
                print(f"      ë‹¤ì–‘ì„±: Diversity={row['Diversity']:.4f}, Coverage={row['Coverage']:.4f}, Novelty={row['Novelty']:.4f}")

        output_filename = 'hybrid_recommender_v12_results.csv'
        results_df.to_csv(output_filename, index=False)
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_filename}")
    else:
        print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤")


if __name__ == "__main__":
    validate_algorithms()
    print("\n" + "="*100)
    print("ğŸš€ ë©”ì¸ í‰ê°€ ì‹œì‘")
    print("="*100)
    main()
