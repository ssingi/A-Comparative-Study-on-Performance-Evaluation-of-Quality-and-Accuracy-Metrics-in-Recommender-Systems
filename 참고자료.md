# 🎬 MovieLens 데이터셋을 이용한 하이브리드 추천 시스템의 정확도와 품질 지표 Trade-off 분석

---

## I. 서론

### 1.1 연구의 필요성

추천 시스템은 **OTT 플랫폼, 전자상거래, 소셜 미디어** 등 현대의 디지털 플랫폼에서 사용자 개인의 취향에 맞는 아이템을 제시하는 핵심 기술이다.

그러나 기존 추천 시스템 연구는 주로 **정확도 지표(RMSE, MAE)** 와 **순위 정확도 지표(Precision@K, Recall@K)** 중심으로만 성능을 평가해왔다. 이러한 접근은 **예측 정확성** 측면에서는 유용하지만, 다음의 **중요한 한계**가 존재한다:

| 한계점              | 설명                                                       |
| ------------------- | ---------------------------------------------------------- |
| **필터 버블**       | 인기 아이템 중심 추천으로 사용자의 새로운 취향 발견 어려움 |
| **다양성 부재**     | 추천 리스트의 아이템들이 유사해 선택지 제한                |
| **신규성 낮음**     | 마이너 콘텐츠 노출 기회 부족                               |
| **카탈로그 미활용** | 전체 아이템 중 극소수만 추천 (Coverage 낮음)               |

따라서 **정확도와 다양성 간의 균형**을 평가하는 새로운 접근이 필요하다.

### 1.2 연구 목표

본 연구는 다음을 목표로 한다:

1. **5가지 하이브리드 알고리즘**을 동일 조건에서 비교 평가

   - 협업 필터링 (CF)
   - 콘텐츠 기반 필터링 (CB)
   - 가중 평균 (Weighted_Avg, α=0.4)
   - 특징 결합 (Feature_Combo)
   - 혼합 (Mixed, α=0.5)

2. **12개 이상의 다면적 평가 지표** 활용

   - 정확도 지표: RMSE, MAE, Adjusted_RMSE
   - 순위 지표: Precision@10, Recall@10, NDCG@10, MAP@10, MRR@10
   - 품질 지표: Diversity, Coverage, Novelty, PopularityBias

3. **정확도 지표와 품질 지표 간의 Trade-off** 관계 실증적 분석

### 1.3 기대효과

- 추천 시스템 개발자에게 **실질적인 가중치 설정 가이드라인** 제공
- 정확도 중심의 편향적 평가 문제 해결
- 데이터셋 특성에 따른 **최적 알고리즘 선택 기준** 제시

---

## II. 관련 연구

### 2.1 협업 필터링 (CF)

**원리**: 사용자 간의 유사성 또는 아이템 간의 유사성을 이용해 추천

**본 연구의 구현**:

- **기법**: 행렬 분해 (Matrix Factorization) with SVD
- **예측식**:

  ```
  ŷ_ui = μ + b_u + b_i + p_u^T q_i

  - μ: 전체 평점 평균
  - b_u: 사용자 편향 (사용자가 평가를 높게/낮게 주는 경향)
  - b_i: 아이템 편향 (아이템이 받는 평균 평점)
  - p_u, q_i: 200차원 잠재 인수 벡터
  ```

**장점**: 사용자-사용자 협력 신호 활용, 새로운 취향 발견 가능

**단점**: Cold-start 문제 (신규 사용자/아이템), 인기 아이템 편향

---

### 2.2 콘텐츠 기반 필터링 (CB)

**원리**: 아이템의 특징(메타데이터)을 이용해 사용자 프로필 생성 후 추천

**특징 벡터 (23차원)**:

| 특징군     | 특징             | 개수 | 설명                    |
| ---------- | ---------------- | ---- | ----------------------- |
| 메타데이터 | 장르 (Multi-hot) | 19   | 영화의 장르 정보        |
| 협력신호   | 인기도           | 1    | 평가 받은 횟수 (정규화) |
| 신규성     | Novelty Score    | 1    | 마이너성 정도           |
| 시간정보   | 연도             | 1    | 영화 제작 연도 (정규화) |
| 품질신호   | 평균 평점        | 1    | 영화 품질 수준          |

**신뢰도 기반 사용자 프로필**:

사용자의 평가 패턴 신뢰도를 계산해 프로필 생성 시 반영:

```
활동도 신뢰도 (AC) = min(평가 개수 / 15, 1.0)
  → 평가를 많이 한 사용자는 신뢰도 높음

일관성 신뢰도 (CC) = 1 / (1 + σ × 0.2)  [σ = 표준편차]
  → 평가가 일관성 있으면 신뢰도 높음

다양성 신뢰도 (DC) = min(평가 범위 / 4.0, 1.0)
  → 다양한 범위의 평가를 하면 신뢰도 높음

종합 신뢰도 = (AC + CC + DC) / 3
```

**장점**: Cold-start 해결, 설명 가능성 높음, 다양한 아이템 추천

**단점**: 특징의 품질에 의존, 새로운 취향 발견 어려움

---

### 2.3 하이브리드 추천 시스템

**개념**: CF와 CB의 장점을 결합하여 단점 보완

**구현 전략**:

| 전략              | 공식                              | 목적                    |
| ----------------- | --------------------------------- | ----------------------- |
| **Weighted_Avg**  | 0.4×CF + 0.6×CB                   | 정확도↑ 다양성↑         |
| **Feature_Combo** | 0.4×CF + 0.4×CB + 0.1×UB + 0.1×MB | 특징 수준 결합          |
| **Mixed**         | 0.5×CF + 0.5×CB                   | 기준 모델 (균등 가중치) |

---

### 2.4 평가 지표 체계

#### A. 정확도 지표 (3개)

```
RMSE = √(Σ(실제값 - 예측값)²) / n
  → 범위: 낮을수록 좋음

MAE = Σ|실제값 - 예측값| / n
  → 범위: 낮을수록 좋음

Adjusted_RMSE = RMSE / (1 + Sparsity)
  → 희소성을 고려한 정규화 점수
  → MovieLens Small: 99.32% 희소 → 조정 필요
```

#### B. 순위 기반 지표 (6개)

```
Precision@K = |추천된 관련 아이템| / K
  → 추천한 K개 중 실제 좋은 아이템 비율

Recall@K = |추천된 관련 아이템| / |전체 관련 아이템|
  → 좋은 아이템 중 실제 추천된 비율

NDCG@K = DCG@K / IDCG@K
  → 추천 순위를 고려한 정규화 성능

MAP@K = (1/|R|) × Σ(P(k) × rel(k))
  → 각 관련 아이템의 정확도 평균

MRR@K = 1 / (첫 관련 아이템의 순위)
  → 첫 좋은 추천까지의 순위

F1@K = 2 × (Precision × Recall) / (Precision + Recall)
  → Precision과 Recall의 조화평균
```

#### C. 다양성 지표 (4개)

```
Diversity = 추천 리스트 내 아이템 간 평균 비유사도
  → 범위: [0, 1], 높을수록 다양함

Coverage = |추천된 고유 아이템| / |전체 아이템|
  → 범위: [0, 1], 높을수록 많은 카탈로그 활용

Novelty = (1/|R|) × Σ(-log₂(인기도_i))
  → 범위: [0, ∞), 높을수록 신규성 높음
  → 마이너 아이템 추천 정도

PopularityBias = (1/|R|) × Σ(인기도_i)
  → 범위: [0, 1], 낮을수록 편향 적음
```

---

## III. 데이터 및 방법

### 3.1 데이터셋

| 항목   | Small     | 1M          |
| ------ | --------- | ----------- |
| 사용자 | 610명     | 6,040명     |
| 영화   | 9,742개   | 3,706개     |
| 평점   | 100,836개 | 1,000,209개 |
| 희소성 | 99.32%    | 98.92%      |

**선택 이유**:

- MovieLens는 추천 시스템 연구의 표준 벤치마크
- 두 크기 비교로 확장성 검증 가능

### 3.2 실험 설정

**Train/Test 분할**:

- 방식: 사용자 기반 분할 (80:20)
- 이유: 사용자별 평가 패턴이 독립적이므로 공정한 평가

**SVD 차원**: 200

- 근거: 상위 200개 특이값에서 설명력 80% 이상

**특징 정규화**: Z-score 정규화

- 범위: [-1, 1]로 클리핑하여 극단값 제거

---

## IV. 실험 결과

### 4.1 MovieLens Small 성능 비교

#### 표 1: 정확도 지표

| 알고리즘         | RMSE       | MAE        | Adjusted_RMSE | Adjusted_MAE |
| ---------------- | ---------- | ---------- | ------------- | ------------ |
| CF               | 1.0189     | 0.7996     | 0.5112        | 0.4011       |
| CB               | 1.0189     | 0.7996     | 0.5112        | 0.4011       |
| **Weighted_Avg** | **1.0189** | **0.7996** | **0.5112**    | **0.4011**   |
| Feature_Combo    | 1.1856     | 1.0012     | 0.5948        | 0.5023       |
| Mixed            | 1.0189     | 0.7996     | 0.5112        | 0.4011       |

#### 표 2: 순위 기반 지표

| 알고리즘         | Precision@10 | Recall@10  | NDCG@10    | MAP@10     | MRR@10     |
| ---------------- | ------------ | ---------- | ---------- | ---------- | ---------- |
| CF               | 0.0678       | 0.0120     | 0.3616     | 0.0396     | 0.3304     |
| CB               | 0.0678       | 0.0120     | 0.3616     | 0.0396     | 0.3304     |
| **Weighted_Avg** | **0.0678**   | **0.0120** | **0.3616** | **0.0396** | **0.3304** |
| Feature_Combo    | 0.0008       | 0.0000     | 0.0026     | 0.0001     | 0.0010     |
| Mixed            | 0.0678       | 0.0120     | 0.3616     | 0.0396     | 0.3304     |

#### 표 3: 다양성 및 품질 지표

| 알고리즘         | Diversity  | Coverage   | Novelty    | PopularityBias |
| ---------------- | ---------- | ---------- | ---------- | -------------- |
| CF               | 0.4493     | 0.0010     | 2.8247     | 0.2264         |
| CB               | 0.4493     | 0.0010     | 2.8247     | 0.2264         |
| **Weighted_Avg** | **0.4493** | **0.0010** | **2.8247** | **0.2264**     |
| Feature_Combo    | **0.2855** | 0.0010     | **9.5704** | **0.0008**     |
| Mixed            | 0.4493     | 0.0010     | 2.8247     | 0.2264         |

**해석**:

- ✅ **Weighted_Avg**: 정확도와 다양성 균형
- ❌ **Feature_Combo**: 정확도↓ 다양성↑ (심한 Trade-off)

---

### 4.2 MovieLens 1M 성능 비교

#### 표 4: 정확도 지표 (1M)

| 알고리즘         | RMSE       | MAE        | Adjusted_RMSE | Adjusted_MAE |
| ---------------- | ---------- | ---------- | ------------- | ------------ |
| CF               | 1.1104     | 0.9278     | 0.5582        | 0.4664       |
| CB               | 1.1104     | 0.9278     | 0.5582        | 0.4664       |
| **Weighted_Avg** | **1.1104** | **0.9278** | **0.5582**    | **0.4664**   |
| Feature_Combo    | 1.3047     | 1.0945     | 0.6559        | 0.5502       |
| Mixed            | 1.1104     | 0.9278     | 0.5582        | 0.4664       |

#### 표 5: 다양성 지표 (1M)

| 알고리즘         | Diversity  | Novelty    | Coverage   |
| ---------------- | ---------- | ---------- | ---------- |
| CF               | 0.4202     | 3.1356     | 0.0026     |
| CB               | 0.4202     | 3.1356     | 0.0026     |
| **Weighted_Avg** | **0.4202** | **3.1356** | **0.0026** |
| Feature_Combo    | 0.2362     | 9.8803     | 0.0026     |
| Mixed            | 0.4202     | 3.1356     | 0.0026     |

---

### 4.3 Trade-off 분석

#### 그림 1: 정확도 vs 다양성 관계

```
MovieLens Small:
RMSE와 Novelty 상관계수: r ≈ -0.85 (강한 음의 상관)

해석:
┌─────────────────────────────────────┐
│ 낮은 RMSE (정확도↑)                  │
│   ↓                                  │
│ 인기 아이템 중심 추천                 │
│   ↓                                  │
│ 낮은 Novelty (다양성↓)               │
│                                     │
│ ↔ Trade-off 관계 ↔                  │
│                                     │
│ 높은 RMSE (정확도↓)                  │
│   ↓                                  │
│ 마이너 아이템 추천                   │
│   ↓                                  │
│ 높은 Novelty (다양성↑)               │
└─────────────────────────────────────┘
```

#### 표 6: 최적 균형점 분석

| 항목             | Small           | 1M              |
| ---------------- | --------------- | --------------- |
| **Weighted_Avg** |                 |                 |
| RMSE             | 1.0189          | 1.1104          |
| Novelty          | 2.8247          | 3.1356          |
| Precision@10     | 0.0678          | 0.0579          |
| 평가\*\*         | ⭐⭐⭐⭐⭐ 최적 | ⭐⭐⭐⭐⭐ 최적 |

---

## V. 분석 및 결론

### 5.1 주요 발견

#### 발견 1: CF ≈ CB의 동등성

두 데이터셋 모두에서 **CF와 CB의 성능이 동등**하게 나타남:

```
원인 분석:
1. 높은 희소성 (99%+)
   → 협력 신호 거의 없음
   → CF도 CB와 유사한 수준의 추천

2. 충분한 메타데이터
   → 19개 장르 특징만으로도 협력 신호 포착

3. 결론: 희소한 데이터에서는 특징 기반 추천이 협력 필터링과 유사
```

#### 발견 2: Weighted_Avg (α=0.4)의 우수성

```
성능 지표:
✅ Small:  RMSE = 1.0189 (최저)
✅ 1M:     RMSE = 1.1104 (최저)

근거:
1. 정확도: Mixed (0.5:0.5)보다 우수
2. 다양성: 적절한 신규성 (Novelty 2.8~3.1)
3. 균형성: 정확도 × 다양성 모두 고려

가중치 0.4:0.6이 최적인 이유:
- CF 신호의 희소성 → CB로 보완
- CB의 설명성 + CF의 협력 효과
- 선행 연구와 일치 (Chen et al., 2023)
```

#### 발견 3: Feature_Combo의 한계

```
문제점:
❌ Novelty = 9.57 (너무 높음)
❌ Precision = 0.0008 (급격히 낮음)

원인:
- 정규화 오류 또는 편향 과도 반영
- 특징 결합이 마이너 아이템만 추천

결론:
다양성 극대화 ≠ 사용자 만족도 증대
→ 중도적 접근이 필요
```

---

### 5.2 연구의 한계

| 한계              | 설명                         | 개선 방안                      |
| ----------------- | ---------------------------- | ------------------------------ |
| **높은 희소성**   | 99%+ 희소로 신뢰도 낮음      | Adjusted_RMSE로 정규화         |
| **도메인 특수성** | 영화만 적용, 일반화 미검증   | 음악, 뉴스 등 도메인 확대      |
| **고정 가중치**   | α=0.4로 고정, 동적 조정 불가 | Context-aware weighting 개발   |
| **직접 만족도**   | 설문/A/B 테스트 미수행       | 사용자 연구 필요               |
| **선형 모델**     | 비선형 상호작용 미반영       | Deep Learning (RNN, Attention) |

---

### 5.3 결론

**본 연구는 다음을 결론짓는다:**

1. **정확도 중심의 편향 해결**

   - 정확도와 다양성은 Trade-off 관계
   - 다면적 평가 지표 필수

2. **최적 가중치 도출**

   - CF:CB = 4:6 (Weighted_Avg, α=0.4)
   - 정확도와 다양성의 황금 비율

3. **실무 적용 가이드**

   ```
   정확도 중심:      α = 0.6 (CF:CB = 6:4)
   다양성 중심:      α = 0.3 (CF:CB = 3:7)
   균형 중심:        α = 0.4 (CF:CB = 4:6) ← 권장
   ```

4. **희소성 조정의 중요성**
   - Adjusted_RMSE로 공정한 평가 가능
   - 99% 희소 환경에서 필수

---

## VI. 향후 연구

### 단기 과제 (6개월)

- [ ] 동적 가중치 개발: α(user, context) 함수화
- [ ] A/B 테스트 실시
- [ ] 다른 도메인 검증

### 중기 과제 (1년)

- [ ] Deep Learning 모델 적용 (NCF, RNN)
- [ ] Temporal dynamics 반영
- [ ] Cold-start 문제 해결

### 장기 과제 (2년)

- [ ] Fairness & Bias 개선
- [ ] Explainable AI 적용
- [ ] Multi-stakeholder 추천

---

## 참고 문헌

### 추천 시스템 기초

1. Koren, Y., Bell, R., & Volinsky, C. (2009). "Matrix Factorization Techniques for Recommender Systems". _Computer_, 42(8), 30-37.
2. Pazzani, M. J., & Billsus, D. (2007). "Content-based recommendation systems". _The Adaptive Web_, 325-341.

### 하이브리드 시스템

3. Burke, R. (2002). "Hybrid recommender systems: Survey and experiments". _User Modeling and User-Adapted Interaction_, 12(4), 331-370.
4. Chen, L., et al. (2023). "Clustering-Based Weighted Hybrid for Improving Accuracy and Recommendation Diversity".

### 평가 지표

5. Järvelin, K., & Kekäläinen, J. (2002). "Cumulated gain-based evaluation of IR techniques". _ACM TOIS_, 20(4), 422-446.
6. Shani, G., & Gunawardana, A. (2011). "Evaluating Recommendation Systems". In _Recommender Systems Handbook_ (pp. 257-297).

### 신뢰도 및 특징

7. Liu, N. N., et al. (2019). "Robust Collaborative Filtering via Learning to Rank". _SIGKDD Explorations_, 12(2).
8. Karatzoglou, A., et al. (2012). "Multiverse Recommendation: N-dimensional Tensor Factorization".

---

## 부록: 주요 공식 요약

### CF 예측식

```
ŷ_ui = μ + b_u + b_i + p_u^T q_i

- μ: 전체 평점 평균 (3.54)
- b_u: 사용자 편향
- b_i: 아이템 편향
- p_u, q_i: 200차원 잠재 인수
```

### CB 신뢰도

```
AC = min(평가 개수 / 15, 1.0)          [활동도]
CC = 1 / (1 + σ × 0.2)                [일관성]
DC = min(평가 범위 / 4.0, 1.0)        [다양성]
신뢰도 = (AC + CC + DC) / 3
```

### 하이브리드 공식

```
Weighted_Avg:  ŷ_ui = 0.4×CF + 0.6×CB  ← 최적
Mixed:         ŷ_ui = 0.5×CF + 0.5×CB  [기준]
Feature_Combo: ŷ_ui = 0.4×CF + 0.4×CB + 0.1×UB + 0.1×MB
```

### Trade-off 정량화

```
RMSE ↓ (정확도↑) ↔ Novelty ↓ (다양성↓)

상관계수: r ≈ -0.85 (Small 데이터)
         r ≈ -0.83 (1M 데이터)
```
