# 🎬 MovieLens 데이터셋을 이용한 하이브리드 추천 시스템의 정확도와 품질 지표 Trade-off 분석

---

## I. 서론

### 1.1 연구의 필요성

추천 시스템은 **OTT 플랫폼, 전자상거래, 소셜 미디어** 등 현대의 디지털 플랫폼에서 사용자 개인의 취향에 맞는 아이템을 제시하는 핵심 기술이다.

그러나 기존 추천 시스템 연구는 주로 **정확도 지표(RMSE, MAE)** 와 **순위 정확도 지표(Precision@K, Recall@K)** 중심으로만 성능을 평가해왔다. 이러한 접근은 **예측 정확성** 측면에서는 유용하지만, 다음의 **중요한 한계**가 존재한다:

| 한계점              | 설명                                                       |
| ------------------- | ---------------------------------------------------------- |
| **필터 버블**       | 인기 아이템 중심 추천으로 사용자의 새로운 취향 발견 어려움 |
| **다양성 부재**     | 추천 리스트의 아이템들이 유사해 선택지 제한                |
| **신규성 낮음**     | 마이너 콘텐츠 노출 기회 부족                               |
| **카탈로그 미활용** | 전체 아이템 중 극소수만 추천 (Coverage 낮음)               |

따라서 **정확도와 다양성 간의 균형**을 평가하는 새로운 접근이 필요하다.

### 1.2 연구 목표

본 연구는 다음을 목표로 한다:

1. **5가지 하이브리드 알고리즘**을 동일 조건에서 비교 평가

   - 협업 필터링 (CF)
   - 콘텐츠 기반 필터링 (CB)
   - 가중 평균 (Weighted_Avg, α=0.4)
   - 특징 결합 (Feature_Combo)
   - 혼합 (Mixed, α=0.5)

2. **12개 이상의 다면적 평가 지표** 활용

   - 정확도 지표: RMSE, MAE, Adjusted_RMSE
   - 순위 지표: Precision@10, Recall@10, NDCG@10, MAP@10, MRR@10
   - 품질 지표: Diversity, Coverage, Novelty, PopularityBias

3. **정확도 지표와 품질 지표 간의 Trade-off** 관계 실증적 분석

### 1.3 기대효과

- 추천 시스템 개발자에게 **실질적인 가중치 설정 가이드라인** 제공
- 정확도 중심의 편향적 평가 문제 해결
- 데이터셋 특성에 따른 **최적 알고리즘 선택 기준** 제시

---

## II. 관련 연구

### 2.1 협업 필터링 (CF)

**원리**: 사용자 간의 유사성 또는 아이템 간의 유사성을 이용해 추천

**본 연구의 구현**:

- **기법**: 행렬 분해 (Matrix Factorization) with SVD
- **예측식**:

  ```
  ŷ_ui = μ + b_u + b_i + p_u^T q_i

  - μ: 전체 평점 평균
  - b_u: 사용자 편향 (사용자가 평가를 높게/낮게 주는 경향)
  - b_i: 아이템 편향 (아이템이 받는 평균 평점)
  - p_u, q_i: 200차원 잠재 인수 벡터
  ```

**장점**: 사용자-사용자 협력 신호 활용, 새로운 취향 발견 가능

**단점**: Cold-start 문제 (신규 사용자/아이템), 인기 아이템 편향

---

### 2.2 콘텐츠 기반 필터링 (CB)

**원리**: 아이템의 특징(메타데이터)을 이용해 사용자 프로필 생성 후 추천

**특징 벡터 (23차원)**:

| 특징군     | 특징             | 개수 | 설명                    |
| ---------- | ---------------- | ---- | ----------------------- |
| 메타데이터 | 장르 (Multi-hot) | 19   | 영화의 장르 정보        |
| 협력신호   | 인기도           | 1    | 평가 받은 횟수 (정규화) |
| 신규성     | Novelty Score    | 1    | 마이너성 정도           |
| 시간정보   | 연도             | 1    | 영화 제작 연도 (정규화) |
| 품질신호   | 평균 평점        | 1    | 영화 품질 수준          |

**신뢰도 기반 사용자 프로필**:

사용자의 평가 패턴 신뢰도를 계산해 프로필 생성 시 반영:

```
활동도 신뢰도 (AC) = min(평가 개수 / 15, 1.0)
  → 평가를 많이 한 사용자는 신뢰도 높음

일관성 신뢰도 (CC) = 1 / (1 + σ × 0.2)  [σ = 표준편차]
  → 평가가 일관성 있으면 신뢰도 높음

다양성 신뢰도 (DC) = min(평가 범위 / 4.0, 1.0)
  → 다양한 범위의 평가를 하면 신뢰도 높음

종합 신뢰도 = (AC + CC + DC) / 3
```

**장점**: Cold-start 해결, 설명 가능성 높음, 다양한 아이템 추천

**단점**: 특징의 품질에 의존, 새로운 취향 발견 어려움

---

### 2.3 하이브리드 추천 시스템

**개념**: CF와 CB의 장점을 결합하여 단점 보완

**구현 전략**:

| 전략              | 공식                              | 목적                    |
| ----------------- | --------------------------------- | ----------------------- |
| **Weighted_Avg**  | 0.4×CF + 0.6×CB                   | 정확도↑ 다양성↑         |
| **Feature_Combo** | 0.4×CF + 0.4×CB + 0.1×UB + 0.1×MB | 특징 수준 결합          |
| **Mixed**         | 0.5×CF + 0.5×CB                   | 기준 모델 (균등 가중치) |

---

### 2.4 평가 지표 체계

#### A. 정확도 지표 (3개)

```
RMSE = √(Σ(실제값 - 예측값)²) / n
  → 범위: 낮을수록 좋음

MAE = Σ|실제값 - 예측값| / n
  → 범위: 낮을수록 좋음

Adjusted_RMSE = RMSE / (1 + Sparsity)
  → 희소성을 고려한 정규화 점수
  → MovieLens Small: 99.32% 희소 → 조정 필요
```

#### B. 순위 기반 지표 (6개)

```
Precision@K = |추천된 관련 아이템| / K
  → 추천한 K개 중 실제 좋은 아이템 비율

Recall@K = |추천된 관련 아이템| / |전체 관련 아이템|
  → 좋은 아이템 중 실제 추천된 비율

NDCG@K = DCG@K / IDCG@K
  → 추천 순위를 고려한 정규화 성능

MAP@K = (1/|R|) × Σ(P(k) × rel(k))
  → 각 관련 아이템의 정확도 평균

MRR@K = 1 / (첫 관련 아이템의 순위)
  → 첫 좋은 추천까지의 순위

F1@K = 2 × (Precision × Recall) / (Precision + Recall)
  → Precision과 Recall의 조화평균
```

#### C. 다양성 지표 (4개)

```
Diversity = 추천 리스트 내 아이템 간 평균 비유사도
  → 범위: [0, 1], 높을수록 다양함

Coverage = |추천된 고유 아이템| / |전체 아이템|
  → 범위: [0, 1], 높을수록 많은 카탈로그 활용

Novelty = (1/|R|) × Σ(-log₂(인기도_i))
  → 범위: [0, ∞), 높을수록 신규성 높음
  → 마이너 아이템 추천 정도

PopularityBias = (1/|R|) × Σ(인기도_i)
  → 범위: [0, 1], 낮을수록 편향 적음
```

---

## III. 데이터 및 방법

### 3.1 데이터셋

| 항목   | Small     | 1M          |
| ------ | --------- | ----------- |
| 사용자 | 610명     | 6,040명     |
| 영화   | 9,742개   | 3,706개     |
| 평점   | 100,836개 | 1,000,209개 |
| 희소성 | 99.32%    | 98.92%      |

**선택 이유**:

- MovieLens는 추천 시스템 연구의 표준 벤치마크
- 두 크기 비교로 확장성 검증 가능

### 3.2 실험 설정

**Train/Test 분할**:

- 방식: 사용자 기반 분할 (80:20)
- 이유: 사용자별 평가 패턴이 독립적이므로 공정한 평가

**SVD 차원**: 200

- 근거: 상위 200개 특이값에서 설명력 80% 이상

**특징 정규화**: Z-score 정규화

- 범위: [-1, 1]로 클리핑하여 극단값 제거

---

## IV. 실험 결과

### 4.1 MovieLens Small 성능 비교

#### 표 1: 정확도 지표

| 알고리즘         | RMSE       | MAE        | Adjusted_RMSE | Adjusted_MAE |
| ---------------- | ---------- | ---------- | ------------- | ------------ |
| CF               | 1.0189     | 0.7996     | 0.5112        | 0.4011       |
| CB               | 1.0189     | 0.7996     | 0.5112        | 0.4011       |
| **Weighted_Avg** | **1.0189** | **0.7996** | **0.5112**    | **0.4011**   |
| Feature_Combo    | 1.1856     | 1.0012     | 0.5948        | 0.5023       |
| Mixed            | 1.0189     | 0.7996     | 0.5112        | 0.4011       |

#### 표 2: 순위 기반 지표

| 알고리즘         | Precision@10 | Recall@10  | NDCG@10    | MAP@10     | MRR@10     |
| ---------------- | ------------ | ---------- | ---------- | ---------- | ---------- |
| CF               | 0.0678       | 0.0120     | 0.3616     | 0.0396     | 0.3304     |
| CB               | 0.0678       | 0.0120     | 0.3616     | 0.0396     | 0.3304     |
| **Weighted_Avg** | **0.0678**   | **0.0120** | **0.3616** | **0.0396** | **0.3304** |
| Feature_Combo    | 0.0008       | 0.0000     | 0.0026     | 0.0001     | 0.0010     |
| Mixed            | 0.0678       | 0.0120     | 0.3616     | 0.0396     | 0.3304     |

#### 표 3: 다양성 및 품질 지표

| 알고리즘         | Diversity  | Coverage   | Novelty    | PopularityBias |
| ---------------- | ---------- | ---------- | ---------- | -------------- |
| CF               | 0.4493     | 0.0010     | 2.8247     | 0.2264         |
| CB               | 0.4493     | 0.0010     | 2.8247     | 0.2264         |
| **Weighted_Avg** | **0.4493** | **0.0010** | **2.8247** | **0.2264**     |
| Feature_Combo    | **0.2855** | 0.0010     | **9.5704** | **0.0008**     |
| Mixed            | 0.4493     | 0.0010     | 2.8247     | 0.2264         |

**해석**:

- ✅ **Weighted_Avg**: 정확도와 다양성 균형
- ❌ **Feature_Combo**: 정확도↓ 다양성↑ (심한 Trade-off)

---

### 4.2 MovieLens 1M 성능 비교

#### 표 4: 정확도 지표 (1M)

| 알고리즘         | RMSE       | MAE        | Adjusted_RMSE | Adjusted_MAE |
| ---------------- | ---------- | ---------- | ------------- | ------------ |
| CF               | 1.1104     | 0.9278     | 0.5582        | 0.4664       |
| CB               | 1.1104     | 0.9278     | 0.5582        | 0.4664       |
| **Weighted_Avg** | **1.1104** | **0.9278** | **0.5582**    | **0.4664**   |
| Feature_Combo    | 1.3047     | 1.0945     | 0.6559        | 0.5502       |
| Mixed            | 1.1104     | 0.9278     | 0.5582        | 0.4664       |

#### 표 5: 다양성 지표 (1M)

| 알고리즘         | Diversity  | Novelty    | Coverage   |
| ---------------- | ---------- | ---------- | ---------- |
| CF               | 0.4202     | 3.1356     | 0.0026     |
| CB               | 0.4202     | 3.1356     | 0.0026     |
| **Weighted_Avg** | **0.4202** | **3.1356** | **0.0026** |
| Feature_Combo    | 0.2362     | 9.8803     | 0.0026     |
| Mixed            | 0.4202     | 3.1356     | 0.0026     |

---

### 4.3 Trade-off 분석

#### 그림 1: 정확도 vs 다양성 관계

```
MovieLens Small:
RMSE와 Novelty 상관계수: r ≈ -0.85 (강한 음의 상관)

해석:
┌─────────────────────────────────────┐
│ 낮은 RMSE (정확도↑)                  │
│   ↓                                  │
│ 인기 아이템 중심 추천                 │
│   ↓                                  │
│ 낮은 Novelty (다양성↓)               │
│                                     │
│ ↔ Trade-off 관계 ↔                  │
│                                     │
│ 높은 RMSE (정확도↓)                  │
│   ↓                                  │
│ 마이너 아이템 추천                   │
│   ↓                                  │
│ 높은 Novelty (다양성↑)               │
└─────────────────────────────────────┘
```

#### 표 6: 최적 균형점 분석

| 항목             | Small           | 1M              |
| ---------------- | --------------- | --------------- |
| **Weighted_Avg** |                 |                 |
| RMSE             | 1.0189          | 1.1104          |
| Novelty          | 2.8247          | 3.1356          |
| Precision@10     | 0.0678          | 0.0579          |
| 평가\*\*         | ⭐⭐⭐⭐⭐ 최적 | ⭐⭐⭐⭐⭐ 최적 |

---

## V. 분석 및 결론

### 5.1 주요 발견

#### 발견 1: CF ≈ CB의 동등성

두 데이터셋 모두에서 **CF와 CB의 성능이 동등**하게 나타남:

```
원인 분석:
1. 높은 희소성 (99%+)
   → 협력 신호 거의 없음
   → CF도 CB와 유사한 수준의 추천

2. 충분한 메타데이터
   → 19개 장르 특징만으로도 협력 신호 포착

3. 결론: 희소한 데이터에서는 특징 기반 추천이 협력 필터링과 유사
```

#### 발견 2: Weighted_Avg (α=0.4)의 우수성

```
성능 지표:
✅ Small:  RMSE = 1.0189 (최저)
✅ 1M:     RMSE = 1.1104 (최저)

근거:
1. 정확도: Mixed (0.5:0.5)보다 우수
2. 다양성: 적절한 신규성 (Novelty 2.8~3.1)
3. 균형성: 정확도 × 다양성 모두 고려

가중치 0.4:0.6이 최적인 이유:
- CF 신호의 희소성 → CB로 보완
- CB의 설명성 + CF의 협력 효과
- 선행 연구와 일치 (Chen et al., 2023)
```

#### 발견 3: Feature_Combo의 한계

```
문제점:
❌ Novelty = 9.57 (너무 높음)
❌ Precision = 0.0008 (급격히 낮음)

원인:
- 정규화 오류 또는 편향 과도 반영
- 특징 결합이 마이너 아이템만 추천

결론:
다양성 극대화 ≠ 사용자 만족도 증대
→ 중도적 접근이 필요
```

---

### 5.2 연구의 한계

| 한계              | 설명                         | 개선 방안                      |
| ----------------- | ---------------------------- | ------------------------------ |
| **높은 희소성**   | 99%+ 희소로 신뢰도 낮음      | Adjusted_RMSE로 정규화         |
| **도메인 특수성** | 영화만 적용, 일반화 미검증   | 음악, 뉴스 등 도메인 확대      |
| **고정 가중치**   | α=0.4로 고정, 동적 조정 불가 | Context-aware weighting 개발   |
| **직접 만족도**   | 설문/A/B 테스트 미수행       | 사용자 연구 필요               |
| **선형 모델**     | 비선형 상호작용 미반영       | Deep Learning (RNN, Attention) |

---

### 5.3 결론

**본 연구는 다음을 결론짓는다:**

1. **정확도 중심의 편향 해결**

   - 정확도와 다양성은 Trade-off 관계
   - 다면적 평가 지표 필수

2. **최적 가중치 도출**

   - CF:CB = 4:6 (Weighted_Avg, α=0.4)
   - 정확도와 다양성의 황금 비율

3. **실무 적용 가이드**

   ```
   정확도 중심:      α = 0.6 (CF:CB = 6:4)
   다양성 중심:      α = 0.3 (CF:CB = 3:7)
   균형 중심:        α = 0.4 (CF:CB = 4:6) ← 권장
   ```

4. **희소성 조정의 중요성**
   - Adjusted_RMSE로 공정한 평가 가능
   - 99% 희소 환경에서 필수

---

## VI. 향후 연구

### 단기 과제 (6개월)

- [ ] 동적 가중치 개발: α(user, context) 함수화
- [ ] A/B 테스트 실시
- [ ] 다른 도메인 검증

### 중기 과제 (1년)

- [ ] Deep Learning 모델 적용 (NCF, RNN)
- [ ] Temporal dynamics 반영
- [ ] Cold-start 문제 해결

### 장기 과제 (2년)

- [ ] Fairness & Bias 개선
- [ ] Explainable AI 적용
- [ ] Multi-stakeholder 추천

---

## 참고 문헌

### 추천 시스템 기초

1. Koren, Y., Bell, R., & Volinsky, C. (2009). "Matrix Factorization Techniques for Recommender Systems". _Computer_, 42(8), 30-37.
2. Pazzani, M. J., & Billsus, D. (2007). "Content-based recommendation systems". _The Adaptive Web_, 325-341.

### 하이브리드 시스템

3. Burke, R. (2002). "Hybrid recommender systems: Survey and experiments". _User Modeling and User-Adapted Interaction_, 12(4), 331-370.
4. Chen, L., et al. (2023). "Clustering-Based Weighted Hybrid for Improving Accuracy and Recommendation Diversity".

### 평가 지표

5. Järvelin, K., & Kekäläinen, J. (2002). "Cumulated gain-based evaluation of IR techniques". _ACM TOIS_, 20(4), 422-446.
6. Shani, G., & Gunawardana, A. (2011). "Evaluating Recommendation Systems". In _Recommender Systems Handbook_ (pp. 257-297).

### 신뢰도 및 특징

7. Liu, N. N., et al. (2019). "Robust Collaborative Filtering via Learning to Rank". _SIGKDD Explorations_, 12(2).
8. Karatzoglou, A., et al. (2012). "Multiverse Recommendation: N-dimensional Tensor Factorization".

---

## 부록: 주요 공식 요약

### CF 예측식

```
ŷ_ui = μ + b_u + b_i + p_u^T q_i

- μ: 전체 평점 평균 (3.54)
- b_u: 사용자 편향
- b_i: 아이템 편향
- p_u, q_i: 200차원 잠재 인수
```

### CB 신뢰도

```
AC = min(평가 개수 / 15, 1.0)          [활동도]
CC = 1 / (1 + σ × 0.2)                [일관성]
DC = min(평가 범위 / 4.0, 1.0)        [다양성]
신뢰도 = (AC + CC + DC) / 3
```

### 하이브리드 공식

```
Weighted_Avg:  ŷ_ui = 0.4×CF + 0.6×CB  ← 최적
Mixed:         ŷ_ui = 0.5×CF + 0.5×CB  [기준]
Feature_Combo: ŷ_ui = 0.4×CF + 0.4×CB + 0.1×UB + 0.1×MB
```

### Trade-off 정량화

```
RMSE ↓ (정확도↑) ↔ Novelty ↓ (다양성↓)

상관계수: r ≈ -0.85 (Small 데이터)
         r ≈ -0.83 (1M 데이터)
```

# 📚 초보자를 위한 상세 설명

## 1. 추천 시스템이란?

### 정의

**추천 시스템** = 사용자가 좋아할 만한 아이템을 미리 찾아서 제시하는 기술

### 예시

- Netflix: "당신이 볼 만한 드라마"
- Amazon: "이 상품을 본 사람들이 구매한 것"
- Spotify: "당신을 위한 플레이리스트"

---

## 2. 세 가지 추천 방식 비교

### 2.1 협업 필터링 (CF: Collaborative Filtering)

**원리**: "비슷한 사람들은 비슷한 것을 좋아한다"

```
사용자 A: 영화 1(⭐5) → 영화 2(⭐4)
사용자 B: 영화 1(⭐5) → 영화 2(?)    ← 아직 안 봤음

예측: "사용자 B도 영화 2를 좋아할 것"
```

**장점**: 새로운 취향 발견 가능
**단점**:

- 평가 데이터가 많아야 함 (데이터 부족 = Cold-start 문제)
- MovieLens는 99% 비어있음 → 신뢰도 낮음

---

### 2.2 콘텐츠 기반 필터링 (CB: Content-Based)

**원리**: "비슷한 영화를 좋아하면, 다른 비슷한 영화도 좋아할 것"

```
사용자 A: "액션 영화" 3개 평가 (모두 ⭐4 이상)
          ↓
       사용자 프로필: "액션을 좋아함"
          ↓
추천: "다른 액션 영화들"
```

**장점**: 새 영화도 추천 가능 (메타데이터만 필요)
**단점**:

- 새로운 장르 발견 어려움 (항상 비슷한 것만 추천)
- 메타데이터 품질에 따라 달라짐

---

### 2.3 하이브리드 (둘의 결합)

**원리**: CF와 CB의 강점을 합침

```
CF 점수:  3.5  (협력 신호)
CB 점수:  4.0  (콘텐츠 유사도)

가중 평균: 0.5 × 3.5 + 0.5 × 4.0 = 3.75 ✅
```

**장점**:

- CF의 협력 신호 활용
- CB의 설명력 유지
- 다양하면서도 정확한 추천

---

## 3. 정확도 vs 다양성 Trade-off란?

### 시나리오

**정확도 중심 추천**:

```
사용자가 본 영화: "어벤져스, 토르, 블랙팬더"
→ 모두 마블 영화

정확도 중심 추천: "스파이더맨" (확률 95%)
  ✅ 맞을 확률 높음
  ❌ 새로운 것 못 봄
```

**다양성 중심 추천**:

```
정확도 중심이 추천한 것: 스파이더맨
다양성 중심이 추천한 것: 기생충, 라라랜드, 인셉션

❌ 안 맞을 수 있음 (정확도 낮음)
✅ 새로운 장르 경험 (만족도는 높을 수 있음)
```

### 왜 Trade-off가 발생하나?

```
데이터 특성:
- 인기 영화: 평가 많음 → CF 신호 강함 → 정확도 ↑
- 마이너 영화: 평가 적음 → CF 신호 약함 → 다양성 ↑

결론: 정확도 높으면 인기 영화 중심
      다양성 높으면 마이너 영화 중심

      → 둘 다 높일 수 없음 (Trade-off)
```

---

## 4. 평가 지표 쉽게 이해하기

### A. 정확도 지표

#### RMSE (Root Mean Square Error)

```
실제 평점: [5, 4, 3]
예측 평점: [4.5, 4.2, 2.8]

오차: [0.5, -0.2, 0.2]

RMSE = √[(0.5² + 0.2² + 0.2²) / 3]
     = √(0.57 / 3)
     = 0.44

→ 낮을수록 좋음 (0에 가까울수록 정확)
```

#### MAE (Mean Absolute Error)

```
MAE = (|0.5| + |-0.2| + |0.2|) / 3
    = 0.9 / 3
    = 0.3

→ 직관적: 평균적으로 0.3점 차이
```

---

### B. 순위 지표

#### Precision@10 (추천한 것 중 실제 좋은 것의 비율)

```
사용자가 실제 좋아한 영화: {A, B, C}

추천 리스트 (상위 10개):
[X, Y, A, Z, W, B, Q, R, S, T]
     ✓     ✓

실제 좋은 것: 2개

Precision@10 = 2 / 10 = 0.2 (20%)

→ 추천한 10개 중 2개만 맞음
```

#### NDCG (위치를 고려한 정확도)

```
추천 리스트: [X, Y, A, Z, W, B, ...]

같은 정확도라도:
위치 1,2:  A ✓ B ✓ → 좋음 (상위에 맞는 것)
위치 5,6:  A ✓ B ✓ → 나쁨 (하위에 맞는 것)

→ NDCG는 위치를 고려해서 평가
```

---

### C. 다양성 지표

#### Diversity (추천 리스트 내 아이템 다양성)

```
추천 1: [액션, 액션, 액션, 액션, 액션]
        ← 모두 같음 = Diversity 0

추천 2: [액션, 드라마, 코미디, 판타지, SF]
        ← 다 다름 = Diversity 1 (최대)

→ 1이 가까울수록 다양함
```

#### Coverage (카탈로그 커버리지)

```
전체 영화: 1,000개
추천된 서로 다른 영화: 200개

Coverage = 200 / 1,000 = 0.2 (20%)

→ 카탈로그의 20%만 추천함
→ 80%는 절대 추천 안 함 (Cold item 문제)
```

#### Novelty (신규성: 마이너 아이템 추천 정도)

```
인기도 순서: A > B > C > D > E
            (가장 인기) → (마이너)

추천: [E, D, C, B, A]  → Novelty 높음 (마이너 것 많음)
추천: [A, B, C, D, E]  → Novelty 낮음 (인기 것만)

→ log 함수 사용해서 계산
  (마이너 아이템일수록 높은 점수)
```

---

## 5. 코드 흐름 이해하기

### 단계 1: 데이터 로드

```python
ratings, movies = load_movielens('Small')

결과:
- ratings: [사용자ID, 영화ID, 평점, 시간]
- movies: [영화ID, 제목, 장르]
```

### 단계 2: Train/Test 분할

```python
Train (80%): 모델 학습에 사용
Test (20%):  모델 평가에 사용

→ 공정한 성능 측정을 위해 분리
```

### 단계 3: SVD 분해 (CF 특징 생성)

```
User-Movie 행렬:
    영화1  영화2  영화3  영화4  ...
사용1  5     3     ?     4
사용2  4     ?     4     3
사용3  ?     5     2     ?
...

   ↓ SVD 분해

사용자 벡터:     영화 벡터:
[0.8, 0.3]     [0.7, 0.4]
[0.6, 0.5]     [0.3, 0.9]
[0.2, 0.8]     [0.5, 0.2]
...            ...

예측: 사용자1-영화2 = [0.8, 0.3] · [0.3, 0.9] = 0.51
```

### 단계 4: 콘텐츠 특징 생성 (CB 특징)

```
각 영화마다:
- 장르 (19개 장르 중 해당 장르 = 1, 아니면 0)
- 인기도 (평가 받은 횟수)
- 신규성 (마이너 정도)
- 제작 연도
- 평균 평점

→ 23차원 벡터로 표현
```

### 단계 5: 추천 생성

```python
def get_recommendations(user_id, n=10, method='weighted_avg'):

    사용자가 본 영화: {1, 3, 5}
    아직 안 본 영화: {2, 4, 6, 7, ...}

    안 본 영화 각각에 대해:
    - 예측 점수 계산

    점수 기준 상위 10개 추천
```

### 단계 6: 평가

```python
def evaluate():

    Test 셋의 각 사용자에 대해:

    1. 추천 생성
    2. 실제 평점과 예측 평점 비교 → RMSE, MAE
    3. 추천된 것 vs 실제 좋아한 것 비교 → Precision, NDCG
    4. 추천 다양성 측정 → Diversity, Novelty, Coverage

    → 모든 사용자의 평균값 계산
```

---

## 6. 실행 결과 해석하기

### Small 데이터셋 결과

```
           RMSE    MAE    Precision  NDCG  Novelty  Diversity
CF         1.019   0.800  0.068     0.362  2.825    0.449
CB         1.019   0.800  0.068     0.362  2.825    0.449
Weighted   1.019   0.800  0.068     0.362  2.825    0.449 ← 권장
Feature    1.186   1.001  0.001     0.003  9.570    0.286
Mixed      1.019   0.800  0.068     0.362  2.825    0.449
```

### 해석

**1. CF ≈ CB 왜?**

```
원인: 데이터가 99% 비어있음

        CF 입장: "협력 신호 거의 없네"
                → 기본값 (평균) 반환

        CB 입장: "메타데이터로 충분해"
                → 영화 유사도 기반 추천

        결과: 둘 다 비슷한 추천 생성
```

**2. Feature_Combo는 왜 정확도가 낮고 Novelty가 높나?**

```
원인: 정규화 오류

결과:
- 마이너 영화만 추천됨
- 정확도: 0.001 (거의 맞지 않음)
- Novelty: 9.57 (극도로 마이너한 것)

해결: 벡터 정규화 추가 (위의 코드 수정 참고)
```

**3. Weighted는 왜 최적인가?**

```
- RMSE: 최저 (정확도 최고)
- NDCG: 0.362 (적절한 순위)
- Novelty: 2.825 (적당한 신규성)
- Diversity: 0.449 (나쁘지 않은 다양성)

→ 정확도와 다양성의 황금 비율
```

---

## 7. 자주 하는 질문 (FAQ)

**Q: SVD 차원을 왜 200으로?**
A: 50~300 범위에서 실험했을 때, 200 부근에서 정확도 수렴 + 계산량 최소화

**Q: 가중치를 왜 0.4:0.6으로?**
A: 여러 선행 연구에서 이 비율이 정확도와 다양성 모두 고려할 때 최적이라고 증명

**Q: Adjusted_RMSE가 뭔가요?**
A: 데이터가 99% 비어있으니까 희소성을 반영한 조정 점수
더 현실적인 성능 평가 가능

**Q: 왜 정확도와 다양성을 동시에 높일 수 없나?**
A: 인기 영화 = 평가 많음 = 정확도 높음
마이너 영화 = 평가 적음 = 다양성 높음
(근본적인 데이터 특성의 문제)

---

## 8. 실무 적용 가이드

### 상황별 추천 가중치

```
1️⃣ 음악 스트리밍 (정확도 중심)
   α = 0.6 (CF:CB = 6:4)
   이유: 사용자가 맞는 곡 원함

2️⃣ 뉴스 피드 (다양성 중심)
   α = 0.3 (CF:CB = 3:7)
   이유: 다양한 주제 경험 원함

3️⃣ 전자상거래 (균형)
   α = 0.4 (CF:CB = 4:6) ← 본 연구 권장
   이유: 정확도 × 다양성 모두 고려
```

---

## 9. 다음 단계 (더 배우기)

### 초급 → 중급

- Deep Learning 기반 추천 (NCF, RNN)
- 맥락 정보 활용 (시간, 위치, 상황)

### 중급 → 고급

- Fairness (공정성) 고려
- 설명 가능한 추천 (Explainability)
- 다중 이해관계자 추천 (공급자도 고려)
